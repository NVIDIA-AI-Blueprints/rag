{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20e694c",
   "metadata": {},
   "source": [
    "# Retriever API Usage with Multimodal Query Support\n",
    "\n",
    "This notebook demonstrates how to use the NVIDIA RAG retriever APIs with **multimodal queries** (text + images). You'll learn how to:\n",
    "\n",
    "- üîç Search for relevant documents using queries that contain images\n",
    "- ü§ñ Generate AI responses using the end-to-end RAG API with vision-language models (VLMs)\n",
    "- üìä Work with multimodal embeddings and vector databases\n",
    "\n",
    "**Use Case**: Query documents with images (e.g., \"What is the price of this item?\" + product image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152f1eb",
   "metadata": {},
   "source": [
    "## üì¶ Setting up the Dependencies\n",
    "\n",
    "This section will guide you through:\n",
    "1. Configuring your NGC API key for accessing NVIDIA services\n",
    "2. Deploying the Milvus vector database\n",
    "3. Setting up NVIDIA NIMs (NVIDIA Inference Microservices) for embeddings and VLM\n",
    "4. Starting the NVIDIA Ingest runtime for document processing\n",
    "5. Launching the RAG server\n",
    "\n",
    "**Note**: This setup uses Docker Compose to orchestrate all services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e628e",
   "metadata": {},
   "source": [
    "### 1. Setup the Default Configurations\n",
    "\n",
    "Import necessary libraries for environment management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03780a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python-dotenv for environment variable management\n",
    "! uv pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a19cef7",
   "metadata": {},
   "source": [
    "Provide your NGC_API_KEY after executing the cell below. You can obtain a key by following steps [here](../docs/quickstart.md##obtain-an-api-key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NGC_API_KEY is already set, otherwise prompt for it\n",
    "# Uncomment the line below to reset your API key\n",
    "# del os.environ['NGC_API_KEY']\n",
    "\n",
    "if os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NGC_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    candidate_api_key = getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert candidate_api_key.startswith(\"nvapi-\"), (\n",
    "        f\"{candidate_api_key[:5]}... is not a valid key\"\n",
    "    )\n",
    "    os.environ[\"NGC_API_KEY\"] = candidate_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec8b61",
   "metadata": {},
   "source": [
    "Login to nvcr.io which is needed for pulling the containers of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03972882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to NVIDIA Container Registry (nvcr.io) to pull required containers\n",
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84642fbb",
   "metadata": {},
   "source": [
    "### 2. Setup the Milvus Vector Database\n",
    "\n",
    "Milvus is a high-performance vector database used to store and search multimodal embeddings.\n",
    "\n",
    "**Configuration Notes**:\n",
    "- By default, Milvus uses GPU indexing for faster performance\n",
    "- Ensure you have provided the correct GPU ID below\n",
    "- If you don't have a GPU available, you can switch to CPU-only Milvus by following the instructions in [milvus-configuration.md](../docs/milvus-configuration.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which GPU to use for Milvus (change if using a different GPU)\n",
    "os.environ[\"VECTORSTORE_GPU_DEVICE_ID\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Milvus vector database service\n",
    "# This will run in the background (-d flag)\n",
    "!docker compose -f ../deploy/compose/vectordb.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe17557",
   "metadata": {},
   "source": [
    "### 3. Setup NVIDIA Inference Microservices (NIMs)\n",
    "\n",
    "NIMs provide optimized inference for AI models. For multimodal RAG, we need:\n",
    "- **VLM (Vision-Language Model)**: `nvidia/nemotron-nano-12b-v2-vl` for understanding images and generating responses\n",
    "- **Embedding Model**: `llama-3.2-nemoretriever-1b-vlm-embed-v1` for creating multimodal embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a135eb",
   "metadata": {},
   "source": [
    "#### Deploy On-Premise Models\n",
    "\n",
    "This section deploys NIMs locally using Docker. Models will be cached to avoid re-downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model cache directory\n",
    "!mkdir -p ~/.cache/model-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390df52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MODEL_DIRECTORY environment variable to specify where models are cached\n",
    "import os\n",
    "\n",
    "os.environ[\"MODEL_DIRECTORY\"] = os.path.expanduser(\"~/.cache/model-cache\")\n",
    "print(\"MODEL_DIRECTORY set to:\", os.environ[\"MODEL_DIRECTORY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy NIMs with VLM and embedding profiles\n",
    "# ‚ö†Ô∏è WARNING: This may take 10-20 minutes as models download (~10GB+)\n",
    "# If the kernel times out, just rerun this cell - it will resume where it left off\n",
    "! USERID=$(id -u) docker compose --profile vlm-ingest --profile vlm-only -f ../deploy/compose/nims.yaml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the status of running containers\n",
    "# Run this cell repeatedly to check if all services are healthy\n",
    "# Look for STATUS showing \"healthy\" or \"Up\" for all containers\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb34a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model names and service URLs for the RAG pipeline\n",
    "# These settings tell the RAG server which models and endpoints to use\n",
    "\n",
    "# VLM (Vision-Language Model) configuration\n",
    "os.environ[\"APP_VLM_MODELNAME\"] = \"nvidia/nemotron-nano-12b-v2-vl\"\n",
    "os.environ[\"APP_VLM_SERVERURL\"] = \"http://vlm-ms:8000/v1\"\n",
    "\n",
    "# Multimodal embedding model configuration\n",
    "os.environ[\"APP_EMBEDDINGS_MODELNAME\"] = \"nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1\"\n",
    "os.environ[\"APP_EMBEDDINGS_SERVERURL\"] = \"nemoretriever-vlm-embedding-ms:8000/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c7037",
   "metadata": {},
   "source": [
    "#### Cloud based deployment\n",
    "Using NVIDIA hosted cloud model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82084d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR and document processing endpoints - cloud hosted\n",
    "os.environ[\"OCR_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-ocr\"\n",
    "os.environ[\"OCR_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"OCR_MODEL_NAME\"] = \"scene_text_ensemble\"\n",
    "os.environ[\"YOLOX_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\"\n",
    "os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"APP_NVINGEST_CAPTIONENDPOINTURL\"] = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "\n",
    "# VLM Model configuration - cloud hosted\n",
    "os.environ[\"APP_VLM_MODELNAME\"] = \"nvidia/nemotron-nano-12b-v2-vl\"\n",
    "os.environ[\"APP_VLM_SERVERURL\"] = \"https://integrate.api.nvidia.com/v1\"\n",
    "os.environ[\"APP_LLM_SERVERURL\"] = \"\"\n",
    "\n",
    "# Multimodal embedding model configuration - cloud hosted\n",
    "os.environ[\"APP_EMBEDDINGS_MODELNAME\"] = \"nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1\"\n",
    "os.environ[\"APP_EMBEDDINGS_SERVERURL\"] = \"https://integrate.api.nvidia.com/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbcfa50",
   "metadata": {},
   "source": [
    "### 4. Setup NVIDIA Ingest Runtime\n",
    "\n",
    "NVIDIA Ingest processes documents to extract text, images, and other elements. We'll configure it to:\n",
    "- Extract images from documents\n",
    "- Handle multimodal content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure NVIDIA Ingest to extract and process images from documents\n",
    "os.environ[\"APP_NVINGEST_STRUCTURED_ELEMENTS_MODALITY\"] = \"\"  # No special handling for structured elements\n",
    "os.environ[\"APP_NVINGEST_IMAGE_ELEMENTS_MODALITY\"] = \"image\"  # Process image elements as images\n",
    "os.environ[\"APP_NVINGEST_EXTRACTIMAGES\"] = \"True\"  # Extract images from documents\n",
    "\n",
    "# Start the ingestor server with Redis\n",
    "! docker compose -f ../deploy/compose/docker-compose-ingestor-server.yaml up -d --build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bd9a3",
   "metadata": {},
   "source": [
    "### 5. Setup the NVIDIA RAG Server\n",
    "\n",
    "The RAG server provides the main API endpoints for search and generation. It orchestrates all the components (embeddings, vector DB, VLM) to deliver intelligent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the RAG server (accessible at localhost:8081)\n",
    "os.environ[\"APP_RANKING_SERVERURL\"] = \"\"\n",
    "! docker compose -f ../deploy/compose/docker-compose-rag-server.yaml up -d --build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce492ce3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Document Ingestion Workflow\n",
    "\n",
    "Now that all services are running, let's ingest documents into a collection.\n",
    "\n",
    "### 6. Create a Collection\n",
    "\n",
    "A collection is a logical grouping of documents in the vector database. Think of it as a database table optimized for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8611aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install aiohttp for async HTTP requests\n",
    "! uv pip install aiohttp\n",
    "\n",
    "# Configure the ingestor server URL\n",
    "# Use \"ingestor-server\" when running in AI Workbench, otherwise \"localhost\"\n",
    "IPADDRESS = (\n",
    "    \"ingestor-server\"\n",
    "    if os.environ.get(\"AI_WORKBENCH\", \"false\") == \"true\"\n",
    "    else \"localhost\"\n",
    ")\n",
    "INGESTOR_SERVER_PORT = \"8082\"\n",
    "BASE_URL = f\"http://{IPADDRESS}:{INGESTOR_SERVER_PORT}\"\n",
    "\n",
    "async def print_response(response):\n",
    "    \"\"\"Helper function to pretty-print API responses.\"\"\"\n",
    "    try:\n",
    "        response_json = await response.json()\n",
    "        print(json.dumps(response_json, indent=2))\n",
    "    except aiohttp.ClientResponseError:\n",
    "        print(await response.text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688bc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a unique name for your collection\n",
    "# Change this if you want to create a different collection\n",
    "collection_name = \"multimodal_query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24378f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import json\n",
    "\n",
    "\n",
    "async def create_collection(\n",
    "    collection_name: str | None = None,\n",
    "    embedding_dimension: int = 2048,\n",
    "    metadata_schema: list = [],\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a new collection in the vector database.\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Unique identifier for the collection\n",
    "        embedding_dimension: Size of the embedding vectors (2048 for llama-3.2-nemoretriever-1b-vlm-embed-v1)\n",
    "        metadata_schema: Optional schema for metadata fields\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"embedding_dimension\": embedding_dimension,\n",
    "        \"metadata_schema\": metadata_schema,\n",
    "    }\n",
    "\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                f\"{BASE_URL}/v1/collection\", json=data, headers=HEADERS\n",
    "            ) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            return 500, {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Create the collection\n",
    "# The embedding dimension is 2048 for the multimodal embedding model we're using\n",
    "await create_collection(\n",
    "    collection_name=collection_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the documents to upload\n",
    "# This PDF contains product images with pricing information\n",
    "FILEPATHS = [\n",
    "    \"../data/multimodal/product_catalog.pdf\",\n",
    "]\n",
    "\n",
    "async def upload_documents(collection_name: str = \"\"):\n",
    "    \"\"\"\n",
    "    Upload and process documents into the collection.\n",
    "    \n",
    "    This will:\n",
    "    1. Extract text and images from the PDFs\n",
    "    2. Chunk the content for optimal retrieval\n",
    "    3. Generate multimodal embeddings\n",
    "    4. Store everything in the vector database\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"blocking\": False,  # Async upload - use status API to check progress\n",
    "        \"split_options\": {\n",
    "            \"chunk_size\": 512,        # Characters per chunk\n",
    "            \"chunk_overlap\": 150      # Overlap between chunks for context\n",
    "        },\n",
    "        \"generate_summary\": False  # Set to True to generate document summaries\n",
    "    }\n",
    "\n",
    "    form_data = aiohttp.FormData()\n",
    "    \n",
    "    # Add all PDF files to the form data\n",
    "    for file_path in FILEPATHS:\n",
    "        form_data.add_field(\"documents\", open(file_path, \"rb\"), \n",
    "                          filename=os.path.basename(file_path), \n",
    "                          content_type=\"application/pdf\")\n",
    "\n",
    "    form_data.add_field(\"data\", json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            # Use POST for new uploads, PATCH for re-ingesting existing documents\n",
    "            async with session.post(f\"{BASE_URL}/v1/documents\", data=form_data) as response:\n",
    "                await print_response(response)\n",
    "                response_json = await response.json()\n",
    "                return response_json\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error uploading documents: {e}\")\n",
    "            return None\n",
    "\n",
    "# Upload the documents and get the task ID for tracking progress\n",
    "upload_response = await upload_documents(collection_name=collection_name)\n",
    "task_id = upload_response.get(\"task_id\") if upload_response else None\n",
    "print(f\"\\nTask ID for tracking: {task_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_task_status(task_id: str):\n",
    "    \"\"\"\n",
    "    Check the status of an asynchronous ingestion task.\n",
    "    \n",
    "    Possible statuses:\n",
    "    - \"pending\": Task is queued\n",
    "    - \"processing\": Currently processing documents\n",
    "    - \"completed\": Successfully finished\n",
    "    - \"failed\": Error occurred\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"task_id\": task_id,\n",
    "    }\n",
    "\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(\n",
    "                f\"{BASE_URL}/v1/status\", params=params, headers=HEADERS\n",
    "            ) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            return 500, {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Check the ingestion status\n",
    "# Run this cell multiple times until status shows \"completed\"\n",
    "await get_task_status(\n",
    "    task_id=[task_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1f1a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Querying with Multimodal Inputs\n",
    "\n",
    "Now that documents are ingested, let's query them using both text and images!\n",
    "\n",
    "### 7. Using the Search and Generate APIs\n",
    "\n",
    "We'll demonstrate two approaches:\n",
    "1. **Search API**: Find relevant documents without generating a response\n",
    "2. **Generate API**: Get an AI-generated answer with citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990ca33",
   "metadata": {},
   "source": [
    "#### Prepare a Multimodal Query\n",
    "\n",
    "To query with an image, we need to:\n",
    "1. Convert the image to base64 encoding\n",
    "2. Format it according to the OpenAI vision API format\n",
    "3. Combine it with a text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dde830",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install requests httpx\n",
    "import base64\n",
    "import requests\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "def get_base64_image(image_source: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert an image to base64 encoding.\n",
    "    \n",
    "    Args:\n",
    "        image_source: Local file path or URL to the image\n",
    "        \n",
    "    Returns:\n",
    "        Base64 encoded string of the image\n",
    "    \"\"\"\n",
    "    if image_source.startswith(('http://', 'https://')):\n",
    "        # Download image from URL\n",
    "        response = requests.get(image_source)\n",
    "        return base64.b64encode(response.content).decode()\n",
    "    else:\n",
    "        # Read local file\n",
    "        with open(image_source, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode()\n",
    "\n",
    "# Convert the query image to base64\n",
    "# Try different images to test different queries:\n",
    "image_b64 = get_base64_image(\"../data/multimodal/Creme_clutch_purse1-small.jpg\")\n",
    "\n",
    "# Display the query image for reference\n",
    "query_image_path = \"../data/multimodal/Creme_clutch_purse1-small.jpg\"\n",
    "print(\"üì∑ Query Image:\")\n",
    "display(Image(filename=query_image_path, width=300))\n",
    "\n",
    "# Format as a data URL\n",
    "image_input = f\"data:image/png;base64,{image_b64}\"\n",
    "\n",
    "# Create the multimodal query with text + image\n",
    "# This follows the OpenAI vision API format\n",
    "query_1 = \"What material is this made of?\"\n",
    "image_query = [\n",
    "    {\"type\": \"text\", \"text\": query_1},\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": image_input,\n",
    "            \"detail\": \"auto\"  # Let the model decide the appropriate detail level\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "RAG_BASE_URL = \"http://localhost:8081\"\n",
    "\n",
    "async def search_documents(payload):\n",
    "    \"\"\"\n",
    "    Search for relevant documents using a multimodal query.\n",
    "    \n",
    "    This performs similarity search in the vector database and optionally\n",
    "    reranks results for better relevance.\n",
    "    \"\"\"\n",
    "    search_url = f\"{RAG_BASE_URL}/v1/search\"\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "        try:\n",
    "            response = await client.post(url=search_url, json=payload)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            search_results = response.json()\n",
    "            print(\"Search Results:\")\n",
    "            \n",
    "            # Display search results with nice formatting\n",
    "            if \"results\" in search_results:\n",
    "                for idx, result in enumerate(search_results[\"results\"]):\n",
    "                    doc_type = result.get(\"document_type\", \"text\")\n",
    "                    content = result.get(\"content\", \"\")\n",
    "                    doc_name = result.get(\"document_name\", f\"Result {idx + 1}\")\n",
    "                    score = result.get(\"score\", \"N/A\")\n",
    "                    \n",
    "                    display(Markdown(f\"**Result {idx + 1}: {doc_name} (Score: {score})**\"))\n",
    "                    try:\n",
    "                        if doc_type == \"image\":\n",
    "                            # Display image results\n",
    "                            image_bytes = base64.b64decode(content)\n",
    "                            display(Image(data=image_bytes))\n",
    "                        else:\n",
    "                            # Display text results\n",
    "                            display(Markdown(f\"```\\n{content}\\n```\"))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error displaying content: {e}\")\n",
    "                        display(Markdown(f\"```\\n{content}\\n```\"))\n",
    "            \n",
    "            return search_results\n",
    "            \n",
    "        except httpx.HTTPStatusError as e:\n",
    "            print(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n",
    "        except httpx.RequestError as e:\n",
    "            print(f\"An error occurred while requesting {e.request.url!r}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Configure the search parameters\n",
    "search_payload = {\n",
    "    \"query\": image_query,                      # Our multimodal query (text + image)\n",
    "    \"messages\": [],                            # No conversation history\n",
    "    \"use_knowledge_base\": True,                # Search the vector database\n",
    "    \"collection_names\": [collection_name],     # Which collection to search\n",
    "    \"vdb_top_k\": 5,                           # Retrieve top 5 results from vector DB\n",
    "    \"vdb_endpoint\": \"http://milvus:19530\",    # Milvus connection string\n",
    "    \"enable_reranker\": False,                  # Set to True for better relevance (slower)\n",
    "    \"reranker_top_k\": 3,                      # If reranker enabled, return top 3\n",
    "    \"filter_expr\": \"\",                        # Optional metadata filter\n",
    "}\n",
    "\n",
    "# Execute the search\n",
    "print(\"üîç Searching for documents matching the query...\\n\")\n",
    "search_result = await search_documents(search_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "\n",
    "async def print_streaming_response_and_citations(response_generator):\n",
    "    \"\"\"\n",
    "    Helper function to display streaming responses with citations.\n",
    "    \n",
    "    This function:\n",
    "    1. Streams the AI-generated response token by token\n",
    "    2. Extracts citations from the first chunk\n",
    "    3. Displays citations (text or images) after the response completes\n",
    "    \"\"\"\n",
    "    first_chunk_data = None\n",
    "    \n",
    "    async for chunk in response_generator:\n",
    "        # Parse Server-Sent Events (SSE) format\n",
    "        if chunk.startswith(\"data: \"):\n",
    "            chunk = chunk[len(\"data: \") :].strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            data = json.loads(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            continue\n",
    "            \n",
    "        choices = data.get(\"choices\", [])\n",
    "        if not choices:\n",
    "            continue\n",
    "            \n",
    "        # Save the first chunk with citations\n",
    "        if first_chunk_data is None and data.get(\"citations\"):\n",
    "            first_chunk_data = data\n",
    "            \n",
    "        # Print streaming text\n",
    "        delta = choices[0].get(\"delta\", {})\n",
    "        text = delta.get(\"content\")\n",
    "        if not text:\n",
    "            message = choices[0].get(\"message\", {})\n",
    "            text = message.get(\"content\", \"\")\n",
    "        print(text, end=\"\", flush=True)\n",
    "        \n",
    "    print()  # Newline after streaming\n",
    "\n",
    "    # Display citations after streaming is done\n",
    "    if first_chunk_data and first_chunk_data.get(\"citations\"):\n",
    "        print(\"\\nüìö Citations:\")\n",
    "        citations = first_chunk_data[\"citations\"]\n",
    "        for idx, citation in enumerate(citations.get(\"results\", [])):\n",
    "            doc_type = citation.get(\"document_type\", \"text\")\n",
    "            content = citation.get(\"content\", \"\")\n",
    "            doc_name = citation.get(\"document_name\", f\"Citation {idx + 1}\")\n",
    "            display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "            try:\n",
    "                # Try to display as image\n",
    "                image_bytes = base64.b64decode(content)\n",
    "                display(Image(data=image_bytes))\n",
    "            except Exception:\n",
    "                # Fall back to text display\n",
    "                display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31071359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "# Configure RAG server URL\n",
    "IPADDRESS = \"rag-server\" if os.environ.get(\"AI_WORKBENCH\", \"false\") == \"true\" else \"localhost\"\n",
    "RAG_SERVER_PORT = \"8081\"\n",
    "RAG_BASE_URL = f\"http://{IPADDRESS}:{RAG_SERVER_PORT}\"\n",
    "generate_url = f\"{RAG_BASE_URL}/v1/generate\"\n",
    "\n",
    "async def generate_answer(payload):\n",
    "    \"\"\"\n",
    "    Generate an AI answer using the RAG pipeline.\n",
    "    \n",
    "    This function:\n",
    "    1. Sends the query to the RAG server\n",
    "    2. Retrieves relevant context from the vector database\n",
    "    3. Streams the AI-generated response\n",
    "    4. Displays citations (sources) used to generate the answer\n",
    "    \"\"\"\n",
    "    rag_response = \"\"\n",
    "    citations = []\n",
    "    is_first_token = True\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "        try:\n",
    "            async with client.stream(\"POST\", url=generate_url, json=payload) as response:\n",
    "                # Raise an exception for bad status codes like 4xx or 5xx\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # Iterate over the streaming response\n",
    "                async for line in response.aiter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        json_str = line[6:].strip()\n",
    "                        if not json_str:\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            data = json.loads(json_str)\n",
    "\n",
    "                            # Extract and display the streaming response\n",
    "                            message = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                            if message:\n",
    "                                rag_response += message\n",
    "\n",
    "                            # Extract and display citations from the first chunk\n",
    "                            if is_first_token and data.get(\"citations\"):\n",
    "                                print(\"\\nüìö Citations:\")\n",
    "                                citations = data[\"citations\"]\n",
    "                                for idx, citation in enumerate(citations.get(\"results\", [])):\n",
    "                                    doc_type = citation.get(\"document_type\", \"text\")\n",
    "                                    content = citation.get(\"content\", \"\")\n",
    "                                    doc_name = citation.get(\"document_name\", f\"Citation {idx + 1}\")\n",
    "                                    display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "                                    try:\n",
    "                                        # Display image citations\n",
    "                                        image_bytes = base64.b64decode(content)\n",
    "                                        display(Image(data=image_bytes))\n",
    "                                    except Exception:\n",
    "                                        # Display text citations\n",
    "                                        display(Markdown(f\"```\\n{content}\\n```\"))\n",
    "                                is_first_token = False\n",
    "\n",
    "                            # Check if streaming is complete\n",
    "                            finish_reason = data.get(\"choices\", [{}])[0].get(\"finish_reason\")\n",
    "                            if finish_reason == \"stop\":\n",
    "                                return rag_response\n",
    "\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Skipping malformed JSON line: {json_str}\")\n",
    "                            continue\n",
    "        \n",
    "        except httpx.HTTPStatusError as e:\n",
    "            print(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n",
    "        except httpx.RequestError as e:\n",
    "            print(f\"An error occurred while requesting {e.request.url!r}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Response complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3049696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the query as a chat message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": image_query  # Our multimodal query (text + image)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the generate API parameters\n",
    "payload = {\n",
    "    \"messages\": messages,                      # Chat conversation\n",
    "    \"use_knowledge_base\": True,                # Enable RAG - use vector DB for context\n",
    "    \"temperature\": 0.2,                        # Lower = more deterministic, higher = more creative\n",
    "    \"top_p\": 0.7,                             # Nucleus sampling parameter\n",
    "    \"max_tokens\": 1024,                       # Maximum response length\n",
    "    \"reranker_top_k\": 2,                      # Keep top 2 results after reranking\n",
    "    \"vdb_top_k\": 10,                          # Retrieve top 10 from vector DB initially\n",
    "    \"vdb_endpoint\": \"http://milvus:19530\",    # Milvus connection\n",
    "    \"collection_names\": [collection_name],     # Which collection to search\n",
    "    \"enable_query_rewriting\": True,            # Improve query before searching\n",
    "    \"enable_citations\": True,                  # Include source citations in response\n",
    "    \"stop\": [],                               # Optional stop sequences\n",
    "    \"filter_expr\": \"\",                        # Optional metadata filter    \n",
    "}\n",
    "\n",
    "# Generate the answer with RAG\n",
    "print(\"ü§ñ Generating answer with RAG...\\n\")\n",
    "await generate_answer(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddc78d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "‚úÖ **Set up the infrastructure**: Deployed Milvus vector DB, NVIDIA NIMs, and RAG services  \n",
    "‚úÖ **Ingested multimodal documents**: Uploaded PDFs with images and extracted their content  \n",
    "‚úÖ **Created multimodal queries**: Combined text and images in your search queries  \n",
    "‚úÖ **Retrieved relevant context**: Used semantic search to find matching documents  \n",
    "‚úÖ **Generated AI responses**: Got intelligent answers with source citations  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Try different queries**: Change the query text or use different query images\n",
    "- **Upload more documents**: Add more PDFs to enrich your knowledge base\n",
    "- **Experiment with parameters**: Adjust `temperature`, `top_k`, reranker settings\n",
    "- **Build applications**: Integrate these APIs into your own applications\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "To stop all services and free up resources:\n",
    "\n",
    "```bash\n",
    "cd ../deploy/compose\n",
    "docker compose -f docker-compose-rag-server.yaml down\n",
    "docker compose -f docker-compose-ingestor-server.yaml down\n",
    "docker compose -f nims.yaml down\n",
    "docker compose -f vectordb.yaml down\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
