{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Retriever Integration – NvidiaRAGRetriever\n",
        "\n",
        "This notebook demonstrates how to use the **`NvidiaRAGRetriever`**, a LangChain-compatible retriever that connects to a running NVIDIA RAG Blueprint server.\n",
        "\n",
        "The retriever acts as a thin HTTP client — ingestion, embedding, vector search, and reranking all happen server-side.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. The **NVIDIA RAG Blueprint** is deployed and the RAG server is running (default port `8081`).\n",
        "2. Documents have already been **ingested** into at least one collection via the ingestion API or [ingestion notebook](./ingestion_api_usage.ipynb).\n",
        "3. Python **3.11+** is installed.\n",
        "\n",
        "---\n",
        "\n",
        "## Environment Setup (run once)\n",
        "\n",
        "Open a **terminal**, `cd` into the `rag/` directory, and run the commands below to create a virtual environment with the `nvidia_rag` package (which includes the retriever module).\n",
        "\n",
        "```bash\n",
        "# 1. Navigate to the rag directory (parent of this notebooks/ folder)\n",
        "cd /Users/rkharwar/Downloads/prd_retriever/rag\n",
        "\n",
        "# 2. Create a virtual environment\n",
        "python3 -m venv .venv\n",
        "\n",
        "# 3. Activate it\n",
        "source .venv/bin/activate\n",
        "\n",
        "# 4. Install nvidia_rag in editable mode (includes all retriever dependencies)\n",
        "pip install -e .\n",
        "\n",
        "# 5. Register the venv as a Jupyter kernel so this notebook can use it\n",
        "pip install ipykernel\n",
        "python -m ipykernel install --user --name nvidia-rag --display-name \"Python (nvidia-rag)\"\n",
        "```\n",
        "\n",
        "After running the above, come back to this notebook and **select the `Python (nvidia-rag)` kernel**:\n",
        "- Click the kernel name in the **top-right corner** of the notebook (e.g. \"Python 3\" or \".venv\")\n",
        "- Choose **Python (nvidia-rag)** from the list\n",
        "\n",
        "Then run the cells below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# If you followed the terminal setup above and selected the \"Python (nvidia-rag)\" kernel,\n",
        "# nvidia_rag is already installed and you can skip this cell.\n",
        "#\n",
        "# Otherwise, uncomment and run the line below to install into the current kernel:\n",
        "# %pip install -e \"..\" --quiet\n",
        "\n",
        "# Quick check that the package is importable:\n",
        "import nvidia_rag.retriever\n",
        "print(f\"nvidia_rag.retriever loaded from: {nvidia_rag.retriever.__file__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG Server URL : http://localhost:8081\n",
            "Collection     : test\n",
            "Auth           : Disabled\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "IPADDRESS = (\n",
        "    \"rag-server\" if os.environ.get(\"AI_WORKBENCH\", \"false\") == \"true\" else \"localhost\"\n",
        ")\n",
        "RAG_SERVER_PORT = \"8081\"\n",
        "BASE_URL = f\"http://{IPADDRESS}:{RAG_SERVER_PORT}\"\n",
        "\n",
        "# Optional: set your API key if the server requires authentication\n",
        "API_KEY = os.environ.get(\"NVIDIA_API_KEY\", None)\n",
        "\n",
        "# Collection to query (must already contain ingested documents)\n",
        "COLLECTION_NAME = \"test\"\n",
        "\n",
        "print(f\"RAG Server URL : {BASE_URL}\")\n",
        "print(f\"Collection     : {COLLECTION_NAME}\")\n",
        "print(f\"Auth           : {'Enabled' if API_KEY else 'Disabled'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Health Check\n",
        "\n",
        "Verify that the RAG server is reachable before using the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: 200\n",
            "{'message': 'Service is up.', 'databases': [], 'object_storage': [], 'nim': []}\n"
          ]
        }
      ],
      "source": [
        "import httpx\n",
        "\n",
        "resp = httpx.get(f\"{BASE_URL}/v1/health\")\n",
        "print(f\"Status: {resp.status_code}\")\n",
        "print(resp.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Basic Usage – Instantiate the Retriever\n",
        "\n",
        "`NvidiaRAGRetriever` subclasses LangChain's `BaseRetriever`, so it supports\n",
        "`.invoke()`, `.ainvoke()`, `.batch()`, and can be composed in LangChain chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/rkharwar/Downloads/prd_retriever/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retriever ready — will query collection 'test' with top_k=5\n"
          ]
        }
      ],
      "source": [
        "from nvidia_rag.retriever import NvidiaRAGRetriever\n",
        "\n",
        "retriever = NvidiaRAGRetriever(\n",
        "    base_url=BASE_URL,\n",
        "    api_key=API_KEY,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    top_k=5,\n",
        ")\n",
        "\n",
        "print(f\"Retriever ready — will query collection '{COLLECTION_NAME}' with top_k=5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Retrieve Documents\n",
        "\n",
        "Call `.invoke()` with a natural-language query. The server handles embedding,\n",
        "vector search, and reranking. Results come back as LangChain `Document` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST http://localhost:8081/v1/search \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What is AI?\n",
            "Retrieved 5 documents\n",
            "\n",
            "--- Document 1 (score: 0.7130321352710473) ---\n",
            "Source : sample_ai_article_extracted.pdf\n",
            "Page   : 1\n",
            "Content: Extracted Content: sample_ai_article.pdf\n",
            "Total extracted items: 640\n",
            " - image: 513\n",
            " - structured: 20\n",
            " - text: 107\n",
            "[Item 1] Type: text\n",
            "Artificial intelligence\n",
            "Artificial intelligence (AI) is the ...\n",
            "\n",
            "--- Document 2 (score: 0.6892219530281222) ---\n",
            "Source : sample_ai_article_extracted.pdf\n",
            "Page   : 274\n",
            "Content: Extracted Content: sample_ai_article.pdf\n",
            "AI itself, with particular emphasis on influential papers and foundational texts that introduced or advanced key concepts in AI. Notably, McCarthy\n",
            "(1999) and...\n",
            "\n",
            "--- Document 3 (score: 0.6892219530281222) ---\n",
            "Source : sample_ai_article_extracted.pdf\n",
            "Page   : 156\n",
            "Content: kowski, Nicole (November 2023). \"What is Artificial Intelligence and How Does AI Work?\n",
            "TechTarget\" (https://www.techtarget.com/searchenterpriseai/definition/AI-Artificial-Intelligenc\n",
            "e). Enterprise ...\n",
            "\n",
            "--- Document 4 (score: 0.6768762581597325) ---\n",
            "Source : sample_ai_article_extracted.pdf\n",
            "Page   : 143\n",
            "Content: -turing-test-fo\n",
            "r-intelligence-what-happens-when-it-does\u0002214721). The Conversation. Archived (http\n",
            "s://web.archive.org/web/20240925040612/\n",
            "https://theconversation.com/ai-is-closer-tha\n",
            "n-ever-to-pa...\n",
            "\n",
            "--- Document 5 (score: 0.6515322590753402) ---\n",
            "Source : sample_ai_article_extracted.pdf\n",
            "Page   : 1\n",
            "Content: b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4]\n",
            "Some companies, such as OpenAI, Google DeepMind and Meta,\n",
            "[5] aim to create artificial general\n",
            "intellig...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"What is AI?\"\n",
        "docs = retriever.invoke(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Retrieved {len(docs)} documents\\n\")\n",
        "\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"--- Document {i+1} (score: {doc.metadata.get('score', 'N/A')}) ---\")\n",
        "    print(f\"Source : {doc.metadata.get('source', 'unknown')}\")\n",
        "    print(f\"Page   : {doc.metadata.get('page_number', 'N/A')}\")\n",
        "    print(f\"Content: {doc.page_content[:200]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Async Retrieval\n",
        "\n",
        "Use `.ainvoke()` for non-blocking retrieval in async applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST http://localhost:8081/v1/search \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.17] Extracted Content: sample_ai_article.pdf\n",
            "[Item 47] Type: image\n",
            "Caption: performing similar functions . The development...\n",
            "[0.17] /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQH/...\n",
            "[0.17] /arxiv.org/archive/cs.LG)].\n",
            "152. Franzen, Carl (8 August 2024). \"Alibaba\n",
            "claims no. 1 spot in AI math models with\n",
            "Qwe...\n",
            "[0.15] center for US$650 million.[232] Nvidia CEO Jensen Huang said\n",
            "...lear power is a good option for the data centers.[233]\n",
            "[0.15] outputs.[110] The multiple layers can progressively extract\n",
            "higher-level features from the raw input. For example, in i...\n"
          ]
        }
      ],
      "source": [
        "docs = await retriever.ainvoke(\"What is AI\")\n",
        "\n",
        "for doc in docs:\n",
        "    print(f\"[{doc.metadata.get('score', 0):.2f}] {doc.page_content[:120]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Using Metadata Filters\n",
        "\n",
        "Pass a filter expression to narrow the search to specific documents or metadata.\n",
        "The filter syntax depends on the vector store backend (Milvus or Elasticsearch).\n",
        "\n",
        "### Milvus filter (string expression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST http://localhost:8081/v1/search \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 2 documents with domain ==education\n",
            "  Page 2: Parent Portal: Weekly progress reports, upcoming assessment alerts, and recommen...\n",
            "  Page 1: Product Requirements Document — AI-Powered Adaptive Learning Platform\n",
            "1. Execut...\n"
          ]
        }
      ],
      "source": [
        "COLLECTION_NAME=\"metatest\"\n",
        "filtered_retriever = NvidiaRAGRetriever(\n",
        "    base_url=BASE_URL,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    top_k=3,\n",
        "    filters='content_metadata[\"domain\"] == \"education\"',\n",
        ")\n",
        "\n",
        "docs = filtered_retriever.invoke(\"deep learning optimization\")\n",
        "print(f\"Retrieved {len(docs)} documents with domain ==education\")\n",
        "for doc in docs:\n",
        "    print(f\"  Page {doc.metadata.get('page_number')}: {doc.page_content[:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Configuring Reranker and Query Rewriting\n",
        "\n",
        "You can control server-side reranking and query rewriting per retriever instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST http://localhost:8081/v1/search \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 5 reranked documents\n",
            "  [0.114] Product Requirements Document — AI-Powered Adaptive Learning Platform\n",
            "1. Executive Summary\n",
            "This do...\n",
            "  [0.114] Parent Portal: Weekly progress reports, upcoming assessment alerts, and recommended\n",
            "home-practice a...\n",
            "  [0.114] Product Requirements Document — Omnichannel Retail Commerce Platform\n",
            "1. Executive Summary\n",
            "This doc...\n",
            "  [0.114] Product Requirements Document — Personal Finance & Wealth Management App\n",
            "1. Executive Summary\n",
            "This...\n",
            "  [0.114] Product Requirements Document — Remote Patient Monitoring & Telehealth Platform\n",
            "1. Executive Summar...\n"
          ]
        }
      ],
      "source": [
        "# High-recall retriever: fetch many candidates, rerank to top 5\n",
        "precise_retriever = NvidiaRAGRetriever(\n",
        "    base_url=BASE_URL,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    top_k=5,\n",
        "    vdb_top_k=200,\n",
        "    enable_reranker=True,\n",
        "    enable_query_rewriting=True,\n",
        ")\n",
        "\n",
        "docs = precise_retriever.invoke(\"GPU memory management best practices\")\n",
        "print(f\"Retrieved {len(docs)} reranked documents\")\n",
        "for doc in docs:\n",
        "    print(f\"  [{doc.metadata.get('score', 0):.3f}] {doc.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Use in a LangChain RAG Chain\n",
        "\n",
        "Compose the retriever with an LLM in a standard LangChain chain.\n",
        "This example uses `ChatNVIDIA` from `langchain-nvidia-ai-endpoints`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uncomment the chain code above after configuring your LLM.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    \"\"\"Join retrieved documents into a single context string.\"\"\"\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer the question based only on the following context:\\n\\n{context}\",\n",
        "        ),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Uncomment and configure the LLM of your choice:\n",
        "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "# llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "\n",
        "# chain = (\n",
        "#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "#     | prompt\n",
        "#     | llm\n",
        "#     | StrOutputParser()\n",
        "# )\n",
        "\n",
        "# response = chain.invoke(\"What are the benefits of using CUDA?\")\n",
        "# print(response)\n",
        "\n",
        "print(\"Uncomment the chain code above after configuring your LLM.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Batch Retrieval\n",
        "\n",
        "LangChain's `BaseRetriever` provides `.batch()` for retrieving multiple queries at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"What is AI?\",\n",
        "    \"How does TensorRT work?\",\n",
        "    \"Explain GPU parallel computing\",\n",
        "]\n",
        "\n",
        "batch_results = retriever.batch(queries)\n",
        "\n",
        "for query, docs in zip(queries, batch_results):\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"  → {len(docs)} documents retrieved\")\n",
        "    if docs:\n",
        "        print(f\"  → Top result: {docs[0].page_content[:80]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Inspecting Document Metadata\n",
        "\n",
        "Each returned `Document` carries rich metadata from the RAG server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "docs = retriever.invoke(\"neural network training\")\n",
        "\n",
        "if docs:\n",
        "    doc = docs[0]\n",
        "    print(\"=== Top Document ===\")\n",
        "    print(f\"Content (first 300 chars):\\n{doc.page_content[:300]}\\n\")\n",
        "    print(\"Metadata:\")\n",
        "    print(json.dumps(doc.metadata, indent=2, default=str))\n",
        "else:\n",
        "    print(\"No documents found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Use as a LangChain Tool (Agent Integration)\n",
        "\n",
        "Wrap the retriever as a tool so a LangChain agent can decide when to search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import create_retriever_tool\n",
        "\n",
        "search_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    name=\"nvidia_docs_search\",\n",
        "    description=\"Search NVIDIA documentation for technical information about GPUs, CUDA, TensorRT, and related topics.\",\n",
        ")\n",
        "\n",
        "# Test the tool directly\n",
        "result = search_tool.invoke(\"CUDA memory allocation\")\n",
        "print(f\"Tool returned {len(result)} characters of context\")\n",
        "print(result[:300], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Cleanup\n",
        "\n",
        "Close the HTTP connections when done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever.close()\n",
        "print(\"Retriever connections closed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Reference\n",
        "\n",
        "| Parameter | Type | Default | Description |\n",
        "|---|---|---|---|\n",
        "| `base_url` | `str` | *required* | Root URL of the NVIDIA RAG server |\n",
        "| `api_key` | `str \\| None` | `None` | Bearer token for authentication |\n",
        "| `collection_name` | `str \\| None` | `None` | Collection to search (server default if None) |\n",
        "| `top_k` | `int` | `10` | Max documents after reranking (1–25) |\n",
        "| `vdb_top_k` | `int` | `100` | Candidates from vector DB before reranking (1–400) |\n",
        "| `filters` | `str \\| list` | `\"\"` | Vector store filter expression |\n",
        "| `enable_reranker` | `bool \\| None` | `None` | Enable server-side reranking |\n",
        "| `enable_query_rewriting` | `bool \\| None` | `None` | Enable server-side query rewriting |\n",
        "| `embedding_model` | `str \\| None` | `None` | Override embedding model name |\n",
        "| `reranker_model` | `str \\| None` | `None` | Override reranker model name |\n",
        "| `timeout` | `float` | `60.0` | HTTP timeout in seconds |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
