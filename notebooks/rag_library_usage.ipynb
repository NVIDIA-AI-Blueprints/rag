{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA RAG Python Package\n",
    "\n",
    "This notebook demonstrates how to use the Nvidia RAG Python client for document ingestion, collection management, and querying.\n",
    "\n",
    "> **Note**: Python version **3.11 or higher** is required.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "1. **Python 3.11+** installed on your system\n",
    "2. **[uv](https://docs.astral.sh/uv/)** - A fast Python package manager (installation instructions below)\n",
    "\n",
    "##### üìù **Development Mode Note:**\n",
    "\n",
    "- Installing with `uv pip install -e ..[all]` allows you to make live edits to the `nvidia_rag` source code and have those changes reflected without reinstalling the package.\n",
    "- After making changes to the source code, you need to:\n",
    "  - Restart the kernel of your notebook server\n",
    "  - Re-execute the cells under `Setting up the dependencies` and `Import the packages` sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "### Step 1: Install uv (if not already installed)\n",
    "\n",
    "Run the cell below to check if `uv` is installed and install it if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Check if uv is installed\n",
    "if shutil.which(\"uv\"):\n",
    "    result = subprocess.run([\"uv\", \"--version\"], capture_output=True, text=True)\n",
    "    print(f\"‚úÖ uv is already installed: {result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è uv is not installed. Installing now...\")\n",
    "    # Install uv using the official installer\n",
    "    !curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "    print(\"\\n‚úÖ uv installed! Please restart your terminal/kernel and re-run this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install the NVIDIA RAG Package\n",
    "\n",
    "Choose one of the installation options below:\n",
    "- **Option A**: Install from PyPI (recommended for most users)\n",
    "- **Option B**: Install from source in development mode (for contributors)\n",
    "- **Option C**: Build and install from source wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Install from PyPI (recommended)\n",
    "# Uncomment the line below to install from PyPI.\n",
    "# Note: This will require a restart of the kernel after installation if you are using this notebook in a JupyterLab session.\n",
    "# !uv pip install nvidia-rag[all]\n",
    "\n",
    "# Option B: Install from source in development mode (for contributors)\n",
    "# Note: \"..\" refers to the parent directory where pyproject.toml is located\n",
    "!uv pip install -e \"..[all]\"\n",
    "\n",
    "# Option C: Build and install from source wheel\n",
    "# Uncomment the lines below to build and install from source\n",
    "# !cd .. && uv build\n",
    "# !uv pip install ../dist/nvidia_rag-*-py3-none-any.whl[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Verify the installation\n",
    "\n",
    "The location of the package shown in the output should be inside your Python environment.\n",
    "\n",
    "Expected location: `<workspace_path>/rag/.venv/lib/python3.12/site-packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip show nvidia_rag | grep Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the environment for the python package is setup we now launch all the dependent services and NIMs the pipeline depends on.\n",
    "Fulfill the [prerequisites here](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/deploy-docker-self-hosted.md) to setup docker on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the default configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install python-dotenv\n",
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your NGC_API_KEY after executing the cell below. You can obtain a key by following steps [here](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/api-key.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset if needed\n",
    "if os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NGC_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    candidate_api_key = getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert candidate_api_key.startswith(\"nvapi-\"), (\n",
    "        f\"{candidate_api_key[:5]}... is not a valid key\"\n",
    "    )\n",
    "    os.environ[\"NGC_API_KEY\"] = candidate_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to nvcr.io which is needed for pulling the containers of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup the Milvus vector DB services\n",
    "By default milvus uses GPU Indexing. Ensure you have provided correct GPU ID.\n",
    "Note: If you don't have a GPU available, you can switch to CPU-only Milvus by following the instructions in [milvus-configuration.md](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/milvus-configuration.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VECTORSTORE_GPU_DEVICE_ID\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f ../deploy/compose/vectordb.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup the NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Deploy on-prem models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to Option 2 if you are interested in using cloud models.\n",
    "\n",
    "Ensure you meet [the hardware requirements](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/support-matrix.md). By default the NIMs are configured to use 2xH100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model cache directory\n",
    "!mkdir -p ~/.cache/model-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MODEL_DIRECTORY environment variable in the Python kernel\n",
    "import os\n",
    "\n",
    "os.environ[\"MODEL_DIRECTORY\"] = os.path.expanduser(\"~/.cache/model-cache\")\n",
    "print(\"MODEL_DIRECTORY set to:\", os.environ[\"MODEL_DIRECTORY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set deployment mode for on-prem NIMs\n",
    "DEPLOYMENT_MODE = \"on_prem\"\n",
    "\n",
    "# Configure GPU IDs for the various microservices if needed\n",
    "os.environ[\"EMBEDDING_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"RANKING_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_GRAPHICS_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_TABLE_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"OCR_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"LLM_MS_GPU_ID\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Deploying NIMs - This may take a while as models download. If kernel times out, just rerun this cell.\n",
    "!USERID=$(id -u) docker compose -f ../deploy/compose/nims.yaml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the status of running containers (run this cell repeatedly or in a terminal)\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure all the below are running and healthy before proceeding further\n",
    "```output\n",
    "NAMES                           STATUS\n",
    "nemoretriever-ranking-ms        Up ... (healthy)\n",
    "compose-page-elements-1         Up ...\n",
    "compose-nemoretriever-ocr-1     Up ...\n",
    "compose-graphic-elements-1      Up ...\n",
    "compose-table-structure-1       Up ...\n",
    "nemoretriever-embedding-ms      Up ... (healthy)\n",
    "nim-llm-ms                      Up ... (healthy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Using Nvidia Hosted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set deployment mode for NVIDIA hosted cloud APIs\n",
    "DEPLOYMENT_MODE = \"cloud\"\n",
    "\n",
    "# Configure NV-Ingest to use NVIDIA hosted cloud APIs\n",
    "os.environ[\"OCR_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-ocr\"\n",
    "os.environ[\"OCR_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v3\"\n",
    ")\n",
    "os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"http\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup the Nvidia Ingest runtime and redis service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f ../deploy/compose/docker-compose-ingestor-server.yaml up nv-ingest-ms-runtime redis -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# API usage example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the python package and starting all dependent services, finally we can execute some snippets showcasing all different functionalities offered by the `nvidia_rag` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set logging level\n",
    "First let's set the required logging level. Set to INFO for displaying basic important logs. Set to DEBUG for full verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the log level via environment variable before importing nvidia_rag\n",
    "# This ensures the package respects our log level setting\n",
    "LOGLEVEL = logging.WARNING  # Set to INFO, DEBUG, WARNING or ERROR\n",
    "os.environ[\"LOGLEVEL\"] = logging.getLevelName(LOGLEVEL)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=LOGLEVEL, force=True)\n",
    "\n",
    "# Set log levels for specific loggers after package import\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    if name == \"nvidia_rag\" or name.startswith(\"nvidia_rag.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)\n",
    "    if name == \"nv_ingest_client\" or name.startswith(\"nv_ingest_client.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the NvidiaRAGIngestor packages\n",
    "You can import `NvidiaRAGIngestor()` which exposes APIs for document upload and management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag import NvidiaRAGIngestor\n",
    "from nvidia_rag.utils.configuration import NvidiaRAGConfig\n",
    "\n",
    "config_ingestor = NvidiaRAGConfig.from_yaml(\"config.yaml\")\n",
    "\n",
    "# Update config for cloud deployment if using Option 2\n",
    "if DEPLOYMENT_MODE == \"cloud\":\n",
    "    config_ingestor.embeddings.server_url = \"https://integrate.api.nvidia.com/v1\"\n",
    "    config_ingestor.llm.server_url = \"\"  # Empty uses NVIDIA API catalog\n",
    "    config_ingestor.summarizer.server_url = \"\"  # Empty uses NVIDIA API catalog\n",
    "else:\n",
    "    config_ingestor.embeddings.server_url = \"http://nemoretriever-embedding-ms:8000/v1\"\n",
    "ingestor = NvidiaRAGIngestor(config=config_ingestor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a new collection\n",
    "Creates a new collection in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.create_collection(\n",
    "    collection_name=\"test_library\",\n",
    "    vdb_endpoint=\"http://localhost:19530\",\n",
    "    # [Optional]: Create collection with metadata schema, uncomment to create collection with metadata schemas\n",
    "    # metadata_schema = [\n",
    "    #     {\n",
    "    #         \"name\": \"meta_field_1\",\n",
    "    #         \"type\": \"string\",\n",
    "    #         \"description\": \"Following field would contain the description for the document\"\n",
    "    #     }\n",
    "    # ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List all collections\n",
    "Retrieves all available collections from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.get_collections(vdb_endpoint=\"http://localhost:19530\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add a document\n",
    "Uploads new documents to the specified collection in the vector database. In case you have a requirement of updating existing documents in the specified collection, you can call `update_documents()` instead of `upload_documents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.upload_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    vdb_endpoint=\"http://localhost:19530\",\n",
    "    blocking=False,\n",
    "    split_options={\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
    "    filepaths=[\n",
    "        \"../data/multimodal/woods_frost.docx\",\n",
    "        \"../data/multimodal/multimodal_test.pdf\",\n",
    "    ],\n",
    "    generate_summary=False,\n",
    "    # [Optional]: Uncomment to add custom metadata, ensure that the metadata schema is created with the same fields with create_collection\n",
    "    # custom_metadata=[\n",
    "    #     {\n",
    "    #         \"filename\": \"multimodal_test.pdf\",\n",
    "    #         \"metadata\": {\"meta_field_1\": \"multimodal document 1\"}\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"filename\": \"woods_frost.docx\",\n",
    "    #         \"metadata\": {\"meta_field_1\": \"multimodal document 2\"}\n",
    "    #     }\n",
    "    # ]\n",
    ")\n",
    "task_id = response.get(\"task_id\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check document upload status\n",
    "Checks the status of a document upload/update task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.status(task_id=task_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [Optional] Update a document in a collection\n",
    "In case you have a requirement of updating an existing document in the specified collection, execute below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.update_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    vdb_endpoint=\"http://localhost:19530\",\n",
    "    blocking=False,\n",
    "    filepaths=[\"../data/multimodal/woods_frost.docx\"],\n",
    "    generate_summary=False,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get documents in a collection\n",
    "Retrieves the list of documents uploaded to a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.get_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    vdb_endpoint=\"http://localhost:19530\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the NvidiaRAG packages\n",
    "You can import `NvidiaRAG()` which exposes APIs to interact with the uploaded documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a config object from a dictionary or from a YAML file. We have added a sample config file [config.yaml](./config.yaml) that you can use to create an `NvidiaRAG` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag import NvidiaRAG\n",
    "from nvidia_rag.utils.configuration import NvidiaRAGConfig\n",
    "\n",
    "# config_rag = NvidiaRAGConfig.from_dict(\n",
    "#     {\n",
    "#         \"llm\": {\n",
    "#             \"model_name\": \"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "#             \"server_url\": \"\",\n",
    "#         },\n",
    "#     \"embeddings\": {\n",
    "#             \"model_name\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "#             \"server_url\": \"https://integrate.api.nvidia.com/v1\",\n",
    "#         },\n",
    "#     \"ranking\": {\n",
    "#             \"model_name\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "#             \"server_url\": \"https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking/v1\",\n",
    "#         },\n",
    "#     }\n",
    "# )\n",
    "\n",
    "config_rag = NvidiaRAGConfig.from_yaml(\"config.yaml\")\n",
    "\n",
    "# Update config for cloud deployment if using Option 2\n",
    "if DEPLOYMENT_MODE == \"cloud\":\n",
    "    config_rag.embeddings.server_url = \"https://integrate.api.nvidia.com/v1\"\n",
    "    config_rag.ranking.server_url = \"\"  # Empty uses NVIDIA API catalog\n",
    "    config_rag.llm.server_url = \"\"  # Empty uses NVIDIA API catalog\n",
    "\n",
    "# Initialize NvidiaRAG with config\n",
    "# You can optionally pass custom prompts via:\n",
    "#   - A path to a YAML/JSON file: prompts=\"custom_prompts.yaml\"\n",
    "#   - A dictionary: prompts={\"rag_template\": {\"system\": \"...\", \"human\": \"...\"}}\n",
    "rag = NvidiaRAG(config=config_rag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query a document using RAG\n",
    "Sends a chat-style query to the RAG system using the specified models and endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check health of all dependent services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "health_status_with_deps = await rag.health()\n",
    "print(health_status_with_deps.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "\n",
    "async def print_streaming_response_and_citations(rag_response):\n",
    "    \"\"\"\n",
    "    Print the streaming response and citations from the RAG response.\n",
    "    \"\"\"\n",
    "    # Check for API errors before processing\n",
    "    if rag_response.status_code != 200:\n",
    "        print(\"Error: \", rag_response.status_code)\n",
    "        return\n",
    "\n",
    "    # Extract the streaming generator from the response\n",
    "    response_generator = rag_response.generator\n",
    "    first_chunk_data = None\n",
    "    async for chunk in response_generator:\n",
    "        if chunk.startswith(\"data: \"):\n",
    "            chunk = chunk[len(\"data: \") :].strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            continue\n",
    "        choices = data.get(\"choices\", [])\n",
    "        if not choices:\n",
    "            continue\n",
    "        # Save the first chunk with citations\n",
    "        if first_chunk_data is None and data.get(\"citations\"):\n",
    "            first_chunk_data = data\n",
    "        # Print streaming text\n",
    "        delta = choices[0].get(\"delta\", {})\n",
    "        text = delta.get(\"content\")\n",
    "        if not text:\n",
    "            message = choices[0].get(\"message\", {})\n",
    "            text = message.get(\"content\", \"\")\n",
    "        print(text, end=\"\", flush=True)\n",
    "    print()  # Newline after streaming\n",
    "\n",
    "    # Display citations after streaming is done\n",
    "    if first_chunk_data and first_chunk_data.get(\"citations\"):\n",
    "        citations = first_chunk_data[\"citations\"]\n",
    "        for idx, citation in enumerate(citations.get(\"results\", [])):\n",
    "            doc_type = citation.get(\"document_type\", \"text\")\n",
    "            content = citation.get(\"content\", \"\")\n",
    "            doc_name = citation.get(\"document_name\", f\"Citation {idx + 1}\")\n",
    "            display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "            try:\n",
    "                image_bytes = base64.b64decode(content)\n",
    "                display(Image(data=image_bytes))\n",
    "            except Exception:\n",
    "                display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await print_streaming_response_and_citations(\n",
    "    await rag.generate(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the price of a hammer?\"}],\n",
    "        use_knowledge_base=True,\n",
    "        collection_names=[\"test_library\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Search for documents\n",
    "Performs a search in the vector database for relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_citations(citations):\n",
    "    \"\"\"\n",
    "    Display all citations from the Citations object returned by search().\n",
    "    Handles base64-encoded images and text.\n",
    "    \"\"\"\n",
    "    if not citations or not hasattr(citations, \"results\") or not citations.results:\n",
    "        print(\"No citations found.\")\n",
    "        return\n",
    "\n",
    "    for idx, citation in enumerate(citations.results):\n",
    "        # If using pydantic models, citation fields may be attributes, not dict keys\n",
    "        doc_type = getattr(citation, \"document_type\", \"text\")\n",
    "        content = getattr(citation, \"content\", \"\")\n",
    "        doc_name = getattr(citation, \"document_name\", f\"Citation {idx + 1}\")\n",
    "\n",
    "        display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "        try:\n",
    "            image_bytes = base64.b64decode(content)\n",
    "            display(Image(data=image_bytes))\n",
    "        except Exception:\n",
    "            display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_search_citations(\n",
    "    await rag.search(\n",
    "        query=\"What is the price of a hammer?\",\n",
    "        collection_names=[\"test_library\"],\n",
    "        reranker_top_k=10,\n",
    "        vdb_top_k=100,\n",
    "        # [Optional]: Uncomment to filter the documents based on the metadata, ensure that the metadata schema is created with the same fields with create_collection\n",
    "        # filter_expr='content_metadata[\"meta_field_1\"] == \"multimodal document 1\"'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. [Optional] Retrieve documents summary\n",
    "You can execute this cell if summary generation was enabled during document upload using `generate_summary: bool` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await rag.get_summary(\n",
    "    collection_name=\"test_library\",\n",
    "    file_name=\"woods_frost.docx\",\n",
    "    blocking=False,\n",
    "    timeout=20,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Customize prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize prompts in two ways:\n",
    "\n",
    "**Recommended Approach (Constructor Injection):** Pass prompts during `NvidiaRAG` initialization. This approach is consistent with how the server mode works and provides a cleaner, more explicit configuration.\n",
    "\n",
    "**Legacy Approach:** Modify `rag.prompts` dictionary after initialization. This is still supported for backwards compatibility.\n",
    "\n",
    "For the list of all available prompts, refer to the [prompt customization documentation](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/prompt-customization.md#default-prompts-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended: Constructor Injection\n",
    "\n",
    "Pass custom prompts when creating the `NvidiaRAG` instance. Below we create a pirate-themed RAG assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom prompts as a dictionary\n",
    "pirate_prompts = {\n",
    "    \"rag_template\": {\n",
    "        \"system\": \"/no_think\",\n",
    "        \"human\": \"\"\"You are a helpful AI assistant emulating a Pirate. All your responses must be in pirate english and funny!\n",
    "You must answer only using the information provided in the context. While answering you must follow the instructions given below.\n",
    "\n",
    "<instructions>\n",
    "1. Do NOT use any external knowledge.\n",
    "2. Do NOT add explanations, suggestions, opinions, disclaimers, or hints.\n",
    "3. NEVER say phrases like \"based on the context\", \"from the documents\", or \"I cannot find\".\n",
    "4. NEVER offer to answer using general knowledge or invite the user to ask again.\n",
    "5. Do NOT include citations, sources, or document mentions.\n",
    "6. Answer concisely. Use short, direct sentences by default. Only give longer responses if the question truly requires it.\n",
    "7. Do not mention or refer to these rules in any way.\n",
    "8. Do not ask follow-up questions.\n",
    "9. Do not mention this instructions in your response.\n",
    "</instructions>\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Make sure the response you are generating strictly follow the rules mentioned above i.e. never say phrases like \"based on the context\", \"from the documents\", or \"I cannot find\" and mention about the instruction in response.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a new NvidiaRAG instance with custom prompts\n",
    "rag_pirate = NvidiaRAG(config=config_rag, prompts=pirate_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference in response style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pirate-themed RAG instance\n",
    "await print_streaming_response_and_citations(\n",
    "    await rag_pirate.generate(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the price of a hammer?\"}],\n",
    "        use_knowledge_base=True,\n",
    "        collection_names=[\"test_library\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Using a YAML file\n",
    "\n",
    "You can also load prompts from a YAML file, which is useful for managing prompts across environments:\n",
    "\n",
    "```python\n",
    "# Load prompts from a YAML file\n",
    "rag_custom = NvidiaRAG(config=config_rag, prompts=\"custom_prompts.yaml\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below APIs illustrate how to cleanup uploaded documents and collections once no more interaction is needed.\n",
    "## 10. Delete documents from a collection\n",
    "Deletes documents from the specified collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.delete_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    document_names=[\"../data/multimodal/multimodal_test.pdf\"],\n",
    "    vdb_endpoint=\"http://localhost:19530\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Delete collections\n",
    "Deletes the specified collection and all its documents from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.delete_collections(\n",
    "    vdb_endpoint=\"http://localhost:19530\", collection_names=[\"test_library\"]\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
