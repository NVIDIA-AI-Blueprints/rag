{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA RAG Python Package - Lite Mode\n",
    "\n",
    "This notebook demonstrates the **containerless deployment** of the NVIDIA RAG Python package, where all operations are performed purely in Python without requiring Docker containers.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Containerless Deployment**: All functionality is accessible through Python APIs without external services\n",
    "- **Simplified Setup**: Uses Milvus Lite (embedded vector database) and NV-Ingest subprocess mode\n",
    "- **Cloud-Based Processing**: Leverages NVIDIA cloud APIs for embeddings, ranking, and LLM inference\n",
    "\n",
    "### Important Limitations\n",
    "\n",
    "> **⚠️ Citation Limitations**: This lite mode does not support **image, table, or chart citations** since the Minio object storage service is not deployed. Only text-based citations are available.\n",
    "\n",
    "> **⚠️ Summary Generation**: Document summary generation is **not supported** in lite mode.\n",
    "\n",
    "> **Note**: You may encounter warnings related to citations and summarization during execution. These warnings are expected in lite mode and can be safely ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for NVIDIA RAG Python Package\n",
    "Please refer to [rag_library_usage.ipynb](./rag_library_usage.ipynb) for detailed installation instructions for the RAG library.\n",
    "**Quick install:**\n",
    "```bash\n",
    "uv pip install nvidia-rag[all]\n",
    "```\n",
    "\n",
    "Install nv-ingest library using below command - **OR** - Run the cell below if Jupyter notebook is started in the same environment:\n",
    "```bash\n",
    "uv pip install nv-ingest==26.1.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the NVIDIA RAG Package and NV-Ingest Library\n",
    "\n",
    "Run the cell below to install the required packages if not already installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option A: Install from PyPI (recommended)\n",
    "# Uncomment the line below to install from PyPI\n",
    "# !uv pip install nvidia-rag[all]\n",
    "\n",
    "# Option B: Install from source in development mode (for contributors)\n",
    "# Note: \"..\" refers to the parent directory where pyproject.toml is located\n",
    "!uv pip install -e \"..[all]\"\n",
    "\n",
    "# Option C: Build and install from source wheel\n",
    "# Uncomment the lines below to build and install from source\n",
    "# !cd .. && uv build\n",
    "# !uv pip install ../dist/nvidia_rag-*-py3-none-any.whl[all]\n",
    "\n",
    "# Install NV-Ingest library in the same environment to run NV-Ingest pipeline\n",
    "!uv pip install nv-ingest==26.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the default configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install python-dotenv\n",
    "import os\n",
    "\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your NGC_API_KEY after executing the cell below. You can obtain a key by following steps [here](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/api-key.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset if needed\n",
    "if os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NGC_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    candidate_api_key = getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert candidate_api_key.startswith(\"nvapi-\"), (\n",
    "        f\"{candidate_api_key[:5]}... is not a valid key\"\n",
    "    )\n",
    "    os.environ[\"NGC_API_KEY\"] = candidate_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to nvcr.io which is needed for pulling the containers of dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup the NIMs\n",
    "\n",
    "Setup the environment variables for NIMs using Nvidia Hosted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are used by nv-ingest-ms-runtime container when using cloud models\n",
    "os.environ[\"OCR_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemotron-ocr\"\n",
    "os.environ[\"OCR_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v3\"\n",
    ")\n",
    "os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"http\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup the NV-Ingest Pipeline Subprocess\n",
    "\n",
    "Apply platform compatibility patches if needed, then launch the NV-Ingest pipeline as a background subprocess for document processing.\n",
    "\n",
    "**Note:** Kindly ensure NV-Ingest container is not running to avoid port conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "macOS Compatibility Patch for nv-ingest\n",
    "\n",
    "The nv-ingest library contains Linux-specific dependencies (libc.so.6) that are not \n",
    "available on macOS systems. This compatibility patch neutralizes the `set_pdeathsig` \n",
    "function to enable cross-platform functionality.\n",
    "\n",
    "Note: This patch should be executed before any nv-ingest functionality is invoked.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Darwin\":  # macOS\n",
    "    print(\"Platform detected: macOS\")\n",
    "    print(\"Applying nv-ingest compatibility patch...\")\n",
    "    \n",
    "    # Import the target module and apply the compatibility patch\n",
    "    import nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners as runners\n",
    "    \n",
    "    # Replace the Linux-specific set_pdeathsig function with a no-op implementation\n",
    "    def _noop_set_pdeathsig():\n",
    "        \"\"\"No-op replacement for Linux-only set_pdeathsig on macOS.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    runners.set_pdeathsig = _noop_set_pdeathsig\n",
    "    print(\"Compatibility patch successfully applied.\")\n",
    "else:\n",
    "    print(f\"Platform detected: {platform.system()}\")\n",
    "    print(\"No compatibility patch required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NV-Ingest pipeline as a background subprocess\n",
    "from nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import run_pipeline\n",
    "\n",
    "run_pipeline(block=False, disable_dynamic_scaling=True, run_in_subprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# API usage example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the python package and starting all dependent services, finally we can execute some snippets showcasing all different functionalities offered by the `nvidia_rag` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set logging level\n",
    "First let's set the required logging level. Set to INFO for displaying basic important logs. Set to DEBUG for full verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the log level via environment variable before importing nvidia_rag\n",
    "# This ensures the package respects our log level setting\n",
    "LOGLEVEL = logging.WARNING  # Set to INFO, DEBUG, WARNING or ERROR\n",
    "os.environ[\"LOGLEVEL\"] = logging.getLevelName(LOGLEVEL)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=LOGLEVEL, force=True)\n",
    "\n",
    "# Set log levels for specific loggers after package import\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    if name == \"nvidia_rag\" or name.startswith(\"nvidia_rag.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)\n",
    "    if name == \"nv_ingest_client\" or name.startswith(\"nv_ingest_client.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the NvidiaRAGIngestor Package in Lite Mode\n",
    "\n",
    "Import `NvidiaRAGIngestor` to access APIs for document upload and management operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag import NvidiaRAGIngestor\n",
    "from nvidia_rag.utils.configuration import NvidiaRAGConfig\n",
    "\n",
    "config_ingestor = NvidiaRAGConfig.from_yaml(\"config.yaml\")\n",
    "# You can update the config object to use different models and endpoints like below\n",
    "# config_ingestor.embeddings.model_name = \"nvidia/llama-nemotron-embed-1b-v2\"\n",
    "# config_ingestor.embeddings.server_url = \"https://integrate.api.nvidia.com/v1\"\n",
    "\n",
    "# Set config for rag lite library mode\n",
    "config_ingestor.vector_store.url = \"./milvus-lite.db\"\n",
    "config_ingestor.nv_ingest.message_client_port = 7671 # Port for NV-Ingest libary mode\n",
    "\n",
    "# Set config for cloud API endpoints\n",
    "config_ingestor.embeddings.server_url = \"https://integrate.api.nvidia.com/v1\"\n",
    "\n",
    "ingestor = NvidiaRAGIngestor(config=config_ingestor, mode=\"lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a new collection\n",
    "Creates a new collection in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.create_collection(\n",
    "    collection_name=\"test_library\",\n",
    "    # [Optional]: Create collection with metadata schema, uncomment to create collection with metadata schemas\n",
    "    # metadata_schema = [\n",
    "    #     {\n",
    "    #         \"name\": \"meta_field_1\",\n",
    "    #         \"type\": \"string\",\n",
    "    #         \"description\": \"Following field would contain the description for the document\"\n",
    "    #     }\n",
    "    # ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List all collections\n",
    "Retrieves all available collections from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.get_collections()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add a document\n",
    "Uploads new documents to the specified collection in the vector database. In case you have a requirement of updating existing documents in the specified collection, you can call `update_documents()` instead of `upload_documents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.upload_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    blocking=False,\n",
    "    split_options={\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
    "    filepaths=[\n",
    "        \"../data/multimodal/woods_frost.docx\",\n",
    "        \"../data/multimodal/multimodal_test.pdf\",\n",
    "    ],\n",
    "    # [Optional]: Uncomment to add custom metadata, ensure that the metadata schema is created with the same fields with create_collection\n",
    "    # custom_metadata=[\n",
    "    #     {\n",
    "    #         \"filename\": \"multimodal_test.pdf\",\n",
    "    #         \"metadata\": {\"meta_field_1\": \"multimodal document 1\"}\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"filename\": \"woods_frost.docx\",\n",
    "    #         \"metadata\": {\"meta_field_1\": \"multimodal document 2\"}\n",
    "    #     }\n",
    "    # ]\n",
    ")\n",
    "task_id = response.get(\"task_id\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check document upload status\n",
    "Checks the status of a document upload/update task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.status(task_id=task_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [Optional] Update a document in a collection\n",
    "In case you have a requirement of updating an existing document in the specified collection, execute below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.update_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    blocking=False,\n",
    "    filepaths=[\"../data/multimodal/woods_frost.docx\"],\n",
    ")\n",
    "task_id = response.get(\"task_id\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get documents in a collection\n",
    "Retrieves the list of documents uploaded to a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.get_documents(\n",
    "    collection_name=\"test_library\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the NvidiaRAG packages\n",
    "You can import `NvidiaRAG()` which exposes APIs to interact with the uploaded documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a config object from a dictionary or from a YAML file. We have added a sample config file [config.yaml](./config.yaml) that you can use to create an `NvidiaRAG` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag import NvidiaRAG\n",
    "from nvidia_rag.utils.configuration import NvidiaRAGConfig\n",
    "\n",
    "config_rag = NvidiaRAGConfig.from_yaml(\"config.yaml\")\n",
    "\n",
    "# Set config for rag lite library mode\n",
    "config_rag.vector_store.url = \"./milvus-lite.db\"\n",
    "config_rag.enable_citations = False\n",
    "\n",
    "# Set config for cloud API endpoints\n",
    "config_rag.embeddings.server_url = \"https://integrate.api.nvidia.com/v1\"\n",
    "config_rag.ranking.server_url = \"\"  # Empty uses NVIDIA API catalog\n",
    "config_rag.llm.server_url = \"\"  # Empty uses NVIDIA API catalog\n",
    "\n",
    "# Initialize NvidiaRAG with config\n",
    "# You can optionally pass custom prompts via:\n",
    "#   - A path to a YAML/JSON file: prompts=\"custom_prompts.yaml\"\n",
    "#   - A dictionary: prompts={\"rag_template\": {\"system\": \"...\", \"human\": \"...\"}}\n",
    "rag = NvidiaRAG(config=config_rag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query a document using RAG\n",
    "Sends a chat-style query to the RAG system using the specified models and endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check health of all dependent services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "health_status_with_deps = await rag.health()\n",
    "print(health_status_with_deps.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "\n",
    "async def print_streaming_response_and_citations(rag_response):\n",
    "    \"\"\"\n",
    "    Print the streaming response and citations from the RAG response.\n",
    "    \"\"\"\n",
    "    # Check for API errors before processing\n",
    "    if rag_response.status_code != 200:\n",
    "        print(\"Error: \", rag_response.status_code)\n",
    "        return\n",
    "\n",
    "    # Extract the streaming generator from the response\n",
    "    response_generator = rag_response.generator\n",
    "    first_chunk_data = None\n",
    "    async for chunk in response_generator:\n",
    "        if chunk.startswith(\"data: \"):\n",
    "            chunk = chunk[len(\"data: \") :].strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            continue\n",
    "        choices = data.get(\"choices\", [])\n",
    "        if not choices:\n",
    "            continue\n",
    "        # Save the first chunk with citations\n",
    "        if first_chunk_data is None and data.get(\"citations\"):\n",
    "            first_chunk_data = data\n",
    "        # Print streaming text\n",
    "        delta = choices[0].get(\"delta\", {})\n",
    "        text = delta.get(\"content\")\n",
    "        if not text:\n",
    "            message = choices[0].get(\"message\", {})\n",
    "            text = message.get(\"content\", \"\")\n",
    "        print(text, end=\"\", flush=True)\n",
    "    print()  # Newline after streaming\n",
    "\n",
    "    # Display citations after streaming is done\n",
    "    if first_chunk_data and first_chunk_data.get(\"citations\"):\n",
    "        citations = first_chunk_data[\"citations\"]\n",
    "        for idx, citation in enumerate(citations.get(\"results\", [])):\n",
    "            doc_type = citation.get(\"document_type\", \"text\")\n",
    "            content = citation.get(\"content\", \"\")\n",
    "            doc_name = citation.get(\"document_name\", f\"Citation {idx + 1}\")\n",
    "            display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "            try:\n",
    "                image_bytes = base64.b64decode(content)\n",
    "                display(Image(data=image_bytes))\n",
    "            except Exception:\n",
    "                display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await print_streaming_response_and_citations(\n",
    "    await rag.generate(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the price of a hammer?\"}],\n",
    "        use_knowledge_base=True,\n",
    "        collection_names=[\"test_library\"],\n",
    "        enable_citations=False,\n",
    "        # embedding_endpoint=\"localhost:9080\" # TODO: Uncomment while using on-prem embeddings\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Search for documents\n",
    "Performs a search in the vector database for relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_citations(citations):\n",
    "    \"\"\"\n",
    "    Display all citations from the Citations object returned by search().\n",
    "    Handles base64-encoded images and text.\n",
    "    \"\"\"\n",
    "    if not citations or not hasattr(citations, \"results\") or not citations.results:\n",
    "        print(\"No citations found.\")\n",
    "        return\n",
    "\n",
    "    for idx, citation in enumerate(citations.results):\n",
    "        # If using pydantic models, citation fields may be attributes, not dict keys\n",
    "        doc_type = getattr(citation, \"document_type\", \"text\")\n",
    "        content = getattr(citation, \"content\", \"\")\n",
    "        doc_name = getattr(citation, \"document_name\", f\"Citation {idx + 1}\")\n",
    "\n",
    "        display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "        try:\n",
    "            image_bytes = base64.b64decode(content)\n",
    "            display(Image(data=image_bytes))\n",
    "        except Exception:\n",
    "            display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_search_citations(\n",
    "    await rag.search(\n",
    "        query=\"What is the price of a hammer?\",\n",
    "        collection_names=[\"test_library\"],\n",
    "        reranker_top_k=10,\n",
    "        vdb_top_k=100,\n",
    "        # embedding_endpoint=\"localhost:9080\" # TODO: Uncomment while using on-prem embeddings\n",
    "        # [Optional]: Uncomment to filter the documents based on the metadata, ensure that the metadata schema is created with the same fields with create_collection\n",
    "        # filter_expr='content_metadata[\"meta_field_1\"] == \"multimodal document 1\"'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below APIs illustrate how to cleanup uploaded documents and collections once no more interaction is needed.\n",
    "## 8. Delete documents from a collection\n",
    "Deletes documents from the specified collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.delete_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    document_names=[\"../data/multimodal/multimodal_test.pdf\"],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Delete collections\n",
    "Deletes the specified collection and all its documents from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.delete_collections(\n",
    "    collection_names=[\"test_library\"]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Stop the NV-Ingest Pipeline Subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Stop process on port 7671\n",
    "PID_7671=$(lsof -ti tcp:7671)\n",
    "if [ -n \"$PID_7671\" ]; then\n",
    "    kill $PID_7671\n",
    "    echo \"Stopped process on port 7671 (PID: $PID_7671)\"\n",
    "else\n",
    "    echo \"No process running on port 7671\"\n",
    "fi\n",
    "\n",
    "# Stop process on port 8265\n",
    "PID_8265=$(lsof -ti tcp:8265)\n",
    "if [ -n \"$PID_8265\" ]; then\n",
    "    kill $PID_8265\n",
    "    echo \"Stopped process on port 8265 (PID: $PID_8265)\"\n",
    "else\n",
    "    echo \"No process running on port 8265\"\n",
    "fi\n",
    "\n",
    "echo \"NV-Ingest Pipeline subprocess check completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-library",
   "language": "python",
   "name": "rag-library"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
