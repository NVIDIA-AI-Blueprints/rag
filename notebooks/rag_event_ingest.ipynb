{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video/Document Continuous Ingestion from Object Storage\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook demonstrates an **automated document and video ingestion pipeline** that:\n",
    "\n",
    "1. Monitors object storage (MinIO) for new uploads via Kafka events\n",
    "2. Routes files to appropriate AI services based on file type\n",
    "3. Indexes documents using NVIDIA RAG Blueprint for semantic search\n",
    "4. Analyzes videos using NVIDIA VSS (Video Search & Summarization)\n",
    "5. Enables RAG Agent for semantic search and contextual Q&A over all ingested content\n",
    "\n",
    "## What Gets Deployed\n",
    "\n",
    "1. **RAG Stack** - Document indexing, vector search, and AI-powered Q&A (NIMs, Milvus, Ingestor)\n",
    "2. **VSS Stack** - Video understanding and summarization (VLM, LLM NIMs, VSS Engine)\n",
    "3. **AIDP Stack** - Event-driven ingestion pipeline (Kafka, MinIO, Consumer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prerequisites\n\n### Hardware\n- **GPU**: 8x RTX PRO 6000 Blackwell\n\n### Software (must be pre-installed)\n- Ubuntu 22.04 or later\n- Docker 24.0+ with Docker Compose v2\n- NVIDIA Driver 570+\n- NVIDIA Container Toolkit\n- Git and Git LFS\n\n### API Keys\n\n<table style=\"margin-left: 0;\">\n<tr><th>Key</th><th>Purpose</th><th>How to Get</th></tr>\n<tr><td><code>NGC_API_KEY</code></td><td>Docker login, NIM deployments</td><td><a href=\"https://org.ngc.nvidia.com/setup/api-keys\">NGC Portal</a> \u2192 Generate API Key</td></tr>\n<tr><td><code>HF_TOKEN</code></td><td>Download VSS models</td><td><a href=\"https://huggingface.co/settings/tokens\">HuggingFace Tokens</a> \u2192 Create token with Read access</td></tr>\n</table>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<table style=\"margin-left: 0;\">\n",
    "<tr><th>Section</th><th>Description</th></tr>\n",
    "<tr><td><b>Setup</b></td><td>Clone repo, install deps, set API keys, load helpers</td></tr>\n",
    "<tr><td><b>Deploy RAG</b></td><td>NIMs, Vector DB, Ingestor, RAG Server</td></tr>\n",
    "<tr><td><b>Deploy VSS</b></td><td>Clone VSS, deploy NIMs and VLM</td></tr>\n",
    "<tr><td><b>Deploy AIDP</b></td><td>Kafka, MinIO, Consumer</td></tr>\n",
    "<tr><td><b>Testing</b></td><td>Upload documents & videos, query RAG</td></tr>\n",
    "<tr><td><b>Clean Up</b></td><td>Stop services, clean data</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **RAG Blueprint**: [NVIDIA RAG Documentation](https://github.com/NVIDIA-AI-Blueprints/rag/blob/develop/docs/deploy-docker-self-hosted.md)\n",
    "- **VSS**: [Video Search & Summarization Documentation](https://docs.nvidia.com/vss/latest/index.html)\n",
    "- **NIM**: [NVIDIA NIM Documentation](https://docs.nvidia.com/nim/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Clone the repository, configure API keys, and load helper functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone Repository\n",
    "\n",
    "Clone the RAG Blueprint repo to `~/rag`. This includes the consumer source code, deploy configs, and sample test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os\n",
    "\n",
    "RAG_REPO_DIR = os.path.expanduser(\"~/rag\")\n",
    "RAG_BRANCH = \"minh/aidp-notebook\"\n",
    "RAG_REPO_URL = \"https://github.com/NVIDIA-AI-Blueprints/rag.git\"\n",
    "\n",
    "# Clone from correct branch\n",
    "subprocess.run(f\"git clone -b {RAG_BRANCH} {RAG_REPO_URL} {RAG_REPO_DIR}\", shell=True, check=True)\n",
    "subprocess.run(\"git lfs pull\", shell=True, cwd=RAG_REPO_DIR, check=True)\n",
    "\n",
    "# Verify\n",
    "for path in [\"deploy/compose\", \"examples/rag_event_ingest/kafka_consumer\", \"examples/rag_event_ingest/data\"]:\n",
    "    status = \"[OK]\" if os.path.exists(os.path.join(RAG_REPO_DIR, path)) else \"[MISSING]\"\n",
    "    print(f\"  {status} {path}\")\n",
    "\n",
    "# Install dependencies\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"minio\", \"aiohttp\", \"requests\", \"python-dotenv\"])\n",
    "print(\"[OK] Ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set API Keys\n",
    "\n",
    "Configure NGC and HuggingFace API keys for NIM deployments and model downloads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "def set_api_key(env_var: str, prompt: str, required: bool = True):\n",
    "    if os.environ.get(env_var):\n",
    "        print(f\"  [OK] {env_var} already set ({os.environ[env_var][:10]}...)\")\n",
    "        return True\n",
    "    key = getpass.getpass(prompt)\n",
    "    if key:\n",
    "        os.environ[env_var] = key\n",
    "        print(f\"  [OK] {env_var} set\")\n",
    "        return True\n",
    "    if required:\n",
    "        print(f\"  [ERROR] {env_var} is required\")\n",
    "        return False\n",
    "    print(f\"  [SKIP] {env_var} (optional)\")\n",
    "    return True\n",
    "\n",
    "set_api_key(\"NGC_API_KEY\", \"Enter NGC_API_KEY (starts with 'nvapi-'): \", required=True)\n",
    "set_api_key(\"HF_TOKEN\", \"Enter HF_TOKEN (optional, press Enter to skip): \", required=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "Shared utilities for deployment, file upload, status checks, and RAG queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, re, subprocess, time, socket, asyncio\n",
    "import aiohttp, requests\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "try:\n",
    "    from minio import Minio\n",
    "    from minio.error import S3Error\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"minio\"])\n",
    "    from minio import Minio\n",
    "    from minio.error import S3Error\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Paths relative to RAG repo root\n",
    "RAG_REPO_DIR = os.path.expanduser(\"~/rag\")\n",
    "EXAMPLE_DIR = os.path.join(RAG_REPO_DIR, \"examples/rag_event_ingest\")\n",
    "AIDP_COMPOSE_FILE = os.path.join(EXAMPLE_DIR, \"deploy/docker-compose.yaml\")\n",
    "DATA_DIR = os.path.join(EXAMPLE_DIR, \"data\")\n",
    "RAG_SERVER_URL = \"http://localhost:8081\"\n",
    "INGESTOR_URL = \"http://localhost:8082\"\n",
    "\n",
    "VSS_DIR = os.path.expanduser(\"~/video-search-and-summarization\")\n",
    "VSS_UI_PORT = 9110\n",
    "VSS_API_PORT = 8110\n",
    "VSS_LLM_PORT = 8107\n",
    "VSS_EMBED_PORT = 8106\n",
    "VSS_RERANK_PORT = 8105\n",
    "LOCAL_NIM_CACHE = os.path.expanduser(\"~/.cache/nim\")\n",
    "\n",
    "MINIO_ENDPOINT = \"localhost:9201\"\n",
    "MINIO_ACCESS_KEY = \"minioadmin\"\n",
    "MINIO_SECRET_KEY = \"minioadmin\"\n",
    "MINIO_BUCKET = \"aidp-bucket\"\n",
    "MINIO_COLLECTION = \"aidp_bucket\"\n",
    "MINIO_CONSOLE_PORT = 9211\n",
    "\n",
    "# =============================================================================\n",
    "# SHARED UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def run_command(cmd: str, capture: bool = False) -> Optional[str]:\n",
    "    \"\"\"Execute a shell command and print it.\"\"\"\n",
    "    print(f\"$ {cmd}\")\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=capture, text=True)\n",
    "    return result.stdout if capture else None\n",
    "\n",
    "def get_host_ip() -> str:\n",
    "    \"\"\"Get host IP address for external access URLs.\"\"\"\n",
    "    try:\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        s.connect((\"8.8.8.8\", 80))\n",
    "        ip = s.getsockname()[0]\n",
    "        s.close()\n",
    "        return ip\n",
    "    except OSError:\n",
    "        return \"localhost\"\n",
    "\n",
    "def get_minio_client() -> Minio:\n",
    "    \"\"\"Create MinIO client for AIDP bucket operations.\"\"\"\n",
    "    return Minio(MINIO_ENDPOINT, access_key=MINIO_ACCESS_KEY, secret_key=MINIO_SECRET_KEY, secure=False)\n",
    "\n",
    "def upload_file(local_path: str, object_name: Optional[str] = None) -> bool:\n",
    "    \"\"\"Upload a local file to MinIO AIDP bucket.\"\"\"\n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"[ERROR] File not found: {local_path}\")\n",
    "        return False\n",
    "    obj = object_name or os.path.basename(local_path)\n",
    "    try:\n",
    "        client = get_minio_client()\n",
    "        if not client.bucket_exists(MINIO_BUCKET):\n",
    "            client.make_bucket(MINIO_BUCKET)\n",
    "        client.fput_object(MINIO_BUCKET, obj, local_path)\n",
    "        print(f\"[OK] Uploaded: {obj}\")\n",
    "        return True\n",
    "    except S3Error as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "        return False\n",
    "\n",
    "def get_consumer_logs(lines: int = 30) -> None:\n",
    "    \"\"\"Show recent Kafka consumer logs.\"\"\"\n",
    "    run_command(f\"docker logs kafka-consumer --tail {lines}\")\n",
    "\n",
    "async def query_rag(question: str, collection: str = None) -> Optional[str]:\n",
    "    \"\"\"Query RAG system and print the answer.\"\"\"\n",
    "    coll = collection or MINIO_COLLECTION\n",
    "    print(f\"Q: {question}\\nCollection: {coll}\\n\" + \"-\" * 40)\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question}],\n",
    "        \"use_knowledge_base\": True,\n",
    "        \"collection_name\": coll,\n",
    "    }\n",
    "    try:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(\n",
    "                f\"{RAG_SERVER_URL}/generate\", json=payload,\n",
    "                timeout=aiohttp.ClientTimeout(total=120),\n",
    "            ) as resp:\n",
    "                text = await resp.text()\n",
    "                # Parse SSE response: extract content from each \"data: {...}\" line\n",
    "                chunks = []\n",
    "                for line in text.split(\"\\n\"):\n",
    "                    if not line.startswith(\"data: \") or line[6:] == \"[DONE]\":\n",
    "                        continue\n",
    "                    try:\n",
    "                        msg = json.loads(line[6:]).get(\"choices\", [{}])[0].get(\"message\", {})\n",
    "                        if msg.get(\"content\"):\n",
    "                            chunks.append(msg[\"content\"])\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                answer = \"\".join(chunks)\n",
    "                print(f\"Answer: {answer}\")\n",
    "                return answer\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "        return None\n",
    "\n",
    "print(f\"[OK] Helpers loaded | Host IP: {get_host_ip()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy RAG\n",
    "\n",
    "Deploy the RAG stack: NIMs (LLM, Embedding, Reranker), Milvus vector database, Ingestor server, and RAG server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_key = os.environ.get(\"NGC_API_KEY\")\n",
    "if not ngc_key:\n",
    "    raise RuntimeError(\"NGC_API_KEY not set! Run the API keys cell first.\")\n",
    "\n",
    "os.chdir(RAG_REPO_DIR)\n",
    "\n",
    "# Set env vars needed by docker compose\n",
    "os.environ[\"NGC_API_KEY\"] = ngc_key\n",
    "os.environ[\"USERID\"] = f\"{os.getuid()}:{os.getgid()}\"\n",
    "os.environ[\"COLLECTION_NAME\"] = MINIO_COLLECTION\n",
    "\n",
    "# Load RAG .env defaults (MODEL_DIRECTORY, etc.)\n",
    "from dotenv import load_dotenv\n",
    "env_file = os.path.join(RAG_REPO_DIR, \"deploy/compose/.env\")\n",
    "if os.path.exists(env_file):\n",
    "    load_dotenv(env_file, override=False)\n",
    "\n",
    "# Login to nvcr.io\n",
    "subprocess.run(f\"echo {ngc_key} | docker login nvcr.io -u '$oauthtoken' --password-stdin\",\n",
    "               shell=True, capture_output=True, text=True, executable=\"/bin/bash\")\n",
    "\n",
    "# Deploy components\n",
    "for label, compose_file in [\n",
    "    (\"NIMs\",      \"deploy/compose/nims.yaml\"),\n",
    "    (\"Vector DB\", \"deploy/compose/vectordb.yaml\"),\n",
    "]:\n",
    "    print(f\"Deploying {label}...\")\n",
    "    run_command(f\"docker compose -f {compose_file} up -d\")\n",
    "\n",
    "print(\"Waiting 30s for Milvus...\")\n",
    "time.sleep(30)\n",
    "\n",
    "for label, compose_file in [\n",
    "    (\"Ingestor\", \"deploy/compose/docker-compose-ingestor-server.yaml\"),\n",
    "    (\"RAG Server\", \"deploy/compose/docker-compose-rag-server.yaml\"),\n",
    "]:\n",
    "    print(f\"Deploying {label}...\")\n",
    "    run_command(f\"docker compose -f {compose_file} up -d\")\n",
    "\n",
    "ip = get_host_ip()\n",
    "print(f\"\\nRAG deployed: http://{ip}:8081 (server) | http://{ip}:8082 (ingestor) | http://{ip}:8090 (UI)\")\n",
    "print(f\"COLLECTION_NAME: {MINIO_COLLECTION}\")\n",
    "print(\"Wait 2-5 minutes for NIMs to load models, then run the status check cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify RAG services are healthy. Wait 2-5 minutes for NIMs to load models.\n",
    "\n",
    "The deployment status should be:\n",
    "```\n",
    "NAMES                            STATUS\n",
    "rag-frontend                     Up About a minute\n",
    "rag-server                       Up About a minute\n",
    "ingestor-server                  Up About a minute\n",
    "milvus-standalone                Up 2 minutes (healthy)\n",
    "milvus-etcd                      Up 2 minutes (healthy)\n",
    "milvus-minio                     Up 2 minutes (healthy)\n",
    "nim-llm-ms                       Up 2 minutes (healthy)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wait 2-5 minutes for services to become healthy.\")\n",
    "print(\"Run this cell again after waiting.\\n\")\n",
    "\n",
    "ip = get_host_ip()\n",
    "for name, port, path in [\n",
    "    (\"RAG Server\", 8081, \"/health\"), (\"Ingestor\", 8082, \"/health\"),\n",
    "    (\"Frontend\", 8090, \"/\"), (\"Milvus\", 19530, \"/v1/vector/collections\"),\n",
    "]:\n",
    "    try:\n",
    "        s = \"[OK]\" if requests.get(f\"http://localhost:{port}{path}\", timeout=10).status_code == 200 else \"[WARN]\"\n",
    "    except requests.ConnectionError:\n",
    "        s = \"[DOWN]\"\n",
    "    except requests.Timeout:\n",
    "        s = \"[TIMEOUT]\"\n",
    "    print(f\"  {s} {name}: http://{ip}:{port}\")\n",
    "run_command(\"docker ps --format 'table {{.Names}}\\t{{.Status}}' | grep -E '(rag|milvus|ingestor|nim|NAMES)'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy VSS\n",
    "\n",
    "Deploy the VSS stack: NIMs (LLM, Embedding, Reranker) and VLM for video analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VSS deployment configuration\n",
    "VSS_REPO_URL = \"https://github.com/NVIDIA-AI-Blueprints/video-search-and-summarization.git\"\n",
    "VSS_GPU_DEVICE = 4       # GPU for NIMs (LLM, Embedding, Reranker)\n",
    "VSS_VLM_GPU_DEVICE = 5   # GPU for VLM (via-server with Cosmos-Reason2)\n",
    "\n",
    "NIM_IMAGES = {\n",
    "    \"vss-llm\":       (\"nvcr.io/nim/meta/llama-3.1-8b-instruct:1.12.0\",      VSS_LLM_PORT),\n",
    "    \"vss-embedding\": (\"nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:1.9.0\", VSS_EMBED_PORT),\n",
    "    \"vss-reranker\":  (\"nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:1.7.0\", VSS_RERANK_PORT),\n",
    "}\n",
    "\n",
    "ngc_key = os.environ.get(\"NGC_API_KEY\", \"\")\n",
    "hf_token = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "if not ngc_key:\n",
    "    raise RuntimeError(\"NGC_API_KEY not set!\")\n",
    "\n",
    "# Docker login\n",
    "subprocess.run(f\"echo {ngc_key} | docker login nvcr.io -u '$oauthtoken' --password-stdin\",\n",
    "               shell=True, capture_output=True, text=True, executable=\"/bin/bash\")\n",
    "\n",
    "# Clone VSS repo\n",
    "if not os.path.exists(VSS_DIR):\n",
    "    print(f\"Cloning {VSS_REPO_URL}...\")\n",
    "    subprocess.run(f\"git clone {VSS_REPO_URL} {VSS_DIR}\", shell=True)\n",
    "else:\n",
    "    print(f\"[OK] VSS repo exists: {VSS_DIR}\")\n",
    "\n",
    "# Deploy NIM containers (all on same GPU)\n",
    "os.makedirs(LOCAL_NIM_CACHE, exist_ok=True)\n",
    "for name, (image, port) in NIM_IMAGES.items():\n",
    "    subprocess.run(f\"docker rm -f {name} 2>/dev/null\", shell=True, capture_output=True)\n",
    "    cmd = f\"\"\"docker run -d --name {name} \\\n",
    "        -u $(id -u) --gpus '\"device={VSS_GPU_DEVICE}\"' --shm-size=16GB \\\n",
    "        --network nvidia-rag -e NGC_API_KEY={ngc_key} \\\n",
    "        -v \"{LOCAL_NIM_CACHE}:/opt/nim/.cache\" \\\n",
    "        -p {port}:8000 -e NIM_LOW_MEMORY_MODE=1 -e NIM_RELAX_MEM_CONSTRAINTS=1 \\\n",
    "        {image}\"\"\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, executable=\"/bin/bash\")\n",
    "    status = \"[OK]\" if result.returncode == 0 else \"[ERROR]\"\n",
    "    print(f\"  {status} {name} -> port {port}\")\n",
    "\n",
    "# Deploy VSS application (VLM on separate GPU)\n",
    "vss_deploy_dir = f\"{VSS_DIR}/deploy/docker/local_deployment_single_gpu\"\n",
    "env_content = f\"\"\"NGC_API_KEY={ngc_key}\n",
    "HF_TOKEN={hf_token}\n",
    "VIA_IMAGE=nvcr.io/nvidia/blueprint/vss-engine:2.4.1\n",
    "FRONTEND_PORT={VSS_UI_PORT}\n",
    "BACKEND_PORT={VSS_API_PORT}\n",
    "MILVUS_DB_HTTP_PORT=19091\n",
    "MILVUS_DB_GRPC_PORT=29530\n",
    "MINIO_PORT=9002\n",
    "MINIO_WEBUI_PORT=9003\n",
    "GRAPH_DB_USERNAME=neo4j\n",
    "GRAPH_DB_PASSWORD=password\n",
    "ARANGO_DB_USERNAME=arangodb\n",
    "ARANGO_DB_PASSWORD=password\n",
    "CA_RAG_CONFIG=./config.yaml\n",
    "GUARDRAILS_CONFIG=./guardrails\n",
    "NVIDIA_VISIBLE_DEVICES={VSS_VLM_GPU_DEVICE}\n",
    "VLM_MODEL_TO_USE=cosmos-reason2\n",
    "MODEL_PATH=git:https://huggingface.co/nvidia/Cosmos-Reason2-8B\n",
    "VLLM_GPU_MEMORY_UTILIZATION=0.4\n",
    "VLM_MAX_MODEL_LEN=20480\n",
    "DISABLE_GUARDRAILS=true\n",
    "DISABLE_CV_PIPELINE=true\n",
    "ENABLE_AUDIO=false\n",
    "\"\"\"\n",
    "with open(f\"{vss_deploy_dir}/.env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "# Patch config.yaml to use our NIM ports\n",
    "config_file = f\"{vss_deploy_dir}/config.yaml\"\n",
    "if os.path.exists(config_file):\n",
    "    cfg = open(config_file).read()\n",
    "    cfg = re.sub(r\":8007/v1\", f\":{VSS_LLM_PORT}/v1\", cfg)\n",
    "    cfg = re.sub(r\":8006/v1\", f\":{VSS_EMBED_PORT}/v1\", cfg)\n",
    "    cfg = re.sub(r\":8005/v1\", f\":{VSS_RERANK_PORT}/v1\", cfg)\n",
    "    open(config_file, \"w\").write(cfg)\n",
    "\n",
    "cmd = f\"cd {vss_deploy_dir} && set -a && source .env && set +a && docker compose up -d\"\n",
    "subprocess.run(cmd, shell=True, capture_output=True, text=True, executable=\"/bin/bash\")\n",
    "\n",
    "ip = get_host_ip()\n",
    "print(f\"\\nVSS deployed: http://{ip}:{VSS_UI_PORT} (UI) | http://{ip}:{VSS_API_PORT} (API)\")\n",
    "print(\"Wait 2-5 minutes for NIMs to load models, then run the status check cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify VSS services are healthy. Wait 2-5 minutes for NIMs to load models.\n",
    "\n",
    "The deployment status should be:\n",
    "```\n",
    "NAMES                                             STATUS\n",
    "local_deployment_single_gpu-via-server-1          Up About a minute\n",
    "local_deployment_single_gpu-elasticsearch-1       Up About a minute\n",
    "local_deployment_single_gpu-graph-db-1            Up About a minute\n",
    "local_deployment_single_gpu-minio-1               Up About a minute\n",
    "local_deployment_single_gpu-arango-db-1           Up About a minute\n",
    "local_deployment_single_gpu-milvus-standalone-1   Up About a minute (healthy)\n",
    "vss-reranker                                      Up About a minute\n",
    "vss-embedding                                     Up About a minute\n",
    "vss-llm                                           Up About a minute\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = get_host_ip()\n",
    "for name, port, path in [\n",
    "    (\"VSS UI\", VSS_UI_PORT, \"/\"), (\"VSS API\", VSS_API_PORT, \"/\"),\n",
    "    (\"LLM NIM\", VSS_LLM_PORT, \"/v1/health/ready\"),\n",
    "    (\"Embedding\", VSS_EMBED_PORT, \"/v1/health/ready\"),\n",
    "    (\"Reranker\", VSS_RERANK_PORT, \"/v1/health/ready\"),\n",
    "]:\n",
    "    try:\n",
    "        requests.get(f\"http://localhost:{port}{path}\", timeout=10)\n",
    "        s = \"[OK]\"\n",
    "    except requests.ConnectionError:\n",
    "        s = \"[DOWN]\"\n",
    "    except requests.Timeout:\n",
    "        s = \"[TIMEOUT]\"\n",
    "    print(f\"  {s} {name}: http://{ip}:{port}\")\n",
    "run_command(\"docker ps --format 'table {{.Names}}\\t{{.Status}}' | grep -E '(vss|via|local_deployment|NAMES)'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy AIDP\n",
    "\n",
    "Deploy the AIDP stack: Kafka message broker, MinIO object storage, and Kafka consumer for automated ingestion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify prerequisites\n",
    "net_check = subprocess.run(\"docker network inspect nvidia-rag\", shell=True, capture_output=True)\n",
    "if net_check.returncode != 0:\n",
    "    raise RuntimeError(\"nvidia-rag network not found. Deploy RAG first.\")\n",
    "\n",
    "ngc_key = os.environ.get(\"NGC_API_KEY\", \"\")\n",
    "if not ngc_key:\n",
    "    raise RuntimeError(\"NGC_API_KEY not set!\")\n",
    "\n",
    "host_ip = get_host_ip()\n",
    "os.environ[\"VSS_SERVER_URL\"] = f\"http://{host_ip}:{VSS_API_PORT}\"\n",
    "\n",
    "# Login + pull + build\n",
    "subprocess.run(f\"echo {ngc_key} | docker login nvcr.io -u '$oauthtoken' --password-stdin\",\n",
    "               shell=True, capture_output=True, text=True, executable=\"/bin/bash\")\n",
    "\n",
    "compose = f\"docker compose -f {AIDP_COMPOSE_FILE}\"\n",
    "subprocess.run(f\"{compose} pull --ignore-pull-failures\", shell=True, capture_output=True, text=True, executable=\"/bin/bash\")\n",
    "subprocess.run(f\"{compose} up -d --build\", shell=True, capture_output=True, text=True, executable=\"/bin/bash\")\n",
    "\n",
    "print(f\"AIDP deployed:\")\n",
    "print(f\"  Kafka UI:      http://{host_ip}:8080\")\n",
    "print(f\"  MinIO Console: http://{host_ip}:{MINIO_CONSOLE_PORT}\")\n",
    "print(f\"  Credentials:   minioadmin / minioadmin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify AIDP services are running.\n",
    "\n",
    "The deployment status should be:\n",
    "```\n",
    "NAMES                            STATUS\n",
    "kafka-consumer                   Up About a minute\n",
    "aidp-kafka-ui                    Up About a minute\n",
    "aidp-minio-mc                    Up About a minute\n",
    "aidp-minio                       Up About a minute (healthy)\n",
    "kafka                            Up About a minute (healthy)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = get_host_ip()\n",
    "print(f\"  Kafka UI:      http://{ip}:8080\")\n",
    "print(f\"  MinIO Console: http://{ip}:{MINIO_CONSOLE_PORT}\")\n",
    "run_command(\"docker ps --format 'table {{.Names}}\\t{{.Status}}' | grep -E '(kafka|minio|NAMES)'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Test the deployment by uploading documents and videos, then querying via RAG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Upload\n",
    "\n",
    "Upload a PDF document to MinIO, which triggers automatic ingestion via Kafka consumer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Upload to Storage\n",
    "\n",
    "Upload the document to MinIO object storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents are included in the repo under examples/rag_event_ingest/data/\n",
    "pdf_path = os.path.join(DATA_DIR, \"documents\", \"Seahawks-Patriots in Super Bowl LX_ What We Learned from Seattle's 29-13 win.pdf\")\n",
    "upload_file(pdf_path, \"Seahawks-Patriots_SuperBowl_LX_Analysis.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Verify Document Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check consumer logs to verify document processing status.\n",
    "\n",
    "The logs should show the document being picked up and successfully ingested:\n",
    "```\n",
    "services.document_indexer - INFO - Task ...: PENDING (0s)\n",
    "services.document_indexer - INFO - Task ...: PENDING (5s)\n",
    "handlers.base - INFO - [DocumentHandler] \u2713 Seahawks-Patriots_SuperBowl_LX_Analysis.pdf \u2192 SUCCESS\n",
    "consumer - INFO - \u2713 SUMMARY: Seahawks-Patriots_SuperBowl_LX_Analysis.pdf | Collection: aidp_bucket | Duration: 12.76s | Status: SUCCESS\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check document processing status\n",
    "print(\"Waiting for document processing...\")\n",
    "get_consumer_logs(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Query Document via RAG\n",
    "\n",
    "Query the ingested document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the document\n",
    "await query_rag(\"What was the final score and who won Super Bowl LX?\", MINIO_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask another question about the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query about key takeaways\n",
    "await query_rag(\"What were the key lessons learned from Seattle's victory in Super Bowl LX?\", MINIO_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Video Upload\n",
    "\n",
    "Upload a video to MinIO, which triggers automatic ingestion via Kafka consumer \u2192 VSS for video analysis \u2192 RAG for indexing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Upload to Storage\n",
    "\n",
    "Upload the video to MinIO object storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample videos are included in the repo under examples/rag_event_ingest/data/\n",
    "video_path = os.path.join(DATA_DIR, \"videos\", \"Seattle Seahawks vs New England Patriots - Super Bowl LX Game Highlights.mp4\")\n",
    "upload_file(video_path)\n",
    "\n",
    "print(\"\\nVideo processing takes longer than documents. Check consumer logs for progress.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Verify Video Ingestion\n",
    "\n",
    "Check consumer logs to verify video processing status.\n",
    "\n",
    "The logs should show the video being picked up and processed by VSS:\n",
    "```\n",
    "handlers.video - INFO - [VideoHandler] Processing video: Seattle Seahawks vs New England Patriots - Super Bowl LX Game Highlights.mp4\n",
    "services.video_analyzer - INFO - Submitting video to VSS...\n",
    "services.video_analyzer - INFO - VSS processing complete\n",
    "handlers.base - INFO - [VideoHandler] \u2713 Seattle Seahawks...mp4 \u2192 SUCCESS\n",
    "consumer - INFO - \u2713 SUMMARY: Seattle Seahawks...mp4 | Collection: aidp_bucket | Duration: ~120s | Status: SUCCESS\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check video processing status\n",
    "print(\"Waiting for video processing...\")\n",
    "get_consumer_logs(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Query Video via RAG\n",
    "\n",
    "Query the video content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query about the video content\n",
    "await query_rag(\"Summarize the video content\", MINIO_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query about a specific time range in the video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query about specific time range\n",
    "await query_rag(\"What happened between 15:00 and 20:00?\", MINIO_COLLECTION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional query: analyze key defensive plays and turnovers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defensive Analysis\n",
    "await query_rag(\"Describe the key defensive plays and turnovers that impacted the game outcome.\", MINIO_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional query: identify critical momentum-changing plays in the second half.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum Shifts\n",
    "await query_rag(\"What were the critical momentum-changing plays in the second half of the game?\", MINIO_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up\n",
    "\n",
    "Stop all services and clean up ingested data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stop RAG Deployment\n",
    "\n",
    "Stop all RAG services (NIMs, Milvus, Ingestor, RAG server).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(RAG_REPO_DIR)\n",
    "for f in [\n",
    "    \"deploy/compose/docker-compose-rag-server.yaml\",\n",
    "    \"deploy/compose/docker-compose-ingestor-server.yaml\",\n",
    "    \"deploy/compose/vectordb.yaml\",\n",
    "    \"deploy/compose/nims.yaml\",\n",
    "]:\n",
    "    run_command(f\"docker compose -f {f} down\")\n",
    "print(\"[OK] RAG stopped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stop VSS Deployment\n",
    "\n",
    "Stop all VSS services (NIMs, VLM, via-server).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vss_deploy_dir = f\"{VSS_DIR}/deploy/docker/local_deployment_single_gpu\"\n",
    "if os.path.exists(vss_deploy_dir):\n",
    "    subprocess.run(f\"cd {vss_deploy_dir} && set -a && source .env 2>/dev/null && set +a && docker compose down\",\n",
    "                   shell=True, executable=\"/bin/bash\", capture_output=True)\n",
    "for name in [\"vss-llm\", \"vss-embedding\", \"vss-reranker\"]:\n",
    "    subprocess.run(f\"docker rm -f {name} 2>/dev/null\", shell=True, capture_output=True)\n",
    "print(\"[OK] VSS stopped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stop AIDP Deployment\n",
    "\n",
    "Stop AIDP services (Kafka, MinIO, Consumer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command(f\"docker compose -f {AIDP_COMPOSE_FILE} down\")\n",
    "print(\"[OK] AIDP stopped\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}