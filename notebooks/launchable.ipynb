{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2d22ff",
   "metadata": {},
   "source": [
    "# NVIDIA RAG Blueprint - Quick Start Guide\n",
    "\n",
    "This notebook walks you through deploying and using the NVIDIA RAG (Retrieval-Augmented Generation) system.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG is a technique that improves AI responses by:\n",
    "1. **Storing** your documents as searchable vectors (like an index)\n",
    "2. **Searching** for relevant content when you ask a question\n",
    "3. **Generating** an answer using only the retrieved context\n",
    "\n",
    "Think of it like `grep` for AI - it finds relevant text first, then answers based on what it found.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "| Section | Description | Time |\n",
    "|---------|-------------|------|\n",
    "| **0. Setup** | Install dependencies, define helper functions | 2 min |\n",
    "| **1. Prerequisites** | Check hardware/software requirements | 2 min |\n",
    "| **2. Deploy** | Start all Docker containers | 10-15 min |\n",
    "| **3. Test APIs** | Verify services are working | 5 min |\n",
    "| **4. Use the Chatbot** | Upload docs, ask questions | 5 min |\n",
    "| **5. Cleanup** | Stop containers, free resources | 1 min |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Before You Start\n",
    "\n",
    "- **GPU Required**: NVIDIA GPU(s) with 80GB+ VRAM total\n",
    "- **API Key Required**: Get one from https://org.ngc.nvidia.com/setup/api-keys\n",
    "- **Disk Space**: ~50GB for model cache\n",
    "- **Memory**: 32GB+ RAM recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b5474",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 0: Setup\n",
    "\n",
    "---\n",
    "\n",
    "Run these cells once to set up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6587f",
   "metadata": {},
   "source": [
    "### 0.1 Install Required Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393eaf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "import sys\n",
    "try:\n",
    "    import pip  # noqa\n",
    "except ImportError:\n",
    "    !{sys.executable} -m ensurepip --upgrade\n",
    "\n",
    "!{sys.executable} -m pip install -q python-dotenv aiohttp requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10669fa",
   "metadata": {},
   "source": [
    "### 0.2 Load Helper Functions\n",
    "\n",
    "All utility functions are defined in this cell. **Run it once before proceeding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NVIDIA RAG Blueprint - Helper Functions\n",
    "=======================================\n",
    "Run this cell once to load all utilities.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Edit these if services run on different hosts\n",
    "# =============================================================================\n",
    "\n",
    "IPADDRESS = \"0.0.0.0\"\n",
    "RAG_SERVER_PORT = \"8081\"\n",
    "INGESTOR_SERVER_PORT = \"8082\"\n",
    "MILVUS_ENDPOINT = \"http://milvus:19530\"\n",
    "\n",
    "RAG_BASE_URL = f\"http://{IPADDRESS}:{RAG_SERVER_PORT}\"\n",
    "INGESTOR_BASE_URL = f\"http://{IPADDRESS}:{INGESTOR_SERVER_PORT}\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DOCKER COMPOSE HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "def docker_compose(yaml_file: str, action: str, args: str = \"\", profile: str = \"\") -> bool:\n",
    "    \"\"\"Run docker compose command.\n",
    "    - Suppresses image pull noise\n",
    "    - Surfaces full error output on failure\n",
    "    \"\"\"\n",
    "\n",
    "    profile_arg = f\"--profile {profile}\" if profile else \"\"\n",
    "    cmd = f\"docker compose -f {yaml_file} {profile_arg} {action} {args}\".strip()\n",
    "    cmd = \" \".join(cmd.split())  # Clean up extra spaces\n",
    "\n",
    "    # Actions that may pull images implicitly\n",
    "    QUIET_ACTIONS = {\"pull\", \"up\"}\n",
    "\n",
    "    print_prefix = \"Pulling images\" if action == \"pull\" else \"Running\"\n",
    "\n",
    "    if action in QUIET_ACTIONS:\n",
    "        print(f\"{print_prefix} from {yaml_file}...\", flush=True)\n",
    "\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            shell=True,\n",
    "            env=os.environ,\n",
    "            stdout=subprocess.PIPE,    # capture\n",
    "            stderr=subprocess.PIPE,    # capture\n",
    "            text=True,\n",
    "        )\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ {action} complete\", flush=True)\n",
    "            return True\n",
    "\n",
    "        # ---- Failure path: show diagnostics ----\n",
    "        print(f\"‚ùå {action} failed\", flush=True)\n",
    "\n",
    "        if result.stdout:\n",
    "            print(\"\\n--- STDOUT ---\")\n",
    "            print(result.stdout)\n",
    "\n",
    "        if result.stderr:\n",
    "            print(\"\\n--- STDERR ---\")\n",
    "            print(result.stderr)\n",
    "\n",
    "        return False\n",
    "\n",
    "    # ---- Non-quiet actions: stream output ----\n",
    "    print(f\"$ {cmd}\", flush=True)\n",
    "    result = subprocess.run(cmd, shell=True, env=os.environ)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ {action} succeeded\", flush=True)\n",
    "        return True\n",
    "\n",
    "    print(f\"‚ùå {action} failed (exit code {result.returncode})\", flush=True)\n",
    "    return False\n",
    "\n",
    "def check_containers():\n",
    "    \"\"\"Show all running Docker containers.\"\"\"\n",
    "    subprocess.run('docker ps --format \"table {{.Names}}\\t{{.Status}}\"', shell=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# API HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "async def api_get(url: str, params: dict = None) -> dict:\n",
    "    \"\"\"Make async GET request.\"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url, params=params) as resp:\n",
    "            if resp.status != 200:\n",
    "                text = await resp.text()\n",
    "                print(f\"‚ùå API error {resp.status}: {text[:200]}\")\n",
    "                return {\"error\": text, \"status\": resp.status}\n",
    "            try:\n",
    "                return await resp.json()\n",
    "            except:\n",
    "                text = await resp.text()\n",
    "                return {\"error\": text, \"raw\": True}\n",
    "\n",
    "\n",
    "async def check_health(base_url: str, name: str) -> bool:\n",
    "    \"\"\"Check if service is healthy.\"\"\"\n",
    "    try:\n",
    "        await api_get(f\"{base_url}/v1/health\")\n",
    "        print(f\"‚úÖ {name} is healthy\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {name} not responding: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COLLECTION MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "async def create_collection(name: str, collection_type: str = \"text\") -> dict:\n",
    "    \"\"\"Create a new collection in the vector database.\"\"\"\n",
    "    url = f\"{INGESTOR_BASE_URL}/v1/collections\"\n",
    "    params = {\n",
    "        \"vdb_endpoint\": MILVUS_ENDPOINT,\n",
    "        \"collection_type\": collection_type\n",
    "    }\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url, params=params, json=[name]) as resp:\n",
    "            if resp.status == 200:\n",
    "                print(f\"‚úÖ Created collection: {name}\")\n",
    "            else:\n",
    "                text = await resp.text()\n",
    "                print(f\"Collection response: {resp.status} - {text[:100]}\")\n",
    "            try:\n",
    "                return await resp.json()\n",
    "            except:\n",
    "                return {\"status\": resp.status}\n",
    "\n",
    "\n",
    "async def list_collections() -> list:\n",
    "    \"\"\"List all vector database collections.\"\"\"\n",
    "    result = await api_get(\n",
    "        f\"{INGESTOR_BASE_URL}/v1/collections\",\n",
    "        {\"vdb_endpoint\": MILVUS_ENDPOINT}\n",
    "    )\n",
    "    collections = result.get(\"collections\", [])\n",
    "    print(f\"Collections ({len(collections)}):\")\n",
    "    for c in collections:\n",
    "        print(f\"  ‚Ä¢ {c.get('collection_name', c)}\")\n",
    "    return collections\n",
    "\n",
    "\n",
    "async def delete_collection(name: str):\n",
    "    \"\"\"Delete a collection.\"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        params = {\"vdb_endpoint\": MILVUS_ENDPOINT, \"collection_names\": [name]}\n",
    "        async with session.delete(\n",
    "            f\"{INGESTOR_BASE_URL}/v1/collections\", params=params\n",
    "        ) as resp:\n",
    "            print(f\"Deleted collection: {name}\")\n",
    "            return await resp.json()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DOCUMENT MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "async def upload_documents(filepaths, collection: str = \"multimodal_data\") -> dict:\n",
    "    \"\"\"\n",
    "    Upload document(s) to vector database in a single request.\n",
    "    \n",
    "    Args:\n",
    "        filepaths: Single filepath (str), list of filepaths, or directory path\n",
    "        collection: Target collection name (must exist before uploading)\n",
    "    \n",
    "    Returns:\n",
    "        Response dict with task_id for tracking upload status\n",
    "    \n",
    "    Chunking options:\n",
    "        - chunk_size: 1024 tokens\n",
    "        - chunk_overlap: 150 tokens\n",
    "    \"\"\"\n",
    "    # Handle single file, list, or directory\n",
    "    if isinstance(filepaths, str):\n",
    "        if os.path.isdir(filepaths):\n",
    "            # It's a directory - get all files\n",
    "            filepaths = [\n",
    "                os.path.join(filepaths, f) \n",
    "                for f in os.listdir(filepaths) \n",
    "                if os.path.isfile(os.path.join(filepaths, f))\n",
    "            ]\n",
    "        else:\n",
    "            filepaths = [filepaths]\n",
    "    \n",
    "    # Configure upload parameters\n",
    "    data = {\n",
    "        \"vdb_endpoint\": MILVUS_ENDPOINT,\n",
    "        \"collection_name\": collection,\n",
    "        \"split_options\": {\n",
    "            \"chunk_size\": 1024,\n",
    "            \"chunk_overlap\": 150\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Prepare multipart form data with all files\n",
    "    form_data = aiohttp.FormData()\n",
    "    for filepath in filepaths:\n",
    "        form_data.add_field(\n",
    "            \"documents\",\n",
    "            open(filepath, \"rb\"),\n",
    "            filename=os.path.basename(filepath),\n",
    "            content_type=\"application/pdf\"\n",
    "        )\n",
    "        print(f\"üì§ Adding: {os.path.basename(filepath)}\")\n",
    "    form_data.add_field(\"data\", json.dumps(data), content_type=\"application/json\")\n",
    "    \n",
    "    # Upload all documents in single request\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                f\"{INGESTOR_BASE_URL}/v1/documents\", data=form_data\n",
    "            ) as resp:\n",
    "                result = await resp.json()\n",
    "                task_id = result.get(\"task_id\", \"\")\n",
    "                print(f\"‚úÖ Upload started ‚Üí Task ID: {task_id}\")\n",
    "                return result\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"‚ùå Upload error: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "async def check_upload_status(task_id: str) -> dict:\n",
    "    \"\"\"Check document upload status.\"\"\"\n",
    "    result = await api_get(\n",
    "        f\"{INGESTOR_BASE_URL}/v1/status\",\n",
    "        {\"task_id\": task_id}\n",
    "    )\n",
    "    print(f\"Task {task_id[:8]}...: {result.get('state', 'UNKNOWN')}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "async def list_documents(collection: str = \"multimodal_data\") -> list:\n",
    "    \"\"\"List documents in collection.\"\"\"\n",
    "    result = await api_get(\n",
    "        f\"{INGESTOR_BASE_URL}/v1/documents\",\n",
    "        {\"collection_name\": collection, \"vdb_endpoint\": MILVUS_ENDPOINT}\n",
    "    )\n",
    "    docs = result.get(\"documents\", [])\n",
    "    print(f\"Documents in '{collection}' ({len(docs)}):\")\n",
    "    for d in docs:\n",
    "        print(f\"  ‚Ä¢ {d.get('document_name', d)}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "async def delete_document(filenames, collection: str = \"multimodal_data\"):\n",
    "    \"\"\"Delete document(s) from collection. Accepts single filename or list.\"\"\"\n",
    "    if isinstance(filenames, str):\n",
    "        filenames = [filenames]\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        params = {\"collection_name\": collection, \"vdb_endpoint\": MILVUS_ENDPOINT}\n",
    "        async with session.delete(\n",
    "            f\"{INGESTOR_BASE_URL}/v1/documents\",\n",
    "            params=params,\n",
    "            json=filenames\n",
    "        ) as resp:\n",
    "            for f in filenames:\n",
    "                print(f\"üóëÔ∏è  Deleted: {f}\")\n",
    "            return await resp.json()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CHAT / QUERY\n",
    "# =============================================================================\n",
    "\n",
    "async def chat(question: str, use_rag: bool = True, collection: str = \"multimodal_data\", \n",
    "               show_citations: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Ask a question to the RAG system.\n",
    "    \n",
    "    Args:\n",
    "        question: Your question\n",
    "        use_rag: If True, search documents first (default). If False, use LLM only.\n",
    "        collection: Collection to search\n",
    "        show_citations: If True, print sources used to generate the response\n",
    "    \n",
    "    Returns:\n",
    "        The AI response text\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question}],\n",
    "        \"use_knowledge_base\": use_rag,\n",
    "        \"collection_names\": [collection] if use_rag else [],\n",
    "        \"temperature\": 0.2,\n",
    "        \"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(\n",
    "            f\"{RAG_BASE_URL}/v1/chat/completions\", json=payload\n",
    "        ) as resp:\n",
    "            content_type = resp.headers.get(\"Content-Type\", \"\")\n",
    "            \n",
    "            if \"text/event-stream\" in content_type:\n",
    "                # Handle streaming response - read full response first\n",
    "                response_text = await resp.text()\n",
    "                concatenated_content = \"\"\n",
    "                citations = None\n",
    "                \n",
    "                for line in response_text.split(\"\\n\"):\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        json_str = line[6:]\n",
    "                        if json_str.strip() == \"[DONE]\":\n",
    "                            continue\n",
    "                        try:\n",
    "                            data = json.loads(json_str)\n",
    "                            content = data.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\", \"\")\n",
    "                            concatenated_content += content\n",
    "                            # Citations are in the first chunk\n",
    "                            if citations is None and \"citations\" in data:\n",
    "                                citations = data.get(\"citations\", {})\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "                \n",
    "                print(concatenated_content)\n",
    "                \n",
    "                # Print citations if available and requested\n",
    "                if show_citations and use_rag and citations:\n",
    "                    results = citations.get(\"results\", [])\n",
    "                    if results:\n",
    "                        print(\"\\n\" + \"-\" * 60)\n",
    "                        print(f\"üìö SOURCES ({len(results)} citations):\")\n",
    "                        print(\"-\" * 60)\n",
    "                        for i, src in enumerate(results, 1):\n",
    "                            doc_name = src.get(\"document_name\", \"unknown\")\n",
    "                            doc_type = src.get(\"document_type\", \"text\")\n",
    "                            score = src.get(\"score\", 0)\n",
    "                            content_preview = src.get(\"content\", \"\")[:150].replace(\"\\n\", \" \")\n",
    "                            # Skip base64 image content in preview\n",
    "                            if content_preview.startswith((\"/9j/\", \"iVBOR\")):\n",
    "                                content_preview = \"[Image content]\"\n",
    "                            print(f\"[{i}] {doc_name} ({doc_type}) - Score: {score:.4f}\")\n",
    "                            print(f\"    {content_preview}...\")\n",
    "                \n",
    "                return concatenated_content\n",
    "            else:\n",
    "                # Handle regular JSON response\n",
    "                response_json = await resp.json()\n",
    "                if \"error\" in response_json:\n",
    "                    print(f\"Error: {response_json['error']}\")\n",
    "                    return \"\"\n",
    "                \n",
    "                content = response_json.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                #print(content)\n",
    "                \n",
    "                # Print citations if available\n",
    "                if show_citations and use_rag:\n",
    "                    citations = response_json.get(\"citations\", {})\n",
    "                    results = citations.get(\"results\", [])\n",
    "                    if results:\n",
    "                        print(\"\\n\" + \"-\" * 60)\n",
    "                        print(f\"üìö SOURCES ({len(results)} citations):\")\n",
    "                        print(\"-\" * 60)\n",
    "                        for i, src in enumerate(results, 1):\n",
    "                            doc_name = src.get(\"document_name\", \"unknown\")\n",
    "                            doc_type = src.get(\"document_type\", \"text\")\n",
    "                            score = src.get(\"score\", 0)\n",
    "                            print(f\"[{i}] {doc_name} ({doc_type}) - Score: {score:.4f}\")\n",
    "                \n",
    "                return content\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEPLOYMENT COMMANDS\n",
    "# =============================================================================\n",
    "\n",
    "def deploy_all():\n",
    "    \"\"\"Start all RAG Blueprint services.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DEPLOYING NVIDIA RAG BLUEPRINT\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    print(\"\\n[1/4] NIM Microservices...\")\n",
    "    docker_compose(\"deploy/compose/nims.yaml\", \"pull\", \"-q\")\n",
    "    docker_compose(\"deploy/compose/nims.yaml\", \"up\", \"-d\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\n[2/4] Vector Database...\")\n",
    "    docker_compose(\"deploy/compose/vectordb.yaml\", \"pull\", \"-q\")\n",
    "    docker_compose(\"deploy/compose/vectordb.yaml\", \"up\", \"-d\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\n[3/4] Ingestor Server...\")\n",
    "    docker_compose(\"deploy/compose/docker-compose-ingestor-server.yaml\", \"pull\", \"-q\")\n",
    "    docker_compose(\"deploy/compose/docker-compose-ingestor-server.yaml\", \"up\", \"-d\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\n[4/4] RAG Server...\")\n",
    "    docker_compose(\"deploy/compose/docker-compose-rag-server.yaml\", \"pull\", \"-q\")\n",
    "    docker_compose(\"deploy/compose/docker-compose-rag-server.yaml\", \"up\", \"-d\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DEPLOYMENT COMPLETE\")\n",
    "    print(\"Wait 2-5 minutes for services to become healthy.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def stop_all():\n",
    "    \"\"\"Stop all RAG Blueprint services.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STOPPING ALL SERVICES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n[1/4] Stopping RAG Server...\")\n",
    "    docker_compose(\"deploy/compose/docker-compose-rag-server.yaml\", \"down\")\n",
    "    \n",
    "    print(\"\\n[2/4] Stopping Ingestor Server...\")\n",
    "    docker_compose(\"deploy/compose/docker-compose-ingestor-server.yaml\", \"down\")\n",
    "    \n",
    "    print(\"\\n[3/4] Stopping Vector Database...\")\n",
    "    docker_compose(\"deploy/compose/vectordb.yaml\", \"down\")\n",
    "    \n",
    "    print(\"\\n[4/4] Stopping NIM Containers...\")\n",
    "    docker_compose(\"deploy/compose/nims.yaml\", \"down\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ ALL SERVICES STOPPED\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# INITIALIZATION MESSAGE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded!\")\n",
    "print()\n",
    "print(\"Available functions:\")\n",
    "print(\"  deploy_all()            - Start all services\")\n",
    "print(\"  stop_all()              - Stop all services\")\n",
    "print(\"  check_containers()      - List running containers\")\n",
    "print(\"  chat(question)          - Ask a question (use_rag=True by default)\")\n",
    "print(\"  upload_documents(paths) - Upload file(s) or directory\")\n",
    "print(\"  list_documents()        - List uploaded files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423be903",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Check Prerequisites\n",
    "\n",
    "---\n",
    "\n",
    "Verify your system meets the requirements before deploying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96a0cb",
   "metadata": {},
   "source": [
    "### 1.1 System Requirements Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYSTEM REQUIREMENTS CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# [1] Operating System\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n[1] Operating System (Ubuntu 22.04+ recommended):\")\n",
    "try:\n",
    "    result = subprocess.run([\"lsb_release\", \"-d\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        os_info = result.stdout.strip()\n",
    "        print(f\"    {os_info}\")\n",
    "        if \"Ubuntu\" in os_info:\n",
    "            # Extract version number\n",
    "            match = re.search(r\"(\\d+\\.\\d+)\", os_info)\n",
    "            if match:\n",
    "                version = float(match.group(1))\n",
    "                if version >= 22.04:\n",
    "                    print(\"    ‚úÖ PASS\")\n",
    "                else:\n",
    "                    warnings.append(f\"Ubuntu {version} detected, 22.04+ recommended\")\n",
    "                    print(f\"    ‚ö†Ô∏è  WARNING: Ubuntu {version} < 22.04\")\n",
    "        else:\n",
    "            warnings.append(\"Non-Ubuntu OS detected\")\n",
    "            print(\"    ‚ö†Ô∏è  WARNING: Non-Ubuntu OS\")\n",
    "    else:\n",
    "        print(\"    Unable to determine OS\")\n",
    "except FileNotFoundError:\n",
    "    print(\"    Non-Linux system (macOS/Windows)\")\n",
    "    warnings.append(\"Non-Linux system detected\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# [2] NVIDIA GPU\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n[2] NVIDIA GPU:\")\n",
    "try:\n",
    "    result = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        gpus = result.stdout.strip().split(\"\\n\")\n",
    "        for gpu in gpus:\n",
    "            print(f\"    {gpu}\")\n",
    "        print(f\"    ‚úÖ PASS ({len(gpus)} GPU(s) detected)\")\n",
    "    else:\n",
    "        errors.append(\"No NVIDIA GPU detected\")\n",
    "        print(\"    ‚ùå FAIL: No NVIDIA GPU detected\")\n",
    "except FileNotFoundError:\n",
    "    errors.append(\"nvidia-smi not found - NVIDIA driver not installed\")\n",
    "    print(\"    ‚ùå FAIL: nvidia-smi not found\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# [3] NVIDIA Driver Version (need 560+)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n[3] NVIDIA Driver Version (need 560+):\")\n",
    "try:\n",
    "    result = subprocess.run([\"nvidia-smi\", \"-q\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        match = re.search(r\"Driver Version\\s*:\\s*(\\d+)\", result.stdout)\n",
    "        if match:\n",
    "            driver_version = int(match.group(1))\n",
    "            print(f\"    Driver Version: {driver_version}\")\n",
    "            if driver_version >= 560:\n",
    "                print(\"    ‚úÖ PASS\")\n",
    "            else:\n",
    "                errors.append(f\"Driver version {driver_version} < 560 required\")\n",
    "                print(f\"    ‚ùå FAIL: Version {driver_version} < 560\")\n",
    "        else:\n",
    "            print(\"    Unable to parse driver version\")\n",
    "except FileNotFoundError:\n",
    "    print(\"    ‚ùå FAIL: nvidia-smi not found\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# [4] CUDA Version (need 12.4+)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n[4] CUDA Version (need 12.4+):\")\n",
    "try:\n",
    "    result = subprocess.run([\"nvidia-smi\", \"-q\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        match = re.search(r\"CUDA Version\\s*:\\s*(\\d+\\.\\d+)\", result.stdout)\n",
    "        if match:\n",
    "            cuda_version = float(match.group(1))\n",
    "            print(f\"    CUDA Version: {cuda_version}\")\n",
    "            if cuda_version >= 12.4:\n",
    "                print(\"    ‚úÖ PASS\")\n",
    "            else:\n",
    "                errors.append(f\"CUDA version {cuda_version} < 12.4 required\")\n",
    "                print(f\"    ‚ùå FAIL: Version {cuda_version} < 12.4\")\n",
    "        else:\n",
    "            print(\"    Unable to parse CUDA version\")\n",
    "except FileNotFoundError:\n",
    "    print(\"    ‚ùå FAIL: nvidia-smi not found\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# [5] Docker Version (need 26.0+)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n[5] Docker Version (need 26.0+):\")\n",
    "if shutil.which(\"docker\"):\n",
    "    result = subprocess.run([\"docker\", \"--version\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"    {result.stdout.strip()}\")\n",
    "        match = re.search(r\"(\\d+)\\.\\d+\", result.stdout)\n",
    "        if match:\n",
    "            docker_major = int(match.group(1))\n",
    "            if docker_major >= 26:\n",
    "                print(\"    ‚úÖ PASS\")\n",
    "            else:\n",
    "                errors.append(f\"Docker version {docker_major} < 26 required\")\n",
    "                print(f\"    ‚ùå FAIL: Major version {docker_major} < 26\")\n",
    "else:\n",
    "    errors.append(\"Docker not installed\")\n",
    "    print(\"    ‚ùå FAIL: Docker not installed\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# [6] Docker Compose Version (need 2.29.1+)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n[6] Docker Compose Version (need 2.29.1+):\")\n",
    "result = subprocess.run([\"docker\", \"compose\", \"version\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"    {result.stdout.strip()}\")\n",
    "    match = re.search(r\"v?(\\d+)\\.(\\d+)\", result.stdout)\n",
    "    if match:\n",
    "        major, minor = int(match.group(1)), int(match.group(2))\n",
    "        if major > 2 or (major == 2 and minor >= 29):\n",
    "            print(\"    ‚úÖ PASS\")\n",
    "        else:\n",
    "            errors.append(f\"Docker Compose {major}.{minor} < 2.29.1 required\")\n",
    "            print(f\"    ‚ùå FAIL: Version {major}.{minor} < 2.29.1\")\n",
    "else:\n",
    "    errors.append(\"Docker Compose not available\")\n",
    "    print(\"    ‚ùå FAIL: Docker Compose not available\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# [7] System Memory (need 32GB+)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n[7] System Memory (need 32GB+):\")\n",
    "try:\n",
    "    result = subprocess.run([\"free\", \"-g\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        lines = result.stdout.strip().split(\"\\n\")\n",
    "        for line in lines[:2]:\n",
    "            print(f\"    {line}\")\n",
    "        # Parse total memory\n",
    "        match = re.search(r\"Mem:\\s+(\\d+)\", result.stdout)\n",
    "        if match:\n",
    "            total_gb = int(match.group(1))\n",
    "            if total_gb >= 32:\n",
    "                print(f\"    ‚úÖ PASS ({total_gb}GB available)\")\n",
    "            elif total_gb >= 16:\n",
    "                warnings.append(f\"Only {total_gb}GB RAM, 32GB+ recommended\")\n",
    "                print(f\"    ‚ö†Ô∏è  WARNING: {total_gb}GB < 32GB recommended\")\n",
    "            else:\n",
    "                errors.append(f\"Only {total_gb}GB RAM, need 32GB+ for full deployment\")\n",
    "                print(f\"    ‚ùå FAIL: {total_gb}GB < 32GB minimum\")\n",
    "except FileNotFoundError:\n",
    "    print(\"    Unable to check (non-Linux system)\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# SUMMARY\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "if errors:\n",
    "    print(\"\\n\" + \"üö®\" * 35)\n",
    "    print(\"üö®\" + \" \" * 66 + \"üö®\")\n",
    "    print(\"üö®   REQUIREMENTS NOT MET - CANNOT PROCEED WITH DEPLOYMENT!        üö®\")\n",
    "    print(\"üö®\" + \" \" * 66 + \"üö®\")\n",
    "    print(\"üö®\" * 35)\n",
    "    print(\"\\n‚ùå ERRORS ({} issues):\".format(len(errors)))\n",
    "    for err in errors:\n",
    "        print(f\"   ‚Ä¢ {err}\")\n",
    "    if warnings:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNINGS ({} issues):\".format(len(warnings)))\n",
    "        for warn in warnings:\n",
    "            print(f\"   ‚Ä¢ {warn}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Please fix the above errors before continuing.\")\n",
    "    print(\"See: https://docs.nvidia.com/ai-blueprints/rag/latest/support-matrix.html\")\n",
    "    print(\"=\" * 70)\n",
    "elif warnings:\n",
    "    print(\"\\n‚úÖ REQUIREMENTS MET (with warnings)\")\n",
    "    print(\"\\n‚ö†Ô∏è  WARNINGS ({} issues):\".format(len(warnings)))\n",
    "    for warn in warnings:\n",
    "        print(f\"   ‚Ä¢ {warn}\")\n",
    "    print(\"\\nYou may proceed, but some features might not work as expected.\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"    ‚úÖ   ALL REQUIREMENTS MET - READY TO DEPLOY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f07762",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Deploy the Blueprint\n",
    "\n",
    "---\n",
    "\n",
    "This section will guide you through:\n",
    "1. Setting your NGC API key for authentication\n",
    "2. Cloning the NVIDIA RAG Blueprint repository  \n",
    "3. Logging into NGC and configuring the environment\n",
    "4. Deploying all services with Docker Compose\n",
    "5. Verifying the deployment status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cbadc6",
   "metadata": {},
   "source": [
    "### 2.1 Set Your NGC API Key\n",
    "\n",
    "Get your key at: https://org.ngc.nvidia.com/setup/api-keys\n",
    "\n",
    "Your key must have permissions for\n",
    "\n",
    "* NGC Catalog\n",
    "* Public API Endpoints\n",
    "\n",
    "> **Note**: Cell 2.1a below is optional - only run it if you need to clear a previously set API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47e9400",
   "metadata": {},
   "source": [
    "### 2.1a Reset API Key (Optional)\n",
    "\n",
    "‚ö†Ô∏è **Only run this cell if you need to clear a previously set API key and start over.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e98c4-d653-46f1-998b-26c91b68f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.pop(\"NGC_API_KEY\", None)\n",
    "print(\"‚úÖ NGC_API_KEY unset (if it existed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282899cb",
   "metadata": {},
   "source": [
    "**Enter Your NGC API Key:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef467d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"‚úÖ NGC_API_KEY already set\")\n",
    "else:\n",
    "    key = getpass.getpass(\"Enter NGC API key (starts with 'nvapi-'): \")\n",
    "    if key.startswith(\"nvapi-\"):\n",
    "        os.environ[\"NGC_API_KEY\"] = key\n",
    "        print(\"‚úÖ API key set\")\n",
    "    else:\n",
    "        print(\"‚ùå Invalid key format - must start with 'nvapi-'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291e5e3",
   "metadata": {},
   "source": [
    "### 2.2 Clone the GitHub Repository\n",
    "\n",
    "Clone the official NVIDIA RAG Blueprint repository and checkout the v2.4 release:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f616c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/NVIDIA-AI-Blueprints/rag.git\"\n",
    "BRANCH = \"v2.4.0\"\n",
    "#BRANCH = \"develop\"\n",
    "# Check if we're already in the rag repo (look for deploy/compose)\n",
    "if os.path.exists(\"deploy/compose\"):\n",
    "    print(f\"‚úÖ Already in rag repo: {os.getcwd()}\")\n",
    "elif os.path.exists(\"../deploy/compose\"):\n",
    "    # We're in a subdirectory (like notebooks/), go up\n",
    "    os.chdir(\"..\")\n",
    "    print(f\"‚úÖ Changed to repo root: {os.getcwd()}\")\n",
    "else:\n",
    "    # Clone if rag folder doesn't exist\n",
    "    if not os.path.exists(\"rag\"):\n",
    "        print(f\"Cloning {REPO_URL}...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL], check=True)\n",
    "        print(\"‚úÖ Clone complete\")\n",
    "    os.chdir(\"rag\")\n",
    "    print(f\"‚úÖ Changed to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f785e",
   "metadata": {},
   "source": [
    "### 2.3 Login to NGC & Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Login to NGC container registry\n",
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\n",
    "\n",
    "# Set user ID for Docker volume permissions\n",
    "os.environ[\"USERID\"] = subprocess.check_output(\"id -u\", shell=True).decode().strip()\n",
    "\n",
    "# Create model cache directory\n",
    "model_cache = os.path.expanduser(\"~/.cache/model-cache\")\n",
    "os.makedirs(model_cache, exist_ok=True)\n",
    "os.environ[\"MODEL_DIRECTORY\"] = model_cache\n",
    "\n",
    "# Configure to use NVIDIA-hosted LLM (reduces local GPU requirements)\n",
    "env_config = '''\n",
    "export APP_LLM_SERVERURL=\"\"\n",
    "export SUMMARY_LLM_SERVERURL=\"\"\n",
    "export SUMMARY_LLM=\"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "export APP_LLM_MODELNAME=\"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "export APP_FILTEREXPRESSIONGENERATOR_MODELNAME=\"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "export APP_QUERYREWRITER_MODELNAME=\"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "'''\n",
    "\n",
    "with open(\"deploy/compose/.env\", \"a\") as f:\n",
    "    f.write(env_config)\n",
    "\n",
    "print(f\"‚úÖ Base LLM configuration written\")\n",
    "print(f\"   Model cache: {model_cache}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f64f20",
   "metadata": {},
   "source": [
    "### 2.3a Save Extracted Content (Optional)\n",
    "\n",
    "‚ö†Ô∏è **Only run this cell if you want to save extracted document content to disk.**\n",
    "\n",
    "This enables saving the extracted text/images from documents during ingestion for inspection or debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable saving extracted content to disk\n",
    "save_config = '''\n",
    "export APP_NVINGEST_SAVETODISK=True\n",
    "# Set host directory path (customize as needed)\n",
    "export INGESTOR_SERVER_EXTERNAL_VOLUME_MOUNT=./volumes/ingestor-server\n",
    "# Set container internal path (customize as needed)\n",
    "export INGESTOR_SERVER_DATA_DIR=/data/\n",
    "'''\n",
    "\n",
    "with open(\"deploy/compose/.env\", \"a\") as f:\n",
    "    f.write(save_config)\n",
    "\n",
    "print(\"‚úÖ Save-to-disk configuration written\")\n",
    "print(\"   Extracted content will be saved to: ./volumes/ingestor-server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144efeb-2e56-46de-ba32-8e475227d33a",
   "metadata": {},
   "source": [
    "### 2.3b Enable LLM Reasoning (Optional)\n",
    "\n",
    "‚ö†Ô∏è **Only run this cell if you want to enable LLM-based reasoning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2fde9c-7316-4e6c-8437-ec129fe10a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Toggle RAG Template Thinking Mode in prompt.yaml\n",
    "# Changes ONLY the rag_template from /no_think to /think (or vice versa)\n",
    "# /think = Step-by-step reasoning (slower, more detailed)\n",
    "# /no_think = Direct responses (faster)\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Toggle: Set to True for thinking mode, False for no_think mode\n",
    "ENABLE_RAG_THINKING = True  # Set to True to enable /think\n",
    "\n",
    "# Path to prompt.yaml\n",
    "PROMPT_YAML_PATH = Path(\"src/nvidia_rag/rag_server/prompt.yaml\")\n",
    "\n",
    "# Read the current file\n",
    "content = PROMPT_YAML_PATH.read_text()\n",
    "\n",
    "if ENABLE_RAG_THINKING:\n",
    "    # Pattern to match only rag_template's system block\n",
    "    pattern = r'(rag_template:\\s*\\n\\s*system:\\s*\\|\\s*\\n\\s*)/no_think'\n",
    "    replacement = r'\\1/think'\n",
    "    new_value = \"/think\"\n",
    "else:\n",
    "    pattern = r'(rag_template:\\s*\\n\\s*system:\\s*\\|\\s*\\n\\s*)/think'\n",
    "    replacement = r'\\1/no_think'\n",
    "    new_value = \"/no_think\"\n",
    "\n",
    "new_content, count = re.subn(pattern, replacement, content, count=1)\n",
    "\n",
    "if count > 0:\n",
    "    PROMPT_YAML_PATH.write_text(new_content)\n",
    "    print(f\"‚úÖ RAG Template updated to: {new_value}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Restart the RAG server for changes to take effect:\")\n",
    "    print(f\"   Run: stop_all() then deploy_all()\")\n",
    "else:\n",
    "    # Check current state\n",
    "    if re.search(r'rag_template:\\s*\\n\\s*system:\\s*\\|\\s*\\n\\s*/think', content):\n",
    "        print(\"‚ÑπÔ∏è  RAG Template is already set to: /think\")\n",
    "    elif re.search(r'rag_template:\\s*\\n\\s*system:\\s*\\|\\s*\\n\\s*/no_think', content):\n",
    "        print(\"‚ÑπÔ∏è  RAG Template is already set to: /no_think\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not find rag_template section in expected format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c8b8b",
   "metadata": {},
   "source": [
    "### 2.3c Image Captioning & VLM Inferencing (Optional)\n",
    "\n",
    "‚ö†Ô∏è **Only run this cell if you want to enable image extraction, captioning, and VLM-based reasoning.**\n",
    "‚ö†Ô∏è **LLM Inferencing for answer generation is not exercised if you enable this**\n",
    "\n",
    "This enables:\n",
    "- **Image extraction** from documents during ingestion\n",
    "- **Image captioning** using a Vision-Language Model (VLM) \n",
    "- **VLM inference** for multimodal queries at runtime\n",
    "\n",
    "Uses the NVIDIA-hosted `nvidia/nemotron-nano-12b-v2-vl` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad36a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable image captioning during ingestion\n",
    "captioning_config = '''\n",
    "export APP_NVINGEST_EXTRACTIMAGES=\"True\"\n",
    "export APP_NVINGEST_CAPTIONENDPOINTURL=\"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "export APP_NVINGEST_CAPTIONMODELNAME=\"nvidia/nemotron-nano-12b-v2-vl\"\n",
    "'''\n",
    "\n",
    "# Enable VLM inferencing for multimodal queries\n",
    "vlm_config = '''\n",
    "export ENABLE_VLM_INFERENCE=\"true\"\n",
    "export APP_VLM_MODELNAME=\"nvidia/nemotron-nano-12b-v2-vl\"\n",
    "export APP_VLM_SERVERURL=\"https://integrate.api.nvidia.com/v1/\"\n",
    "export APP_VLM_TEMPERATURE=0.3\n",
    "export APP_VLM_TOP_P=0.91\n",
    "export APP_VLM_MAX_TOKENS=8192\n",
    "'''\n",
    "\n",
    "with open(\"deploy/compose/.env\", \"a\") as f:\n",
    "    f.write(captioning_config)\n",
    "    f.write(vlm_config)\n",
    "\n",
    "print(\"‚úÖ Image captioning & VLM configuration written\")\n",
    "print(\"   VLM Model: nvidia/nemotron-nano-12b-v2-vl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85036b0-04e6-473d-846e-80b30385db87",
   "metadata": {},
   "source": [
    "### 2.3d Enable VLM Inferencing (Optional)\n",
    "\n",
    "‚ö†Ô∏è **Only run this cell if you want to Enable reasoning with the VLM**\n",
    "\n",
    "This enables saving the extracted text/images from documents during ingestion for inspection or debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954033bf-bedc-41fc-bc12-170bd413f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Toggle VLM Thinking Mode in prompt.yaml\n",
    "# Changes the vlm_template from /no_think to /think (or vice versa)\n",
    "# /think = Step-by-step reasoning (slower, more detailed)\n",
    "# /no_think = Direct responses (faster)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Toggle: Set to True for thinking mode, False for no_think mode\n",
    "ENABLE_VLM_THINKING = True  # Set to True to enable /think\n",
    "\n",
    "# Path to prompt.yaml\n",
    "PROMPT_YAML_PATH = Path(\"src/nvidia_rag/rag_server/prompt.yaml\")\n",
    "\n",
    "# Read the current file\n",
    "content = PROMPT_YAML_PATH.read_text()\n",
    "\n",
    "# Find and update the vlm_template section\n",
    "if ENABLE_VLM_THINKING:\n",
    "    # Change /no_think to /think in vlm_template section\n",
    "    old_block = 'vlm_template:\\n  system: |\\n    /no_think'\n",
    "    new_block = 'vlm_template:\\n  system: |\\n    /think'\n",
    "else:\n",
    "    # Change /think to /no_think in vlm_template section\n",
    "    old_block = 'vlm_template:\\n  system: |\\n    /think'\n",
    "    new_block = 'vlm_template:\\n  system: |\\n    /no_think'\n",
    "\n",
    "if old_block in content:\n",
    "    content = content.replace(old_block, new_block)\n",
    "    PROMPT_YAML_PATH.write_text(content)\n",
    "    mode = \"/think\" if ENABLE_VLM_THINKING else \"/no_think\"\n",
    "    print(f\"‚úÖ VLM Template updated to: {mode}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Restart the RAG server for changes to take effect:\")\n",
    "    print(f\"   Run: stop_all() then deploy_all()\")\n",
    "else:\n",
    "    # Check current state\n",
    "    if 'vlm_template:\\n  system: |\\n    /think' in content:\n",
    "        print(\"‚ÑπÔ∏è  VLM Template is already set to: /think\")\n",
    "    elif 'vlm_template:\\n  system: |\\n    /no_think' in content:\n",
    "        print(\"‚ÑπÔ∏è  VLM Template is already set to: /no_think\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not find vlm_template section in expected format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e5d82",
   "metadata": {},
   "source": [
    "### 2.3e Load Environment Configuration\n",
    "\n",
    "**Run this cell after configuring the optional features above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reasoning mode from prompt.yaml\n",
    "load_dotenv(\"deploy/compose/.env\")\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "prompt_yaml_path = Path(\"src/nvidia_rag/rag_server/prompt.yaml\")\n",
    "\n",
    "# Check if VLM inferencing is enabled\n",
    "vlm_enabled = os.environ.get(\"ENABLE_VLM_INFERENCE\") == \"true\"\n",
    "\n",
    "if prompt_yaml_path.exists():\n",
    "    content = prompt_yaml_path.read_text()\n",
    "    \n",
    "    if vlm_enabled:\n",
    "        print(\"\\n   üîÆ VLM Inferencing: ENABLED\")\n",
    "        \n",
    "        # Check image captioning\n",
    "        if os.environ.get(\"APP_NVINGEST_EXTRACTIMAGES\") == \"True\":\n",
    "            print(\"      ‚îú‚îÄ üñºÔ∏è  Image extraction: ENABLED\")\n",
    "            caption_model = os.environ.get(\"APP_NVINGEST_CAPTIONMODELNAME\", \"not set\")\n",
    "            print(f\"      ‚îú‚îÄ üìù Image captioning: ENABLED\")\n",
    "            print(f\"      ‚îÇ     Model: {caption_model}\")\n",
    "        else:\n",
    "            print(\"      ‚îú‚îÄ üñºÔ∏è  Image extraction: DISABLED\")\n",
    "            print(\"      ‚îú‚îÄ üìù Image captioning: DISABLED\")\n",
    "        \n",
    "        # Check VLM reasoning mode (vlm_template)\n",
    "        if \"/think\" in content.split(\"vlm_template:\")[1].split(\"human:\")[0]:\n",
    "            if \"/no_think\" in content.split(\"vlm_template:\")[1].split(\"human:\")[0]:\n",
    "                print(\"      ‚îî‚îÄ üß† VLM reasoning: DISABLED (/no_think)\")\n",
    "            else:\n",
    "                print(\"      ‚îî‚îÄ üß† VLM reasoning: ENABLED (/think)\")\n",
    "        else:\n",
    "            print(\"      ‚îî‚îÄ üß† VLM reasoning: DISABLED (/no_think)\")\n",
    "    else:\n",
    "        print(\"\\n   üí¨ LLM Inferencing: ENABLED (default)\")\n",
    "        \n",
    "        # Check LLM/RAG reasoning mode (rag_template)\n",
    "        if \"/think\" in content.split(\"rag_template:\")[1].split(\"human:\")[0]:\n",
    "            if \"/no_think\" in content.split(\"rag_template:\")[1].split(\"human:\")[0]:\n",
    "                print(\"      ‚îî‚îÄ üß† LLM reasoning: DISABLED (/no_think)\")\n",
    "            else:\n",
    "                print(\"      ‚îî‚îÄ üß† LLM reasoning: ENABLED (/think)\")\n",
    "        else:\n",
    "            print(\"      ‚îî‚îÄ üß† LLM reasoning: DISABLED (/no_think)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb37635",
   "metadata": {},
   "source": [
    "### 2.4 Deploy All Services\n",
    "\n",
    "**‚è±Ô∏è First run takes 10-15 minutes to pull container images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b9f42d",
   "metadata": {},
   "source": [
    "### 2.5 Verify Deployment\n",
    "\n",
    "Wait for containers to show **\"healthy\"** status (~2-5 minutes after deployment).\n",
    "\n",
    "**Expected Output:**\n",
    "\n",
    "When all services are running correctly, you should see these containers:\n",
    "\n",
    "```\n",
    "CONTAINER ID   NAMES                            STATUS\n",
    "88181d20ba30   rag-frontend                     Up 2 minutes\n",
    "5cf93ea91d4e   rag-server                       Up 2 minutes\n",
    "03ff43bd4f53   compose-nv-ingest-ms-runtime-1   Up 2 minutes (healthy)\n",
    "fcc703631b71   ingestor-server                  Up 2 minutes\n",
    "77f64a4a5146   compose-redis-1                  Up 2 minutes\n",
    "902445432dde   milvus-standalone                Up 3 minutes\n",
    "340bc8210a0d   milvus-minio                     Up 3 minutes (healthy)\n",
    "0be702b87ad6   milvus-etcd                      Up 3 minutes (healthy)\n",
    "fe2751bfa734   nemoretriever-ranking-ms         Up 10 minutes (healthy)\n",
    "7b5ddabf8be7   compose-graphic-elements-1       Up 10 minutes\n",
    "ecfaa5190302   compose-page-elements-1          Up 10 minutes\n",
    "ea8c7fdf20d1   nemoretriever-embedding-ms       Up 10 minutes (healthy)\n",
    "6d62008a9b42   compose-nemoretriever-ocr-1      Up 10 minutes\n",
    "969b9f5c987c   compose-table-structure-1        Up 10 minutes\n",
    "```\n",
    "\n",
    "> **Note:** Container IDs and exact uptimes will vary. The important thing is that all containers are running and the ones marked \"(healthy)\" show that status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_containers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28bfc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Test the Services\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "health_checks_header",
   "metadata": {},
   "source": [
    "### 3.1 Health Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0daced",
   "metadata": {},
   "outputs": [],
   "source": [
    "await check_health(RAG_BASE_URL, \"RAG Server\")\n",
    "await check_health(INGESTOR_BASE_URL, \"Ingestor Server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3a7cb",
   "metadata": {},
   "source": [
    "### 3.2 Test LLM (without RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb45cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Q: What is 2+2?\")\n",
    "print(\"A: \", end=\"\")\n",
    "await chat(\"What is 2+2?\", use_rag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91d623",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Use the Chatbot with Documents\n",
    "\n",
    "---\n",
    "\n",
    "Now let's upload documents and ask questions about them.\n",
    "\n",
    "**In this section, you'll:**\n",
    "1. Download sample Wikipedia articles (AI and Machine Learning)\n",
    "2. Create a vector database collection\n",
    "3. Upload and verify documents are indexed\n",
    "4. Study the extracted results (text, images, tables)\n",
    "5. Test the search endpoint directly\n",
    "6. Compare LLM responses with and without RAG\n",
    "7. Ask your own questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df19d7",
   "metadata": {},
   "source": [
    "### 4.1 Download Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 2 sample documents from Wikipedia\n",
    "SAMPLE_DOCS = [\n",
    "    (\"sample_ai_article.pdf\", \"Artificial_intelligence\"),\n",
    "    (\"sample_ml_article.pdf\", \"Machine_learning\")\n",
    "]\n",
    "\n",
    "for filename, topic in SAMPLE_DOCS:\n",
    "    !curl -sL \"https://en.wikipedia.org/api/rest_v1/page/pdf/{topic}\" -o {filename}\n",
    "    print(f\"‚úÖ Downloaded: {filename}\")\n",
    "\n",
    "!ls -lh *.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1748ff",
   "metadata": {},
   "source": [
    "### 4.2 Create Collection\n",
    "\n",
    "Create the collection before uploading documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the collection first (required before uploading)\n",
    "COLLECTION_NAME = \"multimodal_data\"\n",
    "await create_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2664e1",
   "metadata": {},
   "source": [
    "**Verify Collection Created:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa2167-67e3-403c-b80b-1cd139465264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all collections in the vector database\n",
    "collections = await list_collections()\n",
    "\n",
    "# Show raw data\n",
    "print(f\"\\nRaw data: {collections}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a885b29",
   "metadata": {},
   "source": [
    "### 4.3 Upload Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41fe807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Upload all documents in a single request\n",
    "DOC_FILES = [filename for filename, _ in SAMPLE_DOCS]\n",
    "print(f\"Uploading {len(DOC_FILES)} documents to collection: {COLLECTION_NAME}\\n\")\n",
    "\n",
    "response = await upload_documents(DOC_FILES, collection=COLLECTION_NAME)\n",
    "task_id = response.get(\"task_id\")\n",
    "\n",
    "if task_id:\n",
    "    print(f\"\\nWaiting for upload to complete...\")\n",
    "    for _ in range(24):  # Wait up to 2 minutes\n",
    "        await asyncio.sleep(5)\n",
    "        status = await check_upload_status(task_id)\n",
    "        if status.get(\"state\") == \"FINISHED\":\n",
    "            print(\"\\n‚úÖ Upload complete!\")\n",
    "            break\n",
    "        elif status.get(\"state\") == \"FAILURE\":\n",
    "            print(\"\\n‚ùå Upload failed!\")\n",
    "            break\n",
    "else:\n",
    "    print(\"‚ùå No task_id returned - check ingestor logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d17ad",
   "metadata": {},
   "source": [
    "### 4.4 Verify Upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf518bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "await list_documents(collection=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e565aa7",
   "metadata": {},
   "source": [
    "### 4.5 Study Extracted Results\n",
    "\n",
    "When `APP_NVINGEST_SAVETODISK=True` is enabled, the ingestion pipeline saves the extracted results to:\n",
    "```\n",
    "./volumes/ingestor-server/nv-ingest-results/{collection_name}/{filename}.results.jsonl.gz\n",
    "```\n",
    "\n",
    "Each `.results.jsonl.gz` file contains the extracted content from the document including:\n",
    "- **Text chunks**: Extracted text segments with metadata\n",
    "- **Tables**: Structured table data extracted from the document\n",
    "- **Images**: Base64-encoded images with optional captions\n",
    "- **Charts**: Detected charts with extracted data\n",
    "\n",
    "Let's explore the structure of one result file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML, Image\n",
    "import base64\n",
    "\n",
    "# Configure paths\n",
    "RESULTS_BASE_DIR = Path(\"./deploy/compose/volumes/ingestor-server/nv-ingest-results\")\n",
    "COLLECTION_TO_STUDY = COLLECTION_NAME  # Uses the collection name from earlier cells\n",
    "\n",
    "# List available result files for this collection\n",
    "collection_results_dir = RESULTS_BASE_DIR / COLLECTION_TO_STUDY\n",
    "if collection_results_dir.exists():\n",
    "    result_files = list(collection_results_dir.glob(\"*.results.jsonl.gz\"))\n",
    "    print(f\"üìÅ Found {len(result_files)} result file(s) in collection '{COLLECTION_TO_STUDY}':\\n\")\n",
    "    for f in result_files:\n",
    "        print(f\"  ‚Ä¢ {f.name}\")\n",
    "else:\n",
    "    result_files = []\n",
    "    print(f\"‚ùå Results directory not found: {collection_results_dir}\")\n",
    "    print(\"   Make sure APP_NVINGEST_SAVETODISK=True was set before ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e26b1",
   "metadata": {},
   "source": [
    "**Generate PDF with Extracted Content:**\n",
    "\n",
    "This cell creates a PDF containing all extracted text and images from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import base64\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Install fpdf2 in the correct environment\n",
    "import subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fpdf2\", \"-q\"])\n",
    "\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Create results_study folder\n",
    "RESULTS_STUDY_DIR = Path(\"results_study\")\n",
    "RESULTS_STUDY_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Path to the specific result file (adjust path as needed)\n",
    "RESULT_FILE = Path(\"deploy/compose/volumes/ingestor-server/nv-ingest-results\") / COLLECTION_NAME / \"sample_ai_article.pdf.results.jsonl.gz\"\n",
    "\n",
    "# Load the file\n",
    "extracted_content = []\n",
    "try:\n",
    "    with gzip.open(RESULT_FILE, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                extracted_content.append(json.loads(line))\n",
    "except gzip.BadGzipFile:\n",
    "    with open(str(RESULT_FILE).replace('.gz', ''), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                extracted_content.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(extracted_content)} items from {RESULT_FILE.name}\")\n",
    "\n",
    "# Save full JSON results\n",
    "output_json = RESULTS_STUDY_DIR / \"sample_ai_article_extracted.json\"\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(extracted_content, f, indent=2)\n",
    "\n",
    "# Create PDF with extracted content\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font('Helvetica', 'B', 12)\n",
    "        self.cell(0, 10, 'Extracted Content: sample_ai_article.pdf', border=False, ln=True, align='C')\n",
    "        self.ln(5)\n",
    "    \n",
    "    def footer(self):\n",
    "        self.set_y(-15)\n",
    "        self.set_font('Helvetica', 'I', 8)\n",
    "        self.cell(0, 10, f'Page {self.page_no()}', align='C')\n",
    "\n",
    "pdf = PDF()\n",
    "pdf.set_auto_page_break(auto=True, margin=15)\n",
    "pdf.add_page()\n",
    "\n",
    "# Content breakdown\n",
    "content_types = {}\n",
    "for item in extracted_content:\n",
    "    doc_type = item.get('document_type', 'unknown')\n",
    "    content_types[doc_type] = content_types.get(doc_type, 0) + 1\n",
    "\n",
    "pdf.set_font('Helvetica', 'B', 11)\n",
    "pdf.cell(0, 8, f'Total extracted items: {len(extracted_content)}', ln=True)\n",
    "pdf.ln(3)\n",
    "pdf.set_font('Helvetica', '', 10)\n",
    "for doc_type, count in sorted(content_types.items()):\n",
    "    pdf.cell(0, 6, f'  - {doc_type}: {count}', ln=True)\n",
    "pdf.ln(5)\n",
    "pdf.cell(0, 0, '', border='T', ln=True)\n",
    "pdf.ln(10)\n",
    "\n",
    "# Add each item\n",
    "for i, item in enumerate(extracted_content):\n",
    "    doc_type = item.get('document_type', 'unknown')\n",
    "    \n",
    "    # Section header\n",
    "    pdf.set_font('Helvetica', 'B', 10)\n",
    "    pdf.set_fill_color(230, 230, 230)\n",
    "    pdf.cell(0, 8, f'[Item {i+1}] Type: {doc_type}', ln=True, fill=True)\n",
    "    pdf.ln(3)\n",
    "    \n",
    "    if 'metadata' in item:\n",
    "        meta = item['metadata']\n",
    "        \n",
    "        # Handle text content\n",
    "        if 'content' in meta and doc_type == 'text':\n",
    "            pdf.set_font('Helvetica', '', 9)\n",
    "            content = meta['content']\n",
    "            content = content.encode('latin-1', 'replace').decode('latin-1')\n",
    "            pdf.multi_cell(0, 5, content)\n",
    "            pdf.ln(5)\n",
    "        \n",
    "        # Handle images\n",
    "        elif doc_type == 'image' and 'content' in meta:\n",
    "            try:\n",
    "                img_data = base64.b64decode(meta['content'])\n",
    "                temp_img = RESULTS_STUDY_DIR / f\"temp_img_{i}.png\"\n",
    "                with open(temp_img, 'wb') as img_f:\n",
    "                    img_f.write(img_data)\n",
    "                \n",
    "                pdf.image(str(temp_img), w=min(180, pdf.epw))\n",
    "                \n",
    "                if 'image_metadata' in meta and meta['image_metadata'].get('caption'):\n",
    "                    pdf.set_font('Helvetica', 'I', 8)\n",
    "                    caption = meta['image_metadata']['caption']\n",
    "                    caption = caption.encode('latin-1', 'replace').decode('latin-1')\n",
    "                    pdf.multi_cell(0, 4, f\"Caption: {caption}\")\n",
    "                \n",
    "                temp_img.unlink()\n",
    "                pdf.ln(5)\n",
    "            except Exception as e:\n",
    "                pdf.set_font('Helvetica', 'I', 9)\n",
    "                pdf.cell(0, 6, f'[Image could not be rendered: {e}]', ln=True)\n",
    "        \n",
    "        # Handle tables/structured content\n",
    "        elif doc_type == 'structured' and 'content' in meta:\n",
    "            pdf.set_font('Courier', '', 8)\n",
    "            content = meta['content'][:2000]\n",
    "            content = content.encode('latin-1', 'replace').decode('latin-1')\n",
    "            pdf.multi_cell(0, 4, content)\n",
    "            pdf.ln(5)\n",
    "    \n",
    "    pdf.ln(3)\n",
    "\n",
    "# Save PDF\n",
    "output_pdf = RESULTS_STUDY_DIR / \"sample_ai_article_extracted.pdf\"\n",
    "pdf.output(str(output_pdf))\n",
    "\n",
    "print(f\"\\nResults saved to '{RESULTS_STUDY_DIR}/' folder:\")\n",
    "print(f\"   - {output_json.name} (full JSON)\")\n",
    "print(f\"   - {output_pdf.name} (PDF with text & images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d6de2",
   "metadata": {},
   "source": [
    "After running the cell above, open the `results_study/` folder to explore:\n",
    "\n",
    "- **sample_ai_article_extracted.json** - Raw JSON with all extracted data (text, images, metadata)\n",
    "- **sample_ai_article_extracted.pdf** - Formatted PDF showing text content and embedded images\n",
    "\n",
    "You can modify the `RESULT_FILE` path to study other documents in your collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dbc35d",
   "metadata": {},
   "source": [
    "### 4.6 Test Search Endpoint (OpenAI-Compatible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search(query: str, collection: str = \"multimodal_data\", top_k: int = 3) -> dict:\n",
    "    \"\"\"Search for relevant document chunks without generating an answer.\"\"\"\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"collection_names\": [collection],\n",
    "        \"reranker_top_k\": top_k\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(f\"{RAG_BASE_URL}/v1/search\", json=payload) as resp:\n",
    "            return await resp.json()\n",
    "\n",
    "import base64\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "def is_base64_image(content: str) -> bool:\n",
    "    \"\"\"Check if content is base64 image data.\"\"\"\n",
    "    return content.startswith((\"/9j/\", \"iVBOR\", \"data:image\"))\n",
    "\n",
    "def display_content(content: str, max_len: int = 200):\n",
    "    \"\"\"Display content - renders images, truncates text.\"\"\"\n",
    "    if is_base64_image(content):\n",
    "        # Decode and display image\n",
    "        try:\n",
    "            # Handle data:image prefix if present\n",
    "            if content.startswith(\"data:image\"):\n",
    "                content = content.split(\",\", 1)[1]\n",
    "            img_data = base64.b64decode(content)\n",
    "            display(IPImage(data=img_data, width=400))\n",
    "        except Exception as e:\n",
    "            print(f\"    [Image decode error: {e}]\")\n",
    "    else:\n",
    "        # Display text\n",
    "        text = content[:max_len].replace(\"\\n\", \" \").strip()\n",
    "        text = text + \"...\" if len(content) > max_len else text\n",
    "        print(f\"    {text}\")\n",
    "\n",
    "# Test the search endpoint\n",
    "print(\"Testing /v1/search endpoint...\")\n",
    "print(\"=\" * 60)\n",
    "results = await search(\"What is artificial intelligence?\", top_k=3)\n",
    "\n",
    "print(f\"Found {len(results.get('results', []))} results:\\n\")\n",
    "for i, result in enumerate(results.get(\"results\", []), 1):\n",
    "    score = result.get(\"score\", 0)\n",
    "    content = result.get(\"content\", \"\")\n",
    "    doc_name = result.get(\"document_name\", \"unknown\")\n",
    "    content_type = result.get(\"type\", \"text\")\n",
    "    \n",
    "    print(f\"[{i}] Score: {score:.4f} | Doc: {doc_name} | Type: {content_type}\")\n",
    "    display_content(content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb53c26e",
   "metadata": {},
   "source": [
    "### 4.7 Compare: LLM vs RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8724291",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What are the main approaches to artificial intelligence?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WITHOUT RAG (LLM general knowledge only)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Q: {QUESTION}\")\n",
    "print(\"A: \", end=\"\")\n",
    "await chat(QUESTION, use_rag=False)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"WITH RAG (searches your documents first)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Q: {QUESTION}\")\n",
    "print(\"A: \", end=\"\")\n",
    "await chat(QUESTION, use_rag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2883b3a",
   "metadata": {},
   "source": [
    "### 4.8 Ask Your Own Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7114a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this question and run the cell\n",
    "my_question = \"What is machine learning?\"\n",
    "\n",
    "print(f\"Q: {my_question}\")\n",
    "print(\"A: \", end=\"\")\n",
    "await chat(my_question, use_rag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6be1e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Cleanup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce564b9",
   "metadata": {},
   "source": [
    "### 5.1 Delete Uploaded Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b20f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all uploaded documents\n",
    "doc_names = [filename for filename, _ in SAMPLE_DOCS]\n",
    "await delete_document(doc_names, collection=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ed8d2e",
   "metadata": {},
   "source": [
    "**Delete Collections:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ccbe8-b500-48d0-a14d-7103683e4766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all collections and delete each one\n",
    "collections = await list_collections()\n",
    "\n",
    "if collections:\n",
    "    print(f\"\\nDeleting {len(collections)} collection(s)...\")\n",
    "    for c in collections:\n",
    "        name = c.get(\"collection_name\", c) if isinstance(c, dict) else c\n",
    "        await delete_collection(name)\n",
    "    \n",
    "    print(\"\\n‚úÖ All collections deleted\")\n",
    "    \n",
    "    # Verify\n",
    "    await list_collections()\n",
    "else:\n",
    "    print(\"No collections to delete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d3d26",
   "metadata": {},
   "source": [
    "### 5.2 Stop All Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4363e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26aa1a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Quick Reference\n",
    "\n",
    "---\n",
    "\n",
    "## Function Cheat Sheet\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `deploy_all()` | Start all services |\n",
    "| `stop_all()` | Stop all services |\n",
    "| `check_containers()` | List running containers |\n",
    "| `await chat(question)` | Ask a question (`use_rag=True` by default) |\n",
    "| `await chat(question, use_rag=False)` | Ask LLM directly without document search |\n",
    "| `await upload_documents(paths)` | Upload file(s) or directory to knowledge base |\n",
    "| `await list_documents()` | List uploaded files |\n",
    "| `await delete_document(name)` | Remove a file |\n",
    "| `await list_collections()` | List all collections |\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| Containers not starting | Run `docker logs <container-name>` to see errors |\n",
    "| Out of GPU memory | Run `stop_all()`, or use NVIDIA-hosted LLM |\n",
    "| API not responding | Wait for \"healthy\" status, check `docker logs rag-server` |\n",
    "| Upload stuck | Check `docker logs ingestor-server` |\n",
    "\n",
    "---\n",
    "\n",
    "## Service Ports\n",
    "\n",
    "| Service | Port |\n",
    "|---------|------|\n",
    "| RAG Server | 8081 |\n",
    "| Ingestor Server | 8082 |\n",
    "| RAG Frontend | 8090 |\n",
    "| Milvus | 19530 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1b162",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "---\n",
    "\n",
    "Now that you've deployed the RAG Blueprint and tested basic functionality, explore these notebooks to learn more advanced features:\n",
    "\n",
    "## üü¢ Beginner - Learn the APIs\n",
    "\n",
    "| Notebook | Description |\n",
    "|----------|-------------|\n",
    "| **ingestion_api_usage.ipynb** | Deep dive into the ingestion service - upload, process, and manage documents |\n",
    "| **retriever_api_usage.ipynb** | Explore query techniques, retrieval strategies, and search parameters |\n",
    "\n",
    "## üü° Intermediate - Extend Functionality\n",
    "\n",
    "| Notebook | Description |\n",
    "|----------|-------------|\n",
    "| **summarization.ipynb** | Document summarization with page filtering and multiple strategies |\n",
    "| **nb_metadata.ipynb** | Metadata ingestion, filtering, and extraction for enhanced retrieval |\n",
    "| **rag_library_usage.ipynb** | Native Python client usage - full end-to-end API examples |\n",
    "| **rag_library_lite_usage.ipynb** | Containerless deployment with Milvus Lite (no Docker required) |\n",
    "| **evaluation_01_ragas.ipynb** | Evaluate RAG quality using RAGAS metrics |\n",
    "| **evaluation_02_recall.ipynb** | Measure retrieval recall at various top-k thresholds |\n",
    "\n",
    "## üî¥ Advanced - Build & Customize\n",
    "\n",
    "| Notebook | Description |\n",
    "|----------|-------------|\n",
    "| **image_input.ipynb** | Multimodal queries with text + images (VLM embeddings) |\n",
    "| **building_rag_vdb_operator.ipynb** | Create custom vector database operators (e.g., OpenSearch) |\n",
    "| **mcp_server_usage.ipynb** | Use Model Context Protocol (MCP) transports instead of REST APIs |\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- **Documentation**: https://docs.nvidia.com/ai-blueprints/rag/latest/\n",
    "- **GitHub**: https://github.com/NVIDIA-AI-Blueprints/rag\n",
    "- **Support Matrix**: https://docs.nvidia.com/ai-blueprints/rag/latest/support-matrix.html\n",
    "\n",
    "---\n",
    "\n",
    "**Happy building! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
