{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650fa9f0",
   "metadata": {},
   "source": [
    "## Notebook: Metadata usage, Metadata extraction.\n",
    "\n",
    "This notebook illustrates exercising the metadata feature of the solution. It walks through :-\n",
    "* Ingestion of a few documents with metadata\n",
    "* Q & A with no metadata filtering\n",
    "* Q & A with metadata filtering\n",
    "* **[Extra]** Example of extracting metadata from user queries for inclusion in the RAG /generate API call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58505d5-5436-449a-b316-b943a1a57797",
   "metadata": {},
   "source": [
    "### Install Dependencies and import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78246c4e-040d-4e06-8ed3-edb88ca0c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c0758-fcf8-4c62-a3c4-efa93ef0122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a494be7-ffee-4dfb-968f-c5300f6ba0a2",
   "metadata": {},
   "source": [
    "### Base Configuration\n",
    "\n",
    "* Helper functions in the following cell\n",
    "* The code assumes a docker installation of the RAG Blueprint on the same server that is running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807ea21-f9b8-408b-b2ee-318bef308d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPADDRESS = \"localhost\" # Replace this with the correct IP address if required\n",
    "INGESTOR_SERVER_PORT = \"8082\"\n",
    "INGESTOR_BASE_URL = f\"http://{IPADDRESS}:{INGESTOR_SERVER_PORT}\"  # Replace with your server URL if required\n",
    "\n",
    "async def print_response(response, to_print=True):\n",
    "    \"\"\"Helper to print API response.\"\"\"\n",
    "    try:\n",
    "        response_json = await response.json()\n",
    "        if to_print:\n",
    "            print(json.dumps(response_json, indent=2))\n",
    "        return response_json\n",
    "    except aiohttp.ClientResponseError:\n",
    "        print(await response.text())\n",
    "\n",
    "\n",
    "RAG_SERVER_PORT = \"8081\"\n",
    "RAG_BASE_URL = f\"http://{IPADDRESS}:{RAG_SERVER_PORT}\"  # Replace with your server URL\n",
    "\n",
    "rag_url = f\"{RAG_BASE_URL}/v1/generate\"\n",
    "\n",
    "import json\n",
    "import base64\n",
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "async def print_streaming_response_and_citations(response_generator):\n",
    "    first_chunk_data = None\n",
    "\n",
    "    async for chunk in response_generator:\n",
    "        if chunk.startswith(\"data: \"):\n",
    "            chunk = chunk[len(\"data: \"):].strip()\n",
    "\n",
    "        if not chunk:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data = json.loads(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            print(f\"⚠️ Raw chunk content: {repr(chunk)}\")\n",
    "            continue\n",
    "\n",
    "        choices = data.get(\"choices\", [])\n",
    "        if not choices:\n",
    "            continue\n",
    "\n",
    "        # Capture first chunk with citations (if any)\n",
    "        if first_chunk_data is None and data.get(\"citations\"):\n",
    "            first_chunk_data = data\n",
    "\n",
    "        # Stream the content\n",
    "        delta = choices[0].get(\"delta\", {})\n",
    "        text = delta.get(\"content\")\n",
    "        if not text:\n",
    "            message = choices[0].get(\"message\", {})\n",
    "            text = message.get(\"content\", \"\")\n",
    "        print(text, end='', flush=True)\n",
    "\n",
    "    print()  # Newline after completion\n",
    "\n",
    "    # Display citations if any\n",
    "    if first_chunk_data and first_chunk_data.get(\"citations\"):\n",
    "        citations = first_chunk_data[\"citations\"]\n",
    "        for idx, citation in enumerate(citations.get(\"results\", [])):\n",
    "            doc_type = citation.get(\"document_type\", \"text\")\n",
    "            content = citation.get(\"content\", \"\")\n",
    "            doc_name = citation.get(\"document_name\", f\"Citation {idx+1}\")\n",
    "\n",
    "            display(Markdown(f\"\\n**Citation {idx+1}: {doc_name}**\"))\n",
    "\n",
    "            if doc_type == \"image\":\n",
    "                try:\n",
    "                    image_bytes = base64.b64decode(content)\n",
    "                    display(Image(data=image_bytes))\n",
    "                except Exception as e:\n",
    "                    display(Markdown(f\"⚠️ Failed to render image:\\n```\\n{e}\\n```\"))\n",
    "            else:\n",
    "                display(Markdown(f\"```\\n{content}\\n```\"))\n",
    "\n",
    "async def generate_answer(payload):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=rag_url, json=payload) as response:\n",
    "                buffer = \"\"\n",
    "                async for chunk in response.content.iter_chunked(1024):\n",
    "                    buffer += chunk.decode()\n",
    "                    while \"\\n\" in buffer:\n",
    "                        line, buffer = buffer.split(\"\\n\", 1)\n",
    "                        yield line.strip()\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f85b3-767b-4e8f-82da-9d5c7069d609",
   "metadata": {},
   "source": [
    "### Ensure the solution is up and running \n",
    "#### Health Check Endpoint\n",
    "\n",
    "**Purpose:**\n",
    "This endpoint performs a health check on the server. It returns a 200 status code if the server is operational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e53c4-909b-4b04-975f-c598f33bd797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def fetch_health_status():\n",
    "    \"\"\"Fetch health status asynchronously.\"\"\"\n",
    "    url = f\"{INGESTOR_BASE_URL}/v1/health\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            await print_response(response)\n",
    "\n",
    "# Run the async function\n",
    "await fetch_health_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2850cbb2",
   "metadata": {},
   "source": [
    "### Create collection with the specified metadata schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e658845",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"cars\"\n",
    "\n",
    "async def create_collection(\n",
    "    collection_name: list = None,\n",
    "    embedding_dimension: int = 2048,\n",
    "    metadata_schema: list = []\n",
    "):\n",
    "\n",
    "    data = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"embedding_dimension\": embedding_dimension,\n",
    "        \"metadata_schema\": metadata_schema\n",
    "    }\n",
    "\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(f\"{INGESTOR_BASE_URL}/v1/collection\", json=data, headers=HEADERS) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            return 500, {\"error\": str(e)}\n",
    "\n",
    "metadata_schema = [\n",
    "    {\n",
    "        \"name\": \"manufacturer\",\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"manufacturer\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"model\",\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"model\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"year\",\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"year\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the collection\n",
    "await create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    metadata_schema=metadata_schema # Optional argument, can be commented if metadata is not to be inserted\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334abfc-5832-4e39-8793-818b5265aa1d",
   "metadata": {},
   "source": [
    "### Prepare the files and metadata\n",
    "\n",
    "* Download files\n",
    "* Prepare the metadata\n",
    "* Upload files into the newly created collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709ab2e-c1de-42d4-96b2-eb104f8bd6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "# Mapping of filenames to source URLs\n",
    "files_to_download = {\n",
    "    '2024_Ford_Escape_Owners_Manual_version_1_om_EN-US.pdf': 'https://www.fordservicecontent.com/Ford_Content/Catalog/owner_information/2024_Ford_Escape_Owners_Manual_version_1_om_EN-US.pdf',\n",
    "    '2023_Edge_Owners_Manual_version_2_om_EN-US.pdf': 'https://www.fordservicecontent.com/Ford_Content/Catalog/owner_information/2023_Edge_Owners_Manual_version_2_om_EN-US.pdf',\n",
    "    '2015-Edge-Owner-Manual-version-2_om_EN-US_06_2015.pdf': 'https://www.fordservicecontent.com/Ford_Content/Catalog/owner_information/2015-Edge-Owner-Manual-version-2_om_EN-US_06_2015.pdf'\n",
    "}\n",
    "\n",
    "print(\"Downloading files ...\")\n",
    "for filename, url in files_to_download.items():\n",
    "    destination = os.path.join('./data', filename)\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(destination, 'wb') as f:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"✅ Downloaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to download {filename} - HTTP {response.status_code}\")\n",
    "\n",
    "FILEPATHS = [\n",
    "    \"./data/2015-Edge-Owner-Manual-version-2_om_EN-US_06_2015.pdf\",\n",
    "    \"./data/2023_Edge_Owners_Manual_version_2_om_EN-US.pdf\",\n",
    "    \"./data/2024_Ford_Escape_Owners_Manual_version_1_om_EN-US.pdf\"\n",
    "]\n",
    "\n",
    "CUSTOM_METADATA = [\n",
    "    {\n",
    "        \"filename\": \"2015-Edge-Owner-Manual-version-2_om_EN-US_06_2015.pdf\",\n",
    "        \"metadata\": {\n",
    "            \"manufacturer\": \"ford\",\n",
    "            \"model\": \"edge\",\n",
    "            \"year\": 2015\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"filename\": \"2023_Edge_Owners_Manual_version_2_om_EN-US.pdf\",\n",
    "        \"metadata\": {\n",
    "            \"manufacturer\": \"ford\",\n",
    "            \"model\": \"edge\",\n",
    "            \"year\": 2023\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"filename\": \"2024_Ford_Escape_Owners_Manual_version_1_om_EN-US.pdf\",\n",
    "        \"metadata\": {\n",
    "            \"manufacturer\": \"ford\",\n",
    "            \"model\": \"escape\",\n",
    "            \"year\": 2024\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def stringify_metadata_values(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: stringify_metadata_values(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [stringify_metadata_values(item) for item in obj]\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "print(\"Stringifying relevant metadata ...\")\n",
    "\n",
    "for entry in CUSTOM_METADATA:\n",
    "    entry[\"metadata\"] = stringify_metadata_values(entry[\"metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6cb74-62c3-44be-a096-3029421c5122",
   "metadata": {},
   "source": [
    "### Upload Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d611db2-c663-4014-bd6e-fb0a279a04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import json\n",
    "import os\n",
    "\n",
    "async def upload_documents(collection_name: str = \"\") -> str:\n",
    "    data = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"blocking\": False,\n",
    "        \"split_options\": {\n",
    "            \"chunk_size\": 512,\n",
    "            \"chunk_overlap\": 150\n",
    "        },\n",
    "        \"custom_metadata\": CUSTOM_METADATA,\n",
    "        \"generate_summary\": False\n",
    "    }\n",
    "\n",
    "    form_data = aiohttp.FormData()\n",
    "    for file_path in FILEPATHS:\n",
    "        form_data.add_field(\n",
    "            \"documents\",\n",
    "            open(file_path, \"rb\"),\n",
    "            filename=os.path.basename(file_path),\n",
    "            content_type=\"application/pdf\"\n",
    "        )\n",
    "\n",
    "    form_data.add_field(\"data\", json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(f\"{INGESTOR_BASE_URL}/v1/documents\", data=form_data) as response:\n",
    "                resp_json = await response.json()\n",
    "                print(\"Response:\", resp_json)\n",
    "                return resp_json.get(\"task_id\")  # return the task_id\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "async def get_task_status(\n",
    "    task_id: str\n",
    "):\n",
    "\n",
    "    params = {\n",
    "        \"task_id\": task_id,\n",
    "    }\n",
    "\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(f\"{INGESTOR_BASE_URL}/v1/status\", params=params, headers=HEADERS) as response:\n",
    "                returnval = await print_response(response, False)\n",
    "                return returnval\n",
    "        except aiohttp.ClientError as e:\n",
    "            return 500, {\"error\": str(e)}\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def wait_until_task_complete(task_id):\n",
    "    while True:\n",
    "        result = await get_task_status(task_id=[task_id])\n",
    "        \n",
    "        if result is None:\n",
    "            print(\"❌ No response received, retrying...\")\n",
    "            await asyncio.sleep(5)\n",
    "            continue\n",
    "\n",
    "        state = result.get(\"state\", None)\n",
    "        print(f\"Current state: {state}\")\n",
    "\n",
    "        if state != \"PENDING\":\n",
    "            print(\"✅ Task completed.\")\n",
    "            break\n",
    "        \n",
    "        await asyncio.sleep(15)  # wait 5 seconds before polling again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c7a14-9672-423b-bcd2-642f88de5bb5",
   "metadata": {},
   "source": [
    "### Wait for the documents to be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a7c26-4f63-4087-8d1d-a4b02724c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = await upload_documents(collection_name=COLLECTION_NAME)\n",
    "print(\"Upload documents ...task_id: {}\".format(task_id))\n",
    "if task_id:\n",
    "    await get_task_status(task_id=task_id)\n",
    "else:\n",
    "    print(\"⚠️ Upload failed or no task_id returned.\")\n",
    "    \n",
    "await wait_until_task_complete(task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8a0bc-224b-4482-8d9b-83a5624d4039",
   "metadata": {},
   "source": [
    "### Fetch documents\n",
    "\n",
    "Ensuring the files exist in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250bcc6-2137-40d2-95ff-6202faff4fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def fetch_documents(collection_name: str = \"\"):\n",
    "    url = f\"{INGESTOR_BASE_URL}/v1/documents\"\n",
    "    params = {\"collection_name\": collection_name}\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(url, params=params) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "await fetch_documents(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc9372-1e4e-46fa-bd18-57aa4a00f0ad",
   "metadata": {},
   "source": [
    "### Query: No specification of metadata etc. \n",
    "\n",
    "Notice the response citations come from manuals belonging to the \"escape\" and the \"edge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19604de1-687c-4bb9-886c-bbfffff3dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What precautions should you take when jump starting the edge 2015?\"\n",
    "    }\n",
    "  ],\n",
    "  \"use_knowledge_base\": True,\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"reranker_top_k\": 4,\n",
    "  \"vdb_top_k\": 10,\n",
    "  \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "  \"collection_names\": [\"cars\"],\n",
    "  \"enable_query_rewriting\": True,\n",
    "  \"enable_reranker\": True,\n",
    "  \"enable_citations\": True,\n",
    "  \"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1\",\n",
    "  \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "  \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "  # Provide url of the model endpoints if deployed elsewhere\n",
    "  # \"llm_endpoint\": \"\",\n",
    "  #\"embedding_endpoint\": \"\",\n",
    "  #\"reranker_endpoint\": \"\",\n",
    "  \"stop\": [],\n",
    "  \"filter_expr\": ''\n",
    "}\n",
    "await print_streaming_response_and_citations(generate_answer(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0804f6ea-7c2b-4d4d-8c0a-20270185f8d7",
   "metadata": {},
   "source": [
    "Notice: The results are from the 2015 and the 2023 car models. The question was specifically for the 2015 model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c3994-6042-4c81-ab44-d38d82786895",
   "metadata": {},
   "source": [
    "### Query: Specification of simple metadata\n",
    "\n",
    "In the previous query the responses were from multiple car models but the user really wanted the response only for their car model \n",
    "which might be the \"edge\"\n",
    "Direct the query to a given car model i.e \"edge\"\n",
    "\n",
    "Notice the citations are confined to the \"edge\" model.\n",
    "We successfully used the metadata to limit the search to \"edge\" models alone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18296d2a-835d-4ced-a9d0-709b81015821",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What precautions should you take when jump starting the edge 2015?\"\n",
    "    }\n",
    "  ],\n",
    "  \"use_knowledge_base\": True,\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"reranker_top_k\": 2,\n",
    "  \"vdb_top_k\": 10,\n",
    "  \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "  \"collection_names\": [\"cars\"],\n",
    "  \"enable_query_rewriting\": True,\n",
    "  \"enable_reranker\": True,\n",
    "  \"enable_citations\": True,\n",
    "  \"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1\",\n",
    "  \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "  \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "  # Provide url of the model endpoints if deployed elsewhere\n",
    "  # \"llm_endpoint\": \"\",\n",
    "  #\"embedding_endpoint\": \"\",\n",
    "  #\"reranker_endpoint\": \"\",\n",
    "  \"stop\": [],\n",
    "  \"filter_expr\": 'content_metadata[\"model\"] == \"edge\"'\n",
    "}\n",
    "await print_streaming_response_and_citations(generate_answer(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba72239-8fc0-4840-b2f4-ecb94af4a0f9",
   "metadata": {},
   "source": [
    "Now notice, the answer generated and all citations are from the edge manuals, but they also refer to the 2023 year model despite being asked about the \"2015 edge\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842e6b2-d6a9-4f50-9ab4-e54e016a35c7",
   "metadata": {},
   "source": [
    "### Query: Specification of compound metadata\n",
    "\n",
    "Notice in the previous example we got results from the 2015 and 2023 \"edge\" model. \n",
    "We'd likely want to get th results from a specific model of the car. \n",
    "\n",
    "In the next example we limit the query to a \"2015 edge\" model. \n",
    "Notice, the citations are only from a 2015 edge model car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb5583-f217-4994-a356-2808e2d992fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What precautions should you take when jump starting the edge 2015?\"\n",
    "    }\n",
    "  ],\n",
    "  \"use_knowledge_base\": True,\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"reranker_top_k\": 2,\n",
    "  \"vdb_top_k\": 10,\n",
    "  \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "  \"collection_names\": [\"cars\"],\n",
    "  \"enable_query_rewriting\": True,\n",
    "  \"enable_reranker\": True,\n",
    "  \"enable_citations\": True,\n",
    "  \"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1\",\n",
    "  \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "  \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "  # Provide url of the model endpoints if deployed elsewhere\n",
    "  # \"llm_endpoint\": \"\",\n",
    "  #\"embedding_endpoint\": \"\",\n",
    "  #\"reranker_endpoint\": \"\",\n",
    "  \"stop\": [],\n",
    "  \"filter_expr\": 'content_metadata[\"model\"] == \"edge\" and content_metadata[\"year\"] == \"2015\"'\n",
    "}\n",
    "await print_streaming_response_and_citations(generate_answer(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb38e1-bf0d-4b53-af3b-73f34b43afd2",
   "metadata": {},
   "source": [
    "### Extra: Determine the metadata from the query.\n",
    "\n",
    "It can be envisioned that the relevant metadata flags could be extracted from a user query (wherever applicable)\n",
    "The below cell provides an example of how an LLM could be used to extract metadata K-V pairs that could further be used to build a filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a54c0-e00a-4d38-bb2d-4a57f61ebd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"\"\"Extract elements from the user query if and only if they exist.\n",
    "           There are two possible elements: \"year\" and \"model\".\n",
    "           Return a dictionary containing only the elements found.\n",
    "           Omit any keys that are not present in the query.\n",
    "           All returned strings must be lowercase.\n",
    "           Valid output examples:\n",
    "           {\"year\": \"2023\", \"model\": \"edge\"}\n",
    "           {}\n",
    "           {\"year\": \"2023\"}\n",
    "           {\"model\": \"edge\"}\n",
    "           The only allowed values are:\n",
    "           For \"year\": \"2015\", \"2023\".\n",
    "           For \"model\": \"edge\", \"escape\".\n",
    "           User Query:\n",
    "           \"How do you enable and use the Rear Occupant Alert System in the 2015 escape?\"\n",
    "           The response should be \"model\": \"escape\", \"year\": \"2023\"\n",
    "        \"\"\"\n",
    "    }\n",
    "  ],\n",
    "  \"use_knowledge_base\": False,\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"reranker_top_k\": 2,\n",
    "  \"vdb_top_k\": 10,\n",
    "  \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "  \"collection_names\": [\"cars\"],\n",
    "  \"enable_query_rewriting\": False,\n",
    "  \"enable_reranker\": False,\n",
    "  \"enable_citations\": False,\n",
    "  \"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1\",\n",
    "  \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "  \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "  # Provide url of the model endpoints if deployed elsewhere\n",
    "  # \"llm_endpoint\": \"\",\n",
    "  #\"embedding_endpoint\": \"\",\n",
    "  #\"reranker_endpoint\": \"\",\n",
    "  \"stop\": [],\n",
    "}\n",
    "extracted_metadata = await print_streaming_response_and_citations(generate_answer(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f4a38a-8874-40ce-a098-360733496663",
   "metadata": {},
   "source": [
    "Notice the response is {\"model\": \"escape\", \"year\": \"2023\"} which can then be used \n",
    "to <b>construct the query filter</b>. The LLM could easily be used to generated the \"filter\" itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191cd7f-02d1-45c6-90a7-f3ca86c4046a",
   "metadata": {},
   "source": [
    "### Cleanup: Delete the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eab18f-505d-4813-a332-f0a211aa12a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "async def delete_collections(collection_names: List[str] = \"\"):\n",
    "    url = f\"{INGESTOR_BASE_URL}/v1/collections\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.delete(url, json=collection_names) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "await delete_collections(collection_names=[COLLECTION_NAME])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
