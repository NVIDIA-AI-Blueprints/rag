{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NAT + NVIDIA RAG MCP end-to-end notebook\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Start the NVIDIA RAG MCP server in `streamable_http` mode.\n",
        "2. Create a collection and upload a sample document via the MCP client.\n",
        "3. Configure a NAT workflow using `nvidia_rag_mcp.yaml`.\n",
        "4. Start a NAT server that uses the RAG MCP server as a tool.\n",
        "5. Call the NAT workflow with a natural language query.\n",
        "6. Clean up the collection and stop the NAT and MCP servers.\n",
        "\n",
        "**Prerequisites**\n",
        "\n",
        "- End-to-end NVIDIA RAG workflow is up and running (per the [RAG quickstart](https://github.com/NVIDIA-AI-Blueprints/rag/blob/develop/docs/deploy-docker-self-hosted.md)).\n",
        "- `nat` CLI is installed and on your `PATH` (to install it, see the [NeMo Agent Toolkit installation guide](https://docs.nvidia.com/nemo/agent-toolkit/latest/quick-start/installing.html)).\n",
        "- MCP server/client dependencies are installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: ensure MCP dependencies are installed in this environment.\n",
        "# You can skip this cell if you've already installed them.\n",
        "# Uncomment and run the next line if needed:\n",
        "# %pip install -r ../nvidia_rag_mcp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Start NVIDIA RAG MCP server in streamable_http mode on port 9902.\n",
        "\n",
        "This cell launches the MCP server as a background subprocess and\n",
        "registers a cleanup hook to terminate it when the kernel shuts down.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import atexit\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "MCP_PORT = 9902\n",
        "MCP_HOST = \"127.0.0.1\"\n",
        "\n",
        "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "server_path = os.path.join(repo_root, \"nvidia_rag_mcp\", \"mcp_server.py\")\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    server_path,\n",
        "    \"--transport\",\n",
        "    \"streamable_http\",\n",
        "    \"--host\",\n",
        "    MCP_HOST,\n",
        "    \"--port\",\n",
        "    str(MCP_PORT),\n",
        "]\n",
        "\n",
        "print(\"Launching MCP server:\", \" \".join(cmd))\n",
        "\n",
        "mcp_proc = subprocess.Popen(cmd)\n",
        "atexit.register(lambda: mcp_proc and mcp_proc.poll() is None and mcp_proc.terminate())\n",
        "\n",
        "time.sleep(2.0)\n",
        "print(f\"MCP server PID: {mcp_proc.pid} (http://{MCP_HOST}:{MCP_PORT}/mcp)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Create a collection and upload a sample document via the MCP client.\n",
        "\n",
        "This cell uses the MCP client in streamable_http mode pointed at the\n",
        "MCP server started above on port 9902.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "\n",
        "STREAMABLE_HTTP_URL = \"http://127.0.0.1:9902/mcp\"\n",
        "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "client_path = os.path.join(repo_root, \"nvidia_rag_mcp\", \"mcp_client.py\")\n",
        "\n",
        "COLLECTION = \"my_collection\"\n",
        "pdf_path = os.path.join(repo_root, \"data\", \"multimodal\", \"product_catalog.pdf\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Creating collection via MCP...\")\n",
        "print(\"=\" * 80)\n",
        "create_args = json.dumps({\"collection_names\": [COLLECTION]})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=streamable_http\",\n",
        "    f\"--url={STREAMABLE_HTTP_URL}\",\n",
        "    \"--tool=create_collections\",\n",
        "    f\"--json-args={create_args}\",\n",
        "], check=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Uploading document via MCP...\")\n",
        "print(\"=\" * 80)\n",
        "upload_args = json.dumps({\n",
        "    \"collection_name\": COLLECTION,\n",
        "    \"file_paths\": [pdf_path],\n",
        "    \"blocking\": True,\n",
        "    \"generate_summary\": True,\n",
        "    \"split_options\": {\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=streamable_http\",\n",
        "    f\"--url={STREAMABLE_HTTP_URL}\",\n",
        "    \"--tool=upload_documents\",\n",
        "    f\"--json-args={upload_args}\",\n",
        "], check=False)\n",
        "\n",
        "print(\"\\nDone setting up collection and document for NAT + MCP demo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save `nvidia_rag_mcp.yaml` for NAT\n",
        "\n",
        "Create a file named `nvidia_rag_mcp.yaml` in your project root (for example, in the same directory where you will run `nat serve`) and paste the following content, then save the file.\n",
        "\n",
        "At a high level this YAML:\n",
        "- **Defines an MCP client** (`function_groups.nvidia_rag_mcp`) that connects to the NVIDIA RAG MCP server over `streamable-http` at `http://localhost:9902/mcp` and exposes the `generate` tool to NAT.\n",
        "- **Configures the LLM** (`llms.nim_llm`) that the agent uses when deciding how to call the MCP tool.\n",
        "- **Builds a ReAct agent workflow** (`workflow`) that wires the `nvidia_rag_mcp` tool group and `nim_llm` model together.\n",
        "\n",
        "You normally only need to change this file if you:\n",
        "- Point the MCP server to a different host/port (update the `url`), or\n",
        "- Want to use a different LLM (update `model_name` and related settings).\n",
        "\n",
        "```yaml\n",
        "\n",
        "function_groups:\n",
        "  nvidia_rag_mcp:\n",
        "    _type: mcp_client\n",
        "    server:\n",
        "      transport: streamable-http\n",
        "      url: \"http://localhost:9902/mcp\"\n",
        "    include:\n",
        "      - generate\n",
        "\n",
        "llms:\n",
        "  nim_llm:\n",
        "    _type: nim\n",
        "    model_name: meta/llama-3.1-70b-instruct\n",
        "    temperature: 0.0\n",
        "    max_tokens: 1024\n",
        "\n",
        "workflow:\n",
        "  _type: react_agent\n",
        "  tool_names:\n",
        "    - nvidia_rag_mcp\n",
        "  llm_name: nim_llm\n",
        "  verbose: true\n",
        "  retry_parsing_errors: true\n",
        "  max_retries: 3\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Start the NAT server\n",
        "\n",
        "In a separate terminal (from the project root where `nvidia_rag_mcp.yaml` is saved), start the NAT server pointing at the MCP config:\n",
        "\n",
        "```bash\n",
        "nat serve --config_file nvidia_rag_mcp.yaml --port 8000\n",
        "```\n",
        "\n",
        "Leave this process running while you execute the next cell to send a query via `nat run`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run `nat run`\n",
        "\n",
        "Open a separate terminal **in the directory where `nvidia_rag_mcp.yaml` is saved** and run:\n",
        "\n",
        "```bash\n",
        "nat run --config_file nvidia_rag_mcp.yaml --input \"My rag question is: Tell me about Ratan Basket Shoulder Bag. Use collection my_collection\"\n",
        "```\n",
        "\n",
        "This will invoke the workflow defined in `nvidia_rag_mcp.yaml`, which uses the NVIDIA RAG MCP server (running on `http://localhost:9902/mcp`) to answer based on the document you uploaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Delete the demo collection via the MCP client.\n",
        "\n",
        "Run this cell after you are done experimenting with the RAG collection.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "\n",
        "STREAMABLE_HTTP_URL = \"http://127.0.0.1:9902/mcp\"\n",
        "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "client_path = os.path.join(repo_root, \"nvidia_rag_mcp\", \"mcp_client.py\")\n",
        "\n",
        "COLLECTION = \"my_collection\"\n",
        "\n",
        "print(\"Deleting collection via MCP...\\n\")\n",
        "delete_args = json.dumps({\"collection_names\": [COLLECTION]})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=streamable_http\",\n",
        "    f\"--url={STREAMABLE_HTTP_URL}\",\n",
        "    \"--tool=delete_collections\",\n",
        "    f\"--json-args={delete_args}\",\n",
        "], check=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stop the NAT server\n",
        "\n",
        "To stop the NAT server you started with:\n",
        "\n",
        "```bash\n",
        "nat serve --config_file nvidia_rag_mcp.yaml --port 8000\n",
        "```\n",
        "\n",
        "Go to that terminal and press `Ctrl+C` to terminate it.\n",
        "\n",
        "Alternatively, from a shell in the same machine, you can locate and kill the process explicitly, for example:\n",
        "\n",
        "```bash\n",
        "ps aux | grep \"nat serve --config_file nvidia_rag_mcp.yaml --port 8000\"\n",
        "# Note the PID from the output, then run\n",
        "kill <PID>\n",
        "``` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Stop the MCP server started earlier in this notebook.\n",
        "\n",
        "This cell uses the `mcp_proc` handle created in the server-start cell.\n",
        "If the process is still running, it will be terminated.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    mcp_proc\n",
        "except NameError:\n",
        "    print(\"No MCP server process handle (`mcp_proc`) found in this session.\")\n",
        "else:\n",
        "    if mcp_proc is not None and mcp_proc.poll() is None:\n",
        "        print(f\"Terminating MCP server PID {mcp_proc.pid}...\")\n",
        "        mcp_proc.terminate()\n",
        "        try:\n",
        "            mcp_proc.wait(timeout=10)\n",
        "        except Exception:\n",
        "            print(\"MCP server did not exit in time; sending kill()...\")\n",
        "            mcp_proc.kill()\n",
        "        print(\"MCP server stopped.\")\n",
        "    else:\n",
        "        print(\"MCP server process is not running.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Troubleshooting\n",
        "\n",
        "- **MCP server fails to start or hangs**\n",
        "  - Check that NVIDIA RAG and Ingestor services are up and healthy (see the [RAG quickstart](https://github.com/NVIDIA-AI-Blueprints/rag/blob/develop/docs/deploy-docker-self-hosted.md)).\n",
        "  - Ensure port `9902` is free: `fuser -k 9902/tcp` (Linux) and rerun the server cell.\n",
        "\n",
        "- **MCP client calls (create/upload/delete) fail**\n",
        "  - Verify the MCP server cell printed a valid PID and URL `http://127.0.0.1:9902/mcp`.\n",
        "  - Run the MCP client commands from the notebook again and inspect any error text in the cell output.\n",
        "\n",
        "- **`nat serve` cannot connect to MCP**\n",
        "  - Confirm the MCP server cell is still running (no error in its output, and the stop-MCP cell reports it as running).\n",
        "  - Make sure the `url` in `nvidia_rag_mcp.yaml` matches the MCP server address (default: `http://localhost:9902/mcp`).\n",
        "\n",
        "- **`nat run` errors or returns empty/irrelevant answers**\n",
        "  - Confirm the collection (`my_collection`) exists and the upload cell completed successfully.\n",
        "  - Adjust the `--input` prompt to clearly mention the collection name and the document you uploaded.\n",
        "\n",
        "- **Environment or dependency issues**\n",
        "  - Ensure `nat` is installed and on your `PATH` (see the [NeMo Agent Toolkit installation guide](https://docs.nvidia.com/nemo/agent-toolkit/latest/quick-start/installing.html)).\n",
        "  - (Optional) Rerun the first pip cell to install MCP dependencies in the current environment."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
