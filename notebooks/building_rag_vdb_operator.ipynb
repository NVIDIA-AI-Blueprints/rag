{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Custom Vector Database Operators for NVIDIA RAG\n",
    "## Overview\n",
    "This is an advanced notebook to demonstrate how to create and integrate custom vector database (VDB) operators with the NVIDIA RAG blueprint. This will guide you to build your own VDB implementations that work with `NvidiaRAG` and `NvidiaRAGIngestor` components.\n",
    "## Key Topics Covered\n",
    "- **VDB Operator Architecture** – Understanding the `VDBRag` base class and required interfaces\n",
    "- **Custom Implementation** – Building a complete [OpenSearch VDB](https://opensearch.org/) operator from scratch\n",
    "- **Integration Patterns** – Connecting your custom VDB with NVIDIA RAG pipelines\n",
    "- **Best Practices** – Patterns with error handling and lifecycle management\n",
    "## Prerequisites\n",
    "- NVIDIA NGC API key for accessing models\n",
    "- Docker for running dependent services (Milvus, NIMs, etc.)\n",
    "- Python environment with NVIDIA RAG package installed\n",
    "- Basic understanding of vector databases and RAG concepts\n",
    "**Note:** This pattern can be adapted for any vector database by implementing the same interface methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for NVIDIA RAG Python Package\n",
    "Please refer to [rag_library_usage.ipynb](./rag_library_usage.ipynb) for detailed installation instructions for rag libary.\n",
    "**Quick install (in a Python venv):**\n",
    "```bash\n",
    "# activate python venv using uv\n",
    "uv pip install nvidia-rag[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the dependencies\n",
    "After the environment for the python package is set up, we launch all the dependent services and NIMs that the pipeline depends on.\n",
    "Fulfill the [prerequisites here](../docs/deploy-docker-self-hosted.md) to set up docker on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the default configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install python-dotenv\n",
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your `NGC_API_KEY` after executing the cell below. You can obtain a key by following steps [here](../docs/api-key.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset if needed\n",
    "if os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NGC_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    candidate_api_key = getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert candidate_api_key.startswith(\"nvapi-\"), (\n",
    "        f\"{candidate_api_key[:5]}... is not a valid key\"\n",
    "    )\n",
    "    os.environ[\"NGC_API_KEY\"] = candidate_api_key\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = candidate_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to nvcr.io which is needed for pulling the containers of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup OpenSearch Vector DB using Docker Compose\n",
    "Follow these steps in cells below to set up OpenSearch as your vector database:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1. Create Docker Compose Configuration**\n",
    "Create a `docker-compose-opensearch.yaml` file in `deploy/compose/` directory by running the below cell.\n",
    "**[Optional] Advanced Configuration**\n",
    "- **Memory Settings**: Adjust `OPENSEARCH_JAVA_OPTS` based on your system resources\n",
    "- **Security**: Security plugin is disabled for simplicity. Enable for production use\n",
    "- **Network**: Uses `nvidia-rag` network for integration with other services\n",
    "- **Data Persistence**: To keep data peristent, the opensearch data can be mounted to external volume with following steps:\n",
    "  - Create volume directory and provide required permissions:\n",
    "    ```bash\n",
    "    sudo mkdir -p deploy/compose/volumes/opensearch/\n",
    "    sudo chmod -R 777 deploy/compose/volumes/opensearch/\n",
    "    ```\n",
    "  - Mount volume:\n",
    "    ```yaml\n",
    "        volumes:\n",
    "          - ./volumes/opensearch:/usr/share/opensearch/data/\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_content = \"\"\"services:\n",
    "  opensearch:\n",
    "    image: opensearchproject/opensearch:3.1.0\n",
    "    ports:\n",
    "      - \"9200:9200\"\n",
    "      - \"9300:9300\"\n",
    "    environment:\n",
    "      - cluster.name=opensearch-cluster\n",
    "      - node.name=opensearch-node\n",
    "      - discovery.type=single-node\n",
    "      - bootstrap.memory_lock=true\n",
    "      - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m -XX:MaxDirectMemorySize=10g\"\n",
    "      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=\"myStrongPassword123@456\"\n",
    "      - DISABLE_SECURITY_PLUGIN=true\n",
    "    healthcheck:\n",
    "      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:9200\" ]\n",
    "      interval: 30s\n",
    "      timeout: 20s\n",
    "      retries: 3\n",
    "    profiles: [\"opensearch\"]\n",
    "networks:\n",
    "  default:\n",
    "    name: nvidia-rag\n",
    "\"\"\"\n",
    "with open(\"../deploy/compose/docker-compose-opensearch.yaml\", \"w\") as f:\n",
    "    f.write(yaml_file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2. Start OpenSearch Service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start OpenSearch with the opensearch profile\n",
    "# Make sure elasticsearch container is not running (check with `docker ps`), since it uses same port\n",
    "# ! docker stop <elasticsearch_container_name> # Uncomment this if elasticsearch is running\n",
    "!docker compose -f ../deploy/compose/docker-compose-opensearch.yaml --profile opensearch up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4. Verify OpenSearch is Running and Healthy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if OpenSearch is Running\n",
    "!docker ps | grep opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if OpenSearch is healthy\n",
    "!curl -X GET \"localhost:9200/_cluster/health?pretty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5: Install OpenSearch Python Client**\n",
    "Install the `opensearch-py` client in your current environment. This allows your Python code to connect to and interact with the OpenSearch service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenSearch Python Client in your environment\n",
    "!uv pip install opensearch-py # Use `pip install opensearch-py` if you are not using uv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure MinIO is running (required for Citations)\n",
    "!docker compose -f ../deploy/compose/vectordb.yaml --profile minio up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup the NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: Deploy on-prem models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to Option 2 if you are interested in using NVIDIA-hosted models.\n",
    "Ensure you meet [the hardware requirements](../docs/support-matrix.md). By default the NIMs are configured to use 2xH100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model cache directory\n",
    "!mkdir -p ~/.cache/model-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MODEL_DIRECTORY environment variable in the Python kernel\n",
    "import os\n",
    "os.environ[\"MODEL_DIRECTORY\"] = os.path.expanduser(\"~/.cache/model-cache\")\n",
    "print(\"MODEL_DIRECTORY set to:\", os.environ[\"MODEL_DIRECTORY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPU IDs for the various microservices if needed\n",
    "os.environ[\"EMBEDDING_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"RANKING_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_GRAPHICS_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_TABLE_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"OCR_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"LLM_MS_GPU_ID\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying NIMs - This may take a while as models download. If kernel times out, just rerun this cell.\n",
    "!USERID=$(id -u) docker compose -f ../deploy/compose/nims.yaml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the status of running containers (run this cell repeatedly or in a terminal)\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure all the below are running and healthy before proceeding further\n",
    "```output\n",
    "NAMES                           STATUS\n",
    "nemoretriever-ranking-ms        Up ... (healthy)\n",
    "compose-page-elements-1         Up ...\n",
    "compose-paddle-1                Up ...\n",
    "compose-graphic-elements-1      Up ...\n",
    "compose-table-structure-1       Up ...\n",
    "nemoretriever-embedding-ms      Up ... (healthy)\n",
    "nim-llm-ms                      Up ... (healthy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Using Nvidia Hosted models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OCR_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-ocr\"\n",
    "os.environ[\"OCR_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v3\"\n",
    ")\n",
    "os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"http\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup the Nvidia Ingest runtime and redis service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker compose -f ../deploy/compose/docker-compose-ingestor-server.yaml up nv-ingest-ms-runtime redis -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Custom VDB Operator - Library Mode\n",
    "Create a custom Vector Database operator by implementing the `VDBRag` interface, then pass it to `NvidiaRAG`/`NvidiaRAGIngestor`.\n",
    "Refer to [`docs/change-vectordb.md`](../docs/change-vectordb.md#integrate-in-library-mode-developer-friendly) for detailed steps to integrate your own vector database with Nvidia RAG in library mode.\n",
    "**Note:** \n",
    "The implementation below demonstrates integration in library mode.\n",
    "If you wish to proceed with server mode, please refer to the documentation here: [Integrate Into NVIDIA RAG (Server Mode)](../docs/change-vectordb.md#integrate-into-nvidia-rag-server-mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining your own VDBRag class for Opensearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Helper functions for OpenSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for OpenSearch query construction.\n",
    "This cell provides utility functions to generate OpenSearch queries for:\n",
    "- Retrieving all unique document sources (`get_unique_sources_query`)\n",
    "- Deleting a metadata schema by collection name (`get_delete_metadata_schema_query`)\n",
    "- Fetching the metadata schema for a specific collection (`get_metadata_schema_query`)\n",
    "These helpers are used by the custom VDB operator to manage and query metadata in OpenSearch.\n",
    "\"\"\"\n",
    "def get_unique_sources_query():\n",
    "    \"\"\"\n",
    "    Generate aggregation query to retrieve all unique document sources.\n",
    "    \"\"\"\n",
    "    query_unique_sources = {\n",
    "        \"size\": 0,\n",
    "        \"aggs\": {\n",
    "            \"unique_sources\": {\n",
    "                \"composite\": {\n",
    "                    \"size\": 1000,  # Adjust size depending on number of unique values\n",
    "                    \"sources\": [\n",
    "                        {\n",
    "                            \"source_name\": {\n",
    "                                \"terms\": {\n",
    "                                    \"field\": \"metadata.source.source_name.keyword\"\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "                \"aggs\": {\n",
    "                    \"top_hit\": {\n",
    "                        \"top_hits\": {\n",
    "                            \"size\": 1  # Just one document per source_name\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    return query_unique_sources\n",
    "def get_delete_metadata_schema_query(collection_name: str):\n",
    "    \"\"\"\n",
    "    Create deletion query for removing metadata schema by collection name.\n",
    "    \"\"\"\n",
    "    query_delete_metadata_schema = {\n",
    "        \"query\": {\"term\": {\"collection_name.keyword\": collection_name}}\n",
    "    }\n",
    "    return query_delete_metadata_schema\n",
    "def get_metadata_schema_query(collection_name: str):\n",
    "    \"\"\"\n",
    "    Build search query to retrieve metadata schema for specified collection.\n",
    "    \"\"\"\n",
    "    query_metadata_schema = {\"query\": {\"term\": {\"collection_name\": collection_name}}}\n",
    "    return query_metadata_schema\n",
    "def get_delete_docs_query(source_value: str):\n",
    "    \"\"\"\n",
    "    Construct deletion query for documents matching the source value.\n",
    "    \"\"\"\n",
    "    query_delete_documents = {\n",
    "        \"query\": {\"term\": {\"metadata.source.source_name.keyword\": source_value}}\n",
    "    }\n",
    "    return query_delete_documents\n",
    "def create_metadata_collection_mapping():\n",
    "    \"\"\"Generate Elasticsearch index mapping for metadata schema collections.\"\"\"\n",
    "    return {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"collection_name\": {\n",
    "                    \"type\": \"keyword\"  # or \"text\" depending on your search needs\n",
    "                },\n",
    "                \"metadata_schema\": {\n",
    "                    \"type\": \"object\",  # For JSON-like structure\n",
    "                    \"enabled\": True,  # Set to False if you don't want to index its fields\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "def get_delete_document_info_query(collection_name: str, document_name: str, info_type: str):\n",
    "    \"\"\"\n",
    "    Create deletion query for removing document info entries.\n",
    "    This helper function constructs an OpenSearch query to delete document info\n",
    "    entries matching the specified collection name, document name, and info type.\n",
    "    Used for cleaning up document info when documents are deleted or updated.\n",
    "    Args:\n",
    "        collection_name (str): The name of the collection to delete document info for.\n",
    "        document_name (str): The name of the document to delete info for.\n",
    "        info_type (str): The type of info to delete (e.g., \"document\", \"collection\", \"catalog\").\n",
    "    Returns:\n",
    "        dict: OpenSearch query dictionary for delete_by_query operation.\n",
    "    Example:\n",
    "        >>> query = get_delete_document_info_query(\"my_collection\", \"doc.pdf\", \"document\")\n",
    "        >>> vdb.opensearch_vs.client.delete_by_query(index=\"document_info\", body=query)\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"term\": {\"collection_name\": collection_name}},\n",
    "                    {\"term\": {\"document_name\": document_name}},\n",
    "                    {\"term\": {\"info_type\": info_type}},\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "def get_document_info_query(collection_name: str, document_name: str, info_type: str):\n",
    "    \"\"\"\n",
    "    Create search query for retrieving document info entries.\n",
    "    This helper function constructs an OpenSearch query to search for document info\n",
    "    entries matching the specified collection name, document name, and info type.\n",
    "    Used by get_document_info to retrieve stored document metadata.\n",
    "    Args:\n",
    "        collection_name (str): The name of the collection to retrieve document info for.\n",
    "        document_name (str): The name of the document to retrieve info for.\n",
    "        info_type (str): The type of info to retrieve (e.g., \"document\", \"collection\", \"catalog\").\n",
    "    Returns:\n",
    "        dict: OpenSearch query dictionary for search operation.\n",
    "    Example:\n",
    "        >>> query = get_document_info_query(\"my_collection\", \"doc.pdf\", \"document\")\n",
    "        >>> response = vdb.opensearch_vs.client.search(index=\"document_info\", body=query)\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"term\": {\"collection_name\": collection_name}},\n",
    "                    {\"term\": {\"document_name\": document_name}},\n",
    "                    {\"term\": {\"info_type\": info_type}},\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "def get_collection_document_info_query(info_type: str, collection_name: str):\n",
    "    \"\"\"\n",
    "    Create search query for retrieving document info by info type and collection name.\n",
    "    This helper function constructs an OpenSearch query to retrieve all document info\n",
    "    entries of a specific type for a given collection. Used for aggregating collection-level\n",
    "    information from multiple document entries.\n",
    "    Args:\n",
    "        info_type (str): The type of info to retrieve (e.g., \"collection\", \"catalog\").\n",
    "        collection_name (str): The name of the collection to retrieve info for.\n",
    "    Returns:\n",
    "        dict: OpenSearch query dictionary for search operation.\n",
    "    Example:\n",
    "        >>> query = get_collection_document_info_query(\"collection\", \"my_collection\")\n",
    "        >>> response = vdb.opensearch_vs.client.search(index=\"document_info\", body=query)\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"term\": {\"collection_name\": collection_name}},\n",
    "                    {\"term\": {\"info_type\": info_type}},\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "def get_delete_document_info_query_by_collection_name(collection_name: str):\n",
    "    \"\"\"\n",
    "    Create deletion query for removing all document info entries for a collection.\n",
    "    This helper function constructs an OpenSearch query to delete all document info\n",
    "    entries associated with a specific collection. Used when deleting entire collections\n",
    "    to clean up associated metadata.\n",
    "    Args:\n",
    "        collection_name (str): The name of the collection to delete all document info for.\n",
    "    Returns:\n",
    "        dict: OpenSearch query dictionary for delete_by_query operation.\n",
    "    Example:\n",
    "        >>> query = get_delete_document_info_query_by_collection_name(\"my_collection\")\n",
    "        >>> vdb.opensearch_vs.client.delete_by_query(index=\"document_info\", body=query)\n",
    "    \"\"\"\n",
    "    return {\"query\": {\"term\": {\"collection_name\": collection_name}}}\n",
    "def create_document_info_collection_mapping():\n",
    "    \"\"\"\n",
    "    Generate OpenSearch index mapping for document info collections.\n",
    "    This function creates the index mapping configuration for the document_info\n",
    "    collection, which stores metadata about documents and collections. The mapping\n",
    "    defines the field types and indexing strategies for efficient querying.\n",
    "    Returns:\n",
    "        dict: OpenSearch index mapping configuration dictionary.\n",
    "    Example:\n",
    "        >>> mapping = create_document_info_collection_mapping()\n",
    "        >>> vdb.opensearch_vs.client.indices.create(index=\"document_info\", body=mapping)\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"collection_name\": {\"type\": \"keyword\"},\n",
    "                \"document_name\": {\"type\": \"keyword\"},\n",
    "                \"info_type\": {\"type\": \"keyword\"},\n",
    "                \"info_value\": {\"type\": \"object\", \"enabled\": True},\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the OpenSearch `VDBRagIngest` class**\n",
    "The following cell contains the implementation of the `OpenSearchVDB` class that inherits from `VDBRagIngest`. \n",
    "**Architecture Notes:**\n",
    "- `VDBRagIngest` combines two base classes:\n",
    "  - `VDBRag`: Pure abstract base class for RAG vector database operations\n",
    "  - `VDB` (from nv_ingest_client): Interface required for nv-ingest ingestion operations\n",
    "- **Important**: Inheriting from `VDBRagIngest` (not just `VDBRag`) is required for the class to work with `NvidiaRAGIngestor`\n",
    "- The class implements VDB interface methods (`run`, `write_to_index`, `create_index`, `retrieval`, `reindex`)\n",
    "- Custom metadata can be passed directly as a pandas DataFrame via `meta_dataframe` parameter\n",
    "For a comprehensive understanding of the functionality and design of each method, review the respective docstrings provided with each implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from typing import Any\n",
    "from concurrent.futures import Future\n",
    "\n",
    "from nvidia_rag.utils.vdb.vdb_ingest_base import VDBRagIngest\n",
    "from nvidia_rag.utils.vdb import DEFAULT_METADATA_SCHEMA_COLLECTION, DEFAULT_DOCUMENT_INFO_COLLECTION, SYSTEM_COLLECTIONS\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableAssign, RunnableLambda\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from nv_ingest_client.util.milvus import cleanup_records\n",
    "from opentelemetry import context as otel_context\n",
    "logger = logging.getLogger(__name__)\n",
    "class OpenSearchVDB(VDBRagIngest):\n",
    "    \"\"\"\n",
    "    OpenSearchVDB is a vector database implementation using OpenSearch for RAG (Retrieval-Augmented Generation) applications.\n",
    "    This class provides comprehensive functionality for document ingestion, indexing, retrieval, and metadata management\n",
    "    using OpenSearch as the backend vector database. It supports embedding-based similarity search, metadata filtering,\n",
    "    and collection management operations.\n",
    "    Inherits from VDBRagIngest to provide a standardized interface for vector database operations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        opensearch_url=\"http://localhost:9200\",\n",
    "        index_name=\"test\",\n",
    "        meta_dataframe=None,\n",
    "        meta_source_field=None,\n",
    "        meta_fields=None,\n",
    "        embedding_model=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the OpenSearchVDB instance with connection parameters and metadata configuration.\n",
    "        Args:\n",
    "            opensearch_url (str, optional): The URL endpoint for the OpenSearch cluster.\n",
    "                                          Defaults to \"http://localhost:9200\".\n",
    "            index_name (str, optional): The name of the OpenSearch index to use for storing documents.\n",
    "                                      Defaults to \"test\".\n",
    "            meta_dataframe (pandas.DataFrame, optional): DataFrame containing metadata information for documents.\n",
    "                                                       Used for enriching document metadata during ingestion.\n",
    "            meta_source_field (str, optional): The field name in meta_dataframe that corresponds to document sources.\n",
    "                                             Used for joining metadata with documents.\n",
    "            meta_fields (list, optional): List of metadata field names to extract and store with documents.\n",
    "                                        These fields will be searchable and filterable.\n",
    "            embedding_model (object, optional): The embedding model instance used for generating vector embeddings.\n",
    "                                              Must be compatible with langchain embedding interfaces.\n",
    "        Returns:\n",
    "            None: This is a constructor method that initializes the instance.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the specified OpenSearch URL.\n",
    "            ValueError: If invalid parameters are provided for index_name or embedding_model.\n",
    "        Example:\n",
    "            >>> from sentence_transformers import SentenceTransformer\n",
    "            >>> embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            >>> vdb = OpenSearchVDB(\n",
    "            ...     opensearch_url=\"http://localhost:9200\",\n",
    "            ...     index_name=\"my_documents\",\n",
    "            ...     embedding_model=embedding_model\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        self.opensearch_url = opensearch_url\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index_name = index_name\n",
    "        self.opensearch_vs = self.get_langchain_vectorstore(\n",
    "            collection_name=index_name\n",
    "        )\n",
    "        self.meta_dataframe = meta_dataframe\n",
    "        self.meta_source_field = meta_source_field\n",
    "        self.meta_fields = meta_fields\n",
    "        self._es_connection = self.opensearch_vs.client\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @property\n",
    "    def collection_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the current collection name (index name) for the OpenSearch vector database.\n",
    "        This property provides a standardized interface to access the collection name,\n",
    "        which is internally stored as index_name in OpenSearch terminology.\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            str: The current collection/index name being used for document storage and retrieval.\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(index_name=\"my_collection\")\n",
    "            >>> print(vdb.collection_name)\n",
    "            'my_collection'\n",
    "        \"\"\"\n",
    "        return self.index_name\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @collection_name.setter\n",
    "    def collection_name(self, collection_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Set the collection name (index name) for the OpenSearch vector database.\n",
    "        This property setter allows changing the target collection/index for subsequent\n",
    "        operations. The change affects all future document storage and retrieval operations.\n",
    "        Args:\n",
    "            collection_name (str): The new collection/index name to use. Must be a valid\n",
    "                                 OpenSearch index name (lowercase, no spaces, valid characters).\n",
    "        Returns:\n",
    "            None: This is a setter method that modifies the instance state.\n",
    "        Raises:\n",
    "            ValueError: If the collection_name contains invalid characters for OpenSearch index names.\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB()\n",
    "            >>> vdb.collection_name = \"new_collection\"\n",
    "            >>> print(vdb.collection_name)\n",
    "            'new_collection'\n",
    "        \"\"\"\n",
    "        self.index_name = collection_name\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def create_index(\n",
    "        self,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a new OpenSearch index if it doesn't already exist.\n",
    "        This method initializes a new OpenSearch index with predefined settings optimized\n",
    "        for vector similarity search using FAISS engine. The index is created with 2048\n",
    "        dimensions to accommodate typical embedding models.\n",
    "        Args:\n",
    "            **kwargs: Additional keyword arguments for VDB interface compatibility.\n",
    "                     Supports 'dimension' to override the default embedding dimension.\n",
    "        Returns:\n",
    "            None: This method performs index creation as a side effect.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to create indices on the cluster.\n",
    "            ValueError: If the index_name contains invalid characters.\n",
    "        Side Effects:\n",
    "            - Creates a new OpenSearch index if it doesn't exist\n",
    "            - Configures the index with FAISS engine for vector search\n",
    "            - Sets dimension to 2048 for vector embeddings (unless overridden)\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(index_name=\"my_new_index\")\n",
    "            >>> vdb.create_index()  # Creates index if not exists\n",
    "        \"\"\"\n",
    "        dimension = kwargs.get(\"dimension\", 2048)\n",
    "        if not self.check_collection_exists(self.index_name):\n",
    "            self.opensearch_vs.create_index(\n",
    "                index_name=self.index_name,\n",
    "                dimension=dimension,\n",
    "                engine=\"faiss\"\n",
    "            )\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def write_to_index(self, records: list, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Write processed records with embeddings to the OpenSearch index.\n",
    "        This method processes raw records by cleaning them, extracting text and vector embeddings,\n",
    "        organizing metadata, and then storing everything in the OpenSearch index. The records are\n",
    "        preprocessed using the cleanup_records utility to ensure consistent formatting.\n",
    "        Args:\n",
    "            records (list): List of record dictionaries containing document data. Each record should have:\n",
    "                          - 'text': The document text content\n",
    "                          - 'vector': Pre-computed embedding vector (list/array of floats)\n",
    "                          - 'source': Source identifier for the document\n",
    "                          - 'content_metadata': Additional metadata as dictionary\n",
    "            **kwargs: Additional keyword arguments (currently unused but available for extension)\n",
    "        Returns:\n",
    "            None: This method performs bulk indexing as a side effect.\n",
    "        Raises:\n",
    "            ValueError: If records are malformed or missing required fields.\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            IndexError: If the target index doesn't exist or is misconfigured.\n",
    "        Side Effects:\n",
    "            - Processes and cleans input records using configured metadata settings\n",
    "            - Bulk inserts documents with embeddings into OpenSearch index\n",
    "            - Refreshes the index to make documents immediately searchable\n",
    "            - Logs the number of successfully added records\n",
    "        Example:\n",
    "            >>> records = [\n",
    "            ...     {\n",
    "            ...         'text': 'Sample document text',\n",
    "            ...         'vector': [0.1, 0.2, ...],  # 2048-dim embedding\n",
    "            ...         'source': 'doc1.pdf',\n",
    "            ...         'content_metadata': {'page': 1, 'section': 'intro'}\n",
    "            ...     }\n",
    "            ... ]\n",
    "            >>> vdb.write_to_index(records)\n",
    "        \"\"\"\n",
    "        # Clean up and flatten records to pull appropriate fields from the records\n",
    "        cleaned_records = cleanup_records(\n",
    "            records=records,\n",
    "            meta_dataframe=self.meta_dataframe,\n",
    "            meta_source_field=self.meta_source_field,\n",
    "            meta_fields=self.meta_fields,\n",
    "        )\n",
    "        # Prepare texts, embeddings, and metadatas from cleaned records\n",
    "        texts, embeddings, metadatas = [], [], []\n",
    "        for cleaned_record in cleaned_records:\n",
    "            texts.append(cleaned_record.get(\"text\"))\n",
    "            embeddings.append(cleaned_record.get(\"vector\"))\n",
    "            metadatas.append(\n",
    "                {\n",
    "                    \"source\": cleaned_record.get(\"source\"),\n",
    "                    \"content_metadata\": cleaned_record.get(\"content_metadata\"),\n",
    "                }\n",
    "            )\n",
    "        # Add texts, embeddings, and metadatas to the OpenSearch index\n",
    "        self.opensearch_vs.add_embeddings(\n",
    "            text_embeddings=zip(texts, embeddings),\n",
    "            metadatas=metadatas,\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"Added {len(texts)} records to Opensearch index {self.index_name}\"\n",
    "        )\n",
    "        self.opensearch_vs.client.indices.refresh(index=self.index_name)\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def retrieval(self, queries: list, **kwargs) -> list[dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents from OpenSearch based on input queries.\n",
    "        This method performs semantic similarity search using vector embeddings to find\n",
    "        the most relevant documents for the given queries. Currently, this is a placeholder\n",
    "        method that requires implementation for the specific OpenSearchVDB use case.\n",
    "        Args:\n",
    "            queries (list): List of query strings to search for. Each query will be processed\n",
    "                          to find the most semantically similar documents in the index.\n",
    "            **kwargs: Additional keyword arguments for retrieval configuration such as:\n",
    "                    - top_k (int): Number of top results to return per query\n",
    "                    - filter_expr (dict): Metadata filters to apply during search\n",
    "                    - threshold (float): Minimum similarity score threshold\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: List of retrieved documents with their metadata and scores.\n",
    "                                Each document dict contains:\n",
    "                                - 'text': The document content\n",
    "                                - 'metadata': Document metadata including source info\n",
    "                                - 'score': Similarity score for the query\n",
    "        Raises:\n",
    "            NotImplementedError: This method is currently not implemented and serves as a placeholder.\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            ValueError: If queries are malformed or empty.\n",
    "        Example:\n",
    "            >>> queries = [\"What is machine learning?\", \"How does AI work?\"]\n",
    "            >>> results = vdb.retrieval(queries, top_k=5)\n",
    "            # Currently raises NotImplementedError\n",
    "        \"\"\"\n",
    "        # Placeholder: implement actual retrieval logic\n",
    "        raise NotImplementedError(\"retrieval must be implemented for OpenSearchVDB\")\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def reindex(self, records: list, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Reindex existing documents in the OpenSearch index with updated data or embeddings.\n",
    "        This method handles the reindexing of documents that already exist in the OpenSearch\n",
    "        index. It can be used to update document content, refresh embeddings, or modify\n",
    "        metadata without losing the original document structure.\n",
    "        Args:\n",
    "            records (list): List of record dictionaries containing updated document data.\n",
    "                          Each record should include identifiers to match existing documents.\n",
    "            **kwargs: Additional keyword arguments for reindexing configuration such as:\n",
    "                    - batch_size (int): Number of documents to process in each batch\n",
    "                    - update_embeddings (bool): Whether to regenerate embeddings\n",
    "                    - preserve_metadata (bool): Whether to keep existing metadata\n",
    "        Returns:\n",
    "            None: This method performs reindexing as a side effect.\n",
    "        Raises:\n",
    "            NotImplementedError: This method is currently not implemented and serves as a placeholder.\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            ValueError: If records are malformed or missing required identifiers.\n",
    "            IndexError: If target documents don't exist in the index.\n",
    "        Side Effects:\n",
    "            - Updates existing documents in the OpenSearch index\n",
    "            - Refreshes embeddings if specified\n",
    "            - Maintains document version history for conflict resolution\n",
    "        Example:\n",
    "            >>> updated_records = [\n",
    "            ...     {\n",
    "            ...         'id': 'doc_123',\n",
    "            ...         'text': 'Updated document content',\n",
    "            ...         'vector': [0.2, 0.3, ...],  # New embedding\n",
    "            ...         'metadata': {'version': 2}\n",
    "            ...     }\n",
    "            ... ]\n",
    "            >>> vdb.reindex(updated_records)\n",
    "            # Currently raises NotImplementedError\n",
    "        \"\"\"\n",
    "        # Placeholder: implement actual reindex logic\n",
    "        raise NotImplementedError(\"reindex must be implemented for OpenSearchVDB\")\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def run(\n",
    "        self,\n",
    "        records: list,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Execute the complete document ingestion pipeline for OpenSearch.\n",
    "        This is the main orchestration method that performs the full workflow of\n",
    "        document ingestion: creating the index if it doesn't exist, then writing\n",
    "        all provided records to the index. This method provides a simple interface\n",
    "        for bulk document processing.\n",
    "        Args:\n",
    "            records (list): List of record dictionaries containing document data to ingest.\n",
    "                          Each record should follow the format expected by write_to_index().\n",
    "        Returns:\n",
    "            None: This method performs the complete ingestion workflow as a side effect.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            ValueError: If records are malformed or missing required fields.\n",
    "            PermissionError: If insufficient permissions for index creation or document insertion.\n",
    "        Side Effects:\n",
    "            - Creates the target index if it doesn't already exist\n",
    "            - Bulk inserts all provided records into the OpenSearch index\n",
    "            - Refreshes the index to make documents immediately searchable\n",
    "            - Logs the ingestion progress and results\n",
    "        Example:\n",
    "            >>> records = [\n",
    "            ...     {'text': 'Doc 1 content', 'vector': [...], 'source': 'doc1.pdf'},\n",
    "            ...     {'text': 'Doc 2 content', 'vector': [...], 'source': 'doc2.pdf'}\n",
    "            ... ]\n",
    "            >>> vdb.run(records)  # Complete ingestion workflow\n",
    "        \"\"\"\n",
    "        self.create_index()\n",
    "        self.write_to_index(records)\n",
    "    def run_async(self, records) -> None:\n",
    "        \"\"\"\n",
    "        Run the ingestion process with a Future-based records parameter.\n",
    "        \n",
    "        This method is called by nv_ingest's vdb_upload task when records\n",
    "        are passed as a Future. The method waits for the Future to resolve\n",
    "        via records.result() before processing.\n",
    "        \n",
    "        Note: Despite the name, this is NOT an async method. The 'async'\n",
    "        refers to the fact that records may be a Future that gets resolved.\n",
    "        \n",
    "        Args:\n",
    "            records: A Future containing records to ingest, or list of records\n",
    "        \"\"\"\n",
    "        # If records is a Future, wait for it to resolve\n",
    "        if hasattr(records, 'result'):\n",
    "            records = records.result()\n",
    "        \n",
    "        # Delegate to the standard run method\n",
    "        self.run(records)\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Methods for the VDBRag class for ingestion\n",
    "    async def check_health(self) -> dict[str, Any]:\n",
    "        \"\"\"Check Opensearch database health\"\"\"\n",
    "        status = {\n",
    "            \"service\": \"Opensearch\",\n",
    "            \"url\": self.opensearch_url,\n",
    "            \"status\": \"unknown\",\n",
    "            \"error\": None,\n",
    "        }\n",
    "        if not self.opensearch_url:\n",
    "            status[\"status\"] = \"skipped\"\n",
    "            status[\"error\"] = \"No URL provided\"\n",
    "            return status\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            cluster_health = self.opensearch_vs.client.cluster.health()\n",
    "            indices = self.opensearch_vs.client.cat.indices(format=\"json\")\n",
    "            status[\"status\"] = \"healthy\"\n",
    "            status[\"latency_ms\"] = round((time.time() - start_time) * 1000, 2)\n",
    "            status[\"indices\"] = len(indices)\n",
    "            status[\"cluster_status\"] = cluster_health.get(\"status\", \"unknown\")\n",
    "        except ImportError:\n",
    "            status[\"status\"] = \"error\"\n",
    "            status[\"error\"] = (\n",
    "                \"Opensearch client not available (opensearch-py library not installed)\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            status[\"status\"] = \"error\"\n",
    "            status[\"error\"] = str(e)\n",
    "        return status\n",
    "    def create_collection(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "        dimension: int = 2048,\n",
    "        collection_type: str = \"text\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Create a new collection (index) in OpenSearch with specified configuration.\n",
    "        This method implements the VDBRag interface for collection creation, providing\n",
    "        a standardized way to create new document collections with appropriate vector\n",
    "        search configuration.\n",
    "        Args:\n",
    "            collection_name (str): Name of the collection to create. Must be a valid\n",
    "                                 OpenSearch index name (lowercase, no spaces).\n",
    "            dimension (int, optional): Dimensionality of the vector embeddings to store.\n",
    "                                     Defaults to 2048 to match common embedding models.\n",
    "            collection_type (str, optional): Type of collection being created.\n",
    "                                           Defaults to \"text\" for text document storage.\n",
    "        Returns:\n",
    "            None: This method creates the collection as a side effect.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to create indices.\n",
    "            ValueError: If collection_name contains invalid characters.\n",
    "        Side Effects:\n",
    "            - Creates a new OpenSearch index with vector search capabilities\n",
    "            - Configures the index for the specified embedding dimensions\n",
    "            - Sets up FAISS engine for efficient similarity search\n",
    "        Example:\n",
    "            >>> vdb.create_collection(\"my_documents\", dimension=768, collection_type=\"text\")\n",
    "        \"\"\"\n",
    "        self.create_index()\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def check_collection_exists(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether a specified collection (index) exists in OpenSearch.\n",
    "        This method provides a simple boolean check to determine if a collection\n",
    "        is already present in the OpenSearch cluster before attempting operations\n",
    "        that require the collection to exist.\n",
    "        Args:\n",
    "            collection_name (str): Name of the collection to check for existence.\n",
    "                                 Should be a valid OpenSearch index name.\n",
    "        Returns:\n",
    "            bool: True if the collection exists, False otherwise.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to check index existence.\n",
    "        Example:\n",
    "            >>> exists = vdb.check_collection_exists(\"my_documents\")\n",
    "            >>> if exists:\n",
    "            ...     print(\"Collection already exists\")\n",
    "            >>> else:\n",
    "            ...     vdb.create_collection(\"my_documents\")\n",
    "        \"\"\"\n",
    "        return self.opensearch_vs.client.indices.exists(index=collection_name)\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def get_collection(self) -> list[dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve comprehensive information about all collections in the OpenSearch cluster.\n",
    "        This method scans the OpenSearch cluster for all available indices (collections),\n",
    "        retrieves their document counts, metadata schemas, and collection-level information\n",
    "        including aggregated statistics and catalog metadata. It provides a complete overview\n",
    "        of the vector database contents similar to Milvus and Elasticsearch implementations.\n",
    "        Args:\n",
    "            None (operates on the entire OpenSearch cluster)\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: List of collection information dictionaries. Each dict contains:\n",
    "                - 'collection_name' (str): Name of the collection/index\n",
    "                - 'num_entities' (str): Number of documents in the collection\n",
    "                - 'metadata_schema' (list[dict]): Schema definition for metadata fields\n",
    "                - 'collection_info' (dict): Dictionary containing:\n",
    "                    - Catalog metadata (description, tags, owner, etc.) if available\n",
    "                    - Aggregated statistics (total_elements, doc_type_counts, etc.) if available\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to list indices or access metadata.\n",
    "        Side Effects:\n",
    "            - Creates metadata schema collection if it doesn't exist\n",
    "            - Creates document info collection if it doesn't exist\n",
    "            - Queries all non-hidden indices in the cluster\n",
    "            - Retrieves metadata schemas and collection info for each collection\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\")\n",
    "            >>> collections = vdb.get_collection()\n",
    "            >>> for collection in collections:\n",
    "            ...     print(f\"Collection: {collection['collection_name']}\")\n",
    "            ...     print(f\"Entities: {collection['num_entities']}\")\n",
    "            ...     print(f\"Total elements: {collection['collection_info'].get('total_elements', 0)}\")\n",
    "        \"\"\"\n",
    "        self.create_metadata_schema_collection()\n",
    "        self.create_document_info_collection()\n",
    "        \n",
    "        indices = self.opensearch_vs.client.cat.indices(format=\"json\")\n",
    "        collection_info = []\n",
    "        for index in indices:\n",
    "            index_name = index[\"index\"]\n",
    "            if index_name not in SYSTEM_COLLECTIONS:\n",
    "                if not index_name.startswith(\".\"):\n",
    "                    metadata_schema = self.get_metadata_schema(index_name)\n",
    "                    \n",
    "                    catalog_data = self.get_document_info(\n",
    "                        info_type=\"catalog\",\n",
    "                        collection_name=index_name,\n",
    "                        document_name=\"NA\",\n",
    "                    )\n",
    "                    \n",
    "                    metrics_data = self.get_document_info(\n",
    "                        info_type=\"collection\",\n",
    "                        collection_name=index_name,\n",
    "                        document_name=\"NA\",\n",
    "                    )\n",
    "                    \n",
    "                    collection_info.append({\n",
    "                        \"collection_name\": index_name,\n",
    "                        \"num_entities\": index[\"docs.count\"],\n",
    "                        \"metadata_schema\": metadata_schema,\n",
    "                        \"collection_info\": {**catalog_data, **metrics_data},\n",
    "                    })\n",
    "        return collection_info\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def delete_collections(\n",
    "        self,\n",
    "        collection_names: list[str],\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Delete multiple collections (indices) and their associated metadata from OpenSearch.\n",
    "        This method performs a comprehensive deletion of collections, removing both the\n",
    "        main document indices and their associated metadata schemas. It properly tracks\n",
    "        which collections existed and were deleted vs which were not found.\n",
    "        Args:\n",
    "            collection_names (list[str]): List of collection names to delete. Each name\n",
    "                                        should be a valid OpenSearch index name.\n",
    "        Returns:\n",
    "            dict: Deletion summary containing:\n",
    "                - 'message': Status message about the deletion process\n",
    "                - 'successful': List of successfully deleted collection names\n",
    "                - 'failed': List of dicts with collection names and error messages for failures\n",
    "                - 'total_success': Count of successful deletions\n",
    "                - 'total_failed': Count of failed deletions\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to delete indices.\n",
    "        Side Effects:\n",
    "            - Deletes the specified collections/indices from OpenSearch\n",
    "            - Removes associated metadata schemas from the metadata collection\n",
    "            - Logs the deletion results for auditing purposes\n",
    "        Example:\n",
    "            >>> result = vdb.delete_collections([\"old_docs\", \"temp_collection\"])\n",
    "            >>> print(f\"Deleted: {result['successful']}\")\n",
    "            >>> print(f\"Failed: {result['failed']}\")\n",
    "        \"\"\"\n",
    "        deleted_collections = []\n",
    "        failed_collections = []\n",
    "        \n",
    "        for collection_name in collection_names:\n",
    "            try:\n",
    "                # Check if collection exists before attempting deletion\n",
    "                if self.check_collection_exists(collection_name):\n",
    "                    # Delete the collection\n",
    "                    self.opensearch_vs.client.indices.delete(\n",
    "                        index=collection_name, ignore_unavailable=False\n",
    "                    )\n",
    "                    deleted_collections.append(collection_name)\n",
    "                    logger.info(f\"Deleted collection: {collection_name}\")\n",
    "                else:\n",
    "                    # Collection doesn't exist - add to failed list\n",
    "                    failed_collections.append({\n",
    "                        \"collection_name\": collection_name,\n",
    "                        \"error_message\": f\"Collection {collection_name} not found.\"\n",
    "                    })\n",
    "                    logger.warning(f\"Collection {collection_name} not found.\")\n",
    "            except Exception as e:\n",
    "                # Error during deletion - add to failed list\n",
    "                failed_collections.append({\n",
    "                    \"collection_name\": collection_name,\n",
    "                    \"error_message\": str(e)\n",
    "                })\n",
    "                logger.error(f\"Failed to delete collection {collection_name}: {str(e)}\")\n",
    "        \n",
    "        # Delete the metadata schema for successfully deleted collections\n",
    "        for collection_name in deleted_collections:\n",
    "            try:\n",
    "                self.opensearch_vs.client.delete_by_query(\n",
    "                    index=DEFAULT_METADATA_SCHEMA_COLLECTION,\n",
    "                    body=get_delete_metadata_schema_query(collection_name),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.warning(\n",
    "                    f\"Failed to delete metadata schema for {collection_name}: {e}\"\n",
    "                )\n",
    "                    # Delete document info for successfully deleted collections\n",
    "        try:\n",
    "            self.opensearch_vs.client.delete_by_query(\n",
    "                index=DEFAULT_DOCUMENT_INFO_COLLECTION,\n",
    "                body=get_delete_document_info_query_by_collection_name(collection_name),\n",
    "                conflicts='proceed',\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging_message = f\"Failed to delete document info for {collection_name}: {e}\"\n",
    "            logger.warning(logging_message)\n",
    "        \n",
    "        return {\n",
    "            \"message\": \"Collection deletion process completed.\",\n",
    "            \"successful\": deleted_collections,\n",
    "            \"failed\": failed_collections,\n",
    "            \"total_success\": len(deleted_collections),\n",
    "            \"total_failed\": len(failed_collections),\n",
    "        }\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def get_documents(self, collection_name: str) -> list[dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve all unique documents from the specified collection.\n",
    "        This method queries the OpenSearch index to find all unique documents based on their\n",
    "        source names. It aggregates documents by their source and returns metadata information\n",
    "        along with document-level info (statistics and metadata) for each unique document.\n",
    "        The document_info field contains aggregated statistics similar to Milvus and Elasticsearch\n",
    "        implementations.\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection/index to retrieve documents from.\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: A list of dictionaries containing document information.\n",
    "                Each dictionary has the following structure:\n",
    "                - document_name (str): The basename of the source file\n",
    "                - metadata (dict): Dictionary containing metadata fields and their values\n",
    "                - document_info (dict): Dictionary containing document-level statistics:\n",
    "                    - total_elements (int): Number of elements in the document\n",
    "                    - doc_type_counts (dict): Counts by element type (text, table, image, etc.)\n",
    "                    - file_size (int): Size of the document file\n",
    "                    - date_created (str): Document creation timestamp\n",
    "                    Returns empty dict if no document info is found\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If the collection_name is invalid or doesn't exist.\n",
    "            IndexError: If the aggregation query fails or returns unexpected results.\n",
    "        Side Effects:\n",
    "            - Queries the OpenSearch index for unique source documents\n",
    "            - Retrieves document info from the document_info collection for each document\n",
    "            - Logs informational messages if document info is not found\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(index_name=\"my_collection\")\n",
    "            >>> documents = vdb.get_documents(\"my_collection\")\n",
    "            >>> print(documents[0])\n",
    "            {\n",
    "                'document_name': 'example.pdf',\n",
    "                'metadata': {'author': 'John Doe', 'created': '2024-01-01'},\n",
    "                'document_info': {\n",
    "                    'total_elements': 10,\n",
    "                    'doc_type_counts': {'text': 5, 'table': 3, 'image': 2}\n",
    "                }\n",
    "            }\n",
    "        \"\"\"\n",
    "        metadata_schema = self.get_metadata_schema(collection_name)\n",
    "        response = self.opensearch_vs.client.search(\n",
    "            index=collection_name,\n",
    "            body=get_unique_sources_query(),\n",
    "        )\n",
    "        documents_list = []\n",
    "        for hit in response[\"aggregations\"][\"unique_sources\"][\"buckets\"]:\n",
    "            source_name = hit[\"key\"][\"source_name\"]\n",
    "            metadata = (\n",
    "                hit[\"top_hit\"][\"hits\"][\"hits\"][0][\"_source\"]\n",
    "                .get(\"metadata\", {})\n",
    "                .get(\"content_metadata\", {})\n",
    "            )\n",
    "            metadata_dict = {}\n",
    "            for metadata_item in metadata_schema:\n",
    "                metadata_name = metadata_item.get(\"name\")\n",
    "                metadata_value = metadata.get(metadata_name, None)\n",
    "                if metadata_name == \"filename\" and metadata_value is None:\n",
    "                    metadata_value = os.path.basename(source_name)\n",
    "                metadata_dict[metadata_name] = metadata_value\n",
    "            \n",
    "            doc_basename = os.path.basename(source_name)\n",
    "            documents_list.append({\n",
    "                \"document_name\": doc_basename,\n",
    "                \"metadata\": metadata_dict,\n",
    "                \"document_info\": self.get_document_info(\n",
    "                    info_type=\"document\",\n",
    "                    collection_name=collection_name,\n",
    "                    document_name=doc_basename,\n",
    "                ),\n",
    "            })\n",
    "        return documents_list\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def delete_documents(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "        source_values: list[str],\n",
    "        result_dict: dict[str, list[str]] | None = None,\n",
    "    ) -> bool:\n",
    "        \"\"\"Delete documents from a collection by source values.\n",
    "        This method removes documents from the OpenSearch index that match the provided\n",
    "        source values. It performs bulk deletion with wildcard matching support to handle\n",
    "        cases where stored paths differ from provided paths. After deletion, it also removes\n",
    "        associated document_info entries and refreshes the index to ensure changes are\n",
    "        immediately visible. This implementation matches the behavior of Milvus and Elasticsearch.\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection/index to delete documents from.\n",
    "            source_values (list[str]): List of source identifiers for documents to delete.\n",
    "                These should match the 'source' field values in the stored documents.\n",
    "                Can be full paths or just filenames (wildcard matching handles path differences).\n",
    "            result_dict (dict[str, list[str]] | None, optional): Dictionary to populate with\n",
    "                deletion results. If provided, will contain:\n",
    "                - \"deleted\": List of successfully deleted document names (basenames)\n",
    "                - \"not_found\": List of document names that were not found or failed to delete\n",
    "        Returns:\n",
    "            bool: True if the deletion operation completes successfully, False otherwise.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If collection_name is invalid or source_values is empty.\n",
    "            PermissionError: If insufficient permissions to delete documents from the index.\n",
    "        Side Effects:\n",
    "            - Removes matching documents from the OpenSearch index (tries exact match first,\n",
    "              then wildcard match by basename if exact match fails)\n",
    "            - Removes associated document_info entries for deleted documents\n",
    "            - Refreshes the index to make deletions immediately visible\n",
    "            - Populates result_dict with deletion status if provided\n",
    "            - Logs the deletion operations and document_info cleanup for audit purposes\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(index_name=\"my_collection\")\n",
    "            >>> result = {}\n",
    "            >>> success = vdb.delete_documents(\n",
    "            ...     \"my_collection\",\n",
    "            ...     [\"document1.pdf\", \"document2.pdf\"],\n",
    "            ...     result_dict=result\n",
    "            ... )\n",
    "            >>> print(f\"Deleted: {result['deleted']}\")\n",
    "            >>> print(f\"Not found: {result['not_found']}\")\n",
    "        \"\"\"\n",
    "        # Initialize result_dict if provided\n",
    "        if result_dict is not None:\n",
    "            result_dict[\"deleted\"] = []\n",
    "            result_dict[\"not_found\"] = []\n",
    "        \n",
    "        # Get existing documents to determine which ones actually exist\n",
    "        source_to_basename = {\n",
    "            source: os.path.basename(source) if \"/\" in source else source\n",
    "            for source in source_values\n",
    "        }\n",
    "        existing_doc_basenames = set()\n",
    "        \n",
    "        if result_dict is not None:\n",
    "            try:\n",
    "                all_docs = self.get_documents(collection_name)\n",
    "                existing_doc_basenames = {\n",
    "                    doc.get(\"document_name\") for doc in all_docs\n",
    "                }\n",
    "            except Exception as e:\n",
    "                logging_message = f\"Could not fetch existing documents: {e}\"\n",
    "                logger.warning(logging_message)\n",
    "        \n",
    "        # Delete documents\n",
    "        for source_value in source_values:\n",
    "            basename = source_to_basename[source_value]\n",
    "            \n",
    "            # Try exact match first\n",
    "            exact_query = get_delete_docs_query(source_value)\n",
    "            exact_result = self.opensearch_vs.client.delete_by_query(\n",
    "                index=collection_name, body=exact_query, conflicts='proceed',\n",
    "            )\n",
    "            deleted_count = exact_result.get('deleted', 0)\n",
    "            \n",
    "            # If no exact match, try wildcard match by basename\n",
    "            # This handles cases where stored path differs from provided path\n",
    "            if deleted_count == 0:\n",
    "                wildcard_query = {\n",
    "                    \"query\": {\n",
    "                        \"wildcard\": {\n",
    "                            \"metadata.source.source_name.keyword\": f\"*{basename}\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                wildcard_result = self.opensearch_vs.client.delete_by_query(\n",
    "                    index=collection_name, body=wildcard_query, conflicts='proceed',\n",
    "                )\n",
    "                deleted_count = wildcard_result.get('deleted', 0)\n",
    "            \n",
    "            # Delete document_info entry for this document if deletion was successful\n",
    "            if deleted_count > 0:\n",
    "                try:\n",
    "                    self.opensearch_vs.client.delete_by_query(\n",
    "                        index=DEFAULT_DOCUMENT_INFO_COLLECTION,\n",
    "                        body=get_delete_document_info_query(\n",
    "                            collection_name=collection_name,\n",
    "                            document_name=basename,\n",
    "                            info_type=\"document\",\n",
    "                        ),\n",
    "                        conflicts='proceed',\n",
    "                    )\n",
    "                    logging_message = f\"Deleted document_info for {basename}\"\n",
    "                    logger.info(logging_message)\n",
    "                except Exception as e:\n",
    "                    logging_message = f\"Failed to delete document_info for {basename}: {e}\"\n",
    "                    logger.warning(logging_message)\n",
    "            \n",
    "            # Track deletion status if result_dict is provided\n",
    "            if result_dict is not None:\n",
    "                if deleted_count > 0:\n",
    "                    # Document was successfully deleted\n",
    "                    result_dict[\"deleted\"].append(basename)\n",
    "                elif basename in existing_doc_basenames:\n",
    "                    # Document exists but delete failed (no chunks matched)\n",
    "                    result_dict[\"not_found\"].append(basename)\n",
    "                else:\n",
    "                    # Document doesn't exist\n",
    "                    result_dict[\"not_found\"].append(basename)\n",
    "        \n",
    "        # Refresh the index to make deletions immediately visible\n",
    "        self.opensearch_vs.client.indices.refresh(index=collection_name)\n",
    "        \n",
    "        # Add a small delay to ensure the refresh propagates\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        return True\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def create_metadata_schema_collection(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the metadata schema storage collection.\n",
    "        This method creates a dedicated OpenSearch index for storing metadata schemas\n",
    "        associated with document collections. The schema collection uses a predefined\n",
    "        mapping optimized for metadata field definitions and collection associations.\n",
    "        Args:\n",
    "            None (uses instance attributes for configuration)\n",
    "        Returns:\n",
    "            None: This method performs index creation as a side effect.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to create indices.\n",
    "            ValueError: If the default metadata schema collection name is invalid.\n",
    "        Side Effects:\n",
    "            - Creates the DEFAULT_METADATA_SCHEMA_COLLECTION index if it doesn't exist\n",
    "            - Applies the metadata collection mapping for schema storage\n",
    "            - Logs the creation status and configuration details\n",
    "        Note:\n",
    "            This is typically called once during system initialization. Subsequent calls\n",
    "            will detect the existing collection and log that it already exists.\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\")\n",
    "            >>> vdb.create_metadata_schema_collection()\n",
    "            # Creates the metadata schema storage index\n",
    "        \"\"\"\n",
    "        mapping = create_metadata_collection_mapping()\n",
    "        if not self.check_collection_exists(collection_name=DEFAULT_METADATA_SCHEMA_COLLECTION):\n",
    "            self.opensearch_vs.client.indices.create(\n",
    "                index=DEFAULT_METADATA_SCHEMA_COLLECTION, body=mapping\n",
    "            )\n",
    "            logging_message = (\n",
    "                f\"Collection {DEFAULT_METADATA_SCHEMA_COLLECTION} created \"\n",
    "                + f\"at {self.opensearch_url} with mapping {mapping}\"\n",
    "            )\n",
    "            logger.info(logging_message)\n",
    "        else:\n",
    "            logging_message = f\"Collection {DEFAULT_METADATA_SCHEMA_COLLECTION} already exists at {self.opensearch_url}\"\n",
    "            logger.info(logging_message)\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def add_metadata_schema(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "        metadata_schema: list[dict[str, Any]],\n",
    "    ) -> None:\n",
    "        \"\"\"Add metadata schema to an OpenSearch index.\n",
    "        This method stores or updates the metadata schema definition for a specific\n",
    "        collection. It first removes any existing schema for the collection, then\n",
    "        adds the new schema definition to enable consistent metadata handling.\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection to associate with this schema.\n",
    "            metadata_schema (list[dict[str, Any]]): List of metadata field definitions.\n",
    "                Each dictionary should contain field specifications like:\n",
    "                - name (str): The metadata field name\n",
    "                - type (str): The data type (string, integer, date, etc.)\n",
    "                - description (str): Optional field description\n",
    "        Returns:\n",
    "            None: This method stores the schema as a side effect.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If collection_name is invalid or metadata_schema is malformed.\n",
    "            PermissionError: If insufficient permissions to modify the metadata schema index.\n",
    "        Side Effects:\n",
    "            - Removes any existing metadata schema for the collection\n",
    "            - Stores the new metadata schema in the dedicated schema index\n",
    "            - Logs the schema addition with details for audit purposes\n",
    "        Example:\n",
    "            >>> schema = [\n",
    "            ...     {\"name\": \"author\", \"type\": \"string\", \"description\": \"Document author\"},\n",
    "            ...     {\"name\": \"created_date\", \"type\": \"date\", \"description\": \"Creation date\"}\n",
    "            ... ]\n",
    "            >>> vdb.add_metadata_schema(\"my_collection\", schema)\n",
    "            # Schema is now stored and associated with my_collection\n",
    "        \"\"\"\n",
    "        # Delete the metadata schema from the index\n",
    "        _ = self.opensearch_vs.client.delete_by_query(\n",
    "            index=DEFAULT_METADATA_SCHEMA_COLLECTION,\n",
    "            body=get_delete_metadata_schema_query(collection_name),\n",
    "        )\n",
    "        # Add the metadata schema to the index\n",
    "        data = {\n",
    "            \"collection_name\": collection_name,\n",
    "            \"metadata_schema\": metadata_schema,\n",
    "        }\n",
    "        self.opensearch_vs.client.index(index=DEFAULT_METADATA_SCHEMA_COLLECTION, body=data)\n",
    "        logger.info(\n",
    "            f\"Metadata schema added to the Opensearch index {collection_name}. Metadata schema: {metadata_schema}\"\n",
    "        )\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def get_metadata_schema(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Get the metadata schema for a collection in the OpenSearch index.\n",
    "        This method retrieves the stored metadata schema definition for a specific\n",
    "        collection from the dedicated metadata schema index. Returns an empty list\n",
    "        if no schema is found.\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection to retrieve schema for.\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: List of metadata field definitions for the collection.\n",
    "                Each dictionary contains field specifications. Returns empty list if\n",
    "                no schema is found for the collection.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If collection_name is invalid.\n",
    "            IndexError: If the metadata schema index is corrupted or inaccessible.\n",
    "        Side Effects:\n",
    "            - Queries the metadata schema index\n",
    "            - Logs informational messages about schema retrieval status\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\")\n",
    "            >>> schema = vdb.get_metadata_schema(\"my_collection\")\n",
    "            >>> print(f\"Found {len(schema)} metadata fields\")\n",
    "            Found 3 metadata fields\n",
    "            >>> print(schema[0])\n",
    "            {\"name\": \"author\", \"type\": \"string\", \"description\": \"Document author\"}\n",
    "        \"\"\"\n",
    "        query = get_metadata_schema_query(collection_name)\n",
    "        response = self.opensearch_vs.client.search(\n",
    "            index=DEFAULT_METADATA_SCHEMA_COLLECTION, body=query\n",
    "        )\n",
    "        if len(response[\"hits\"][\"hits\"]) > 0:\n",
    "            return response[\"hits\"][\"hits\"][0][\"_source\"][\"metadata_schema\"]\n",
    "        else:\n",
    "            logging_message = (\n",
    "                f\"No metadata schema found for the collection: {collection_name}.\"\n",
    "                + \" Possible reason: The collection is not created with metadata schema.\"\n",
    "            )\n",
    "            logger.info(logging_message)\n",
    "            return []\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Methods for the VDBRag class for retrieval\n",
    "    def get_langchain_vectorstore(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "    ) -> VectorStore:\n",
    "        \"\"\"Get the vectorstore for a collection.\n",
    "        This method creates and returns a LangChain-compatible OpenSearchVectorSearch\n",
    "        instance configured for the specified collection. The vectorstore can be used\n",
    "        directly with LangChain retrievers and chains.\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection/index to create vectorstore for.\n",
    "        Returns:\n",
    "            VectorStore: A configured OpenSearchVectorSearch instance that implements\n",
    "                the LangChain VectorStore interface for semantic search operations.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If collection_name is invalid or doesn't exist.\n",
    "            ImportError: If required LangChain dependencies are not installed.\n",
    "        Side Effects:\n",
    "            - Creates a new vectorstore instance (lightweight operation)\n",
    "            - Validates connection to the specified OpenSearch index\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\",\n",
    "            ...                     embedding_model=embedding_model)\n",
    "            >>> vectorstore = vdb.get_langchain_vectorstore(\"my_collection\")\n",
    "            >>> retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "            >>> docs = retriever.get_relevant_documents(\"query text\")\n",
    "        \"\"\"\n",
    "        return OpenSearchVectorSearch(\n",
    "            opensearch_url=self.opensearch_url,\n",
    "            index_name=collection_name,\n",
    "            embedding_function=self.embedding_model,\n",
    "            use_ssl=False,\n",
    "        )\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def retrieval_langchain(\n",
    "        self,\n",
    "        query: str,\n",
    "        collection_name: str,\n",
    "        vectorstore: OpenSearchVectorSearch = None,\n",
    "        top_k: int = 10,\n",
    "        filter_expr: list[dict[str, Any]] = [],\n",
    "        otel_ctx: otel_context = None,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Perform semantic search and return top-k relevant documents.\n",
    "        This method executes semantic similarity search using LangChain's retrieval\n",
    "        framework. It supports filtering, custom vectorstore instances, and OpenTelemetry\n",
    "        tracing for observability.\n",
    "        Args:\n",
    "            query (str): The search query text to find similar documents for.\n",
    "            collection_name (str): The name of the collection to search in.\n",
    "            vectorstore (OpenSearchVectorSearch, optional): Pre-configured vectorstore\n",
    "                instance. If None, creates a new one for the collection.\n",
    "            top_k (int, optional): Maximum number of documents to return. Defaults to 10.\n",
    "            filter_expr (list[dict[str, Any]], optional): List of filter expressions to\n",
    "                apply during search. Each dict should contain field-value constraints.\n",
    "            otel_ctx (otel_context, optional): OpenTelemetry context for distributed tracing.\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: List of retrieved documents with similarity scores,\n",
    "                content, and metadata. Each document includes the collection_name in\n",
    "                its metadata for multi-collection support.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If query is empty or collection_name is invalid.\n",
    "            IndexError: If the search operation fails or returns malformed results.\n",
    "        Side Effects:\n",
    "            - Executes similarity search against the OpenSearch index\n",
    "            - Measures and logs retrieval latency for performance monitoring\n",
    "            - Adds collection_name to each document's metadata\n",
    "            - Attaches/detaches OpenTelemetry context for tracing\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\")\n",
    "            >>> filters = [{\"term\": {\"metadata.author\": \"john_doe\"}}]\n",
    "            >>> docs = vdb.retrieval_langchain(\n",
    "            ...     query=\"machine learning concepts\",\n",
    "            ...     collection_name=\"research_papers\",\n",
    "            ...     top_k=5,\n",
    "            ...     filter_expr=filters\n",
    "            ... )\n",
    "            >>> print(f\"Found {len(docs)} relevant documents\")\n",
    "        \"\"\"\n",
    "        if vectorstore is None:\n",
    "            vectorstore = self.get_langchain_vectorstore(collection_name)\n",
    "        if not filter_expr:\n",
    "            filter_expr = []\n",
    "        token = otel_context.attach(otel_ctx)\n",
    "        start_time = time.time()\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": top_k, \"fetch_k\": top_k}\n",
    "        )\n",
    "        retriever_lambda = RunnableLambda(\n",
    "            lambda x: retriever.invoke(x, filter=filter_expr)\n",
    "        )\n",
    "        retriever_chain = {\"context\": retriever_lambda} | RunnableAssign(\n",
    "            {\"context\": lambda input: input[\"context\"]}\n",
    "        )\n",
    "        retriever_docs = retriever_chain.invoke(query, config={\"run_name\": \"retriever\"})\n",
    "        docs = retriever_docs.get(\"context\", [])\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        logger.info(f\" OpenSearchVectorSearch Retriever latency: {latency:.4f} seconds\")\n",
    "        otel_context.detach(token)\n",
    "        return self._add_collection_name_to_retreived_docs(docs, collection_name)\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _add_collection_name_to_retreived_docs(\n",
    "        docs: list[Document], collection_name: str\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"Add the collection name to the retrieved documents.\n",
    "        This is done to ensure the collection name is available in the\n",
    "        metadata of the documents for preparing citations in case of multi-collection retrieval.\n",
    "        This static utility method enhances document metadata by adding the source\n",
    "        collection name to each document. This is essential for multi-collection\n",
    "        RAG scenarios where citations need to identify the source collection.\n",
    "        Args:\n",
    "            docs (list[Document]): List of LangChain Document objects retrieved from search.\n",
    "                Each document should have content and metadata attributes.\n",
    "            collection_name (str): The name of the collection these documents originated from.\n",
    "                This will be added to each document's metadata.\n",
    "        Returns:\n",
    "            list[Document]: The same list of documents with collection_name added to\n",
    "                each document's metadata under the \"collection_name\" key.\n",
    "        Raises:\n",
    "            AttributeError: If documents don't have the expected metadata attribute.\n",
    "            TypeError: If docs is not a list or contains non-Document objects.\n",
    "        Side Effects:\n",
    "            - Modifies the metadata of each input document in-place\n",
    "            - Adds \"collection_name\" field to document metadata\n",
    "        Note:\n",
    "            This is a static method that can be used independently of class instances.\n",
    "            It's designed to be called after retrieval operations to prepare documents\n",
    "            for citation generation in multi-collection scenarios.\n",
    "        Example:\n",
    "            >>> from langchain.schema import Document\n",
    "            >>> docs = [\n",
    "            ...     Document(page_content=\"Text 1\", metadata={\"source\": \"doc1.pdf\"}),\n",
    "            ...     Document(page_content=\"Text 2\", metadata={\"source\": \"doc2.pdf\"})\n",
    "            ... ]\n",
    "            >>> enhanced_docs = OpenSearchVDB._add_collection_name_to_retreived_docs(\n",
    "            ...     docs, \"research_collection\"\n",
    "            ... )\n",
    "            >>> print(enhanced_docs[0].metadata)\n",
    "            {\"source\": \"doc1.pdf\", \"collection_name\": \"research_collection\"}\n",
    "        \"\"\"\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"collection_name\"] = collection_name\n",
    "        return docs\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Catalog Metadata Management Methods\n",
    "    def get_catalog_metadata(self, collection_name: str) -> dict[str, Any]:\n",
    "        \"\"\"Get catalog metadata for a collection.\n",
    "        Args:\n",
    "            collection_name: Name of the collection\n",
    "        Returns:\n",
    "            Dictionary containing catalog metadata fields\n",
    "        \"\"\"\n",
    "        return self.get_document_info(\n",
    "            info_type=\"catalog\", collection_name=collection_name, document_name=\"NA\"\n",
    "        )\n",
    "    def update_catalog_metadata(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "        updates: dict[str, Any],\n",
    "    ) -> None:\n",
    "        \"\"\"Update catalog metadata for a collection.\n",
    "        Args:\n",
    "            collection_name: Name of the collection\n",
    "            updates: Dictionary of fields to update\n",
    "        \"\"\"\n",
    "        from datetime import datetime, UTC\n",
    "        existing = self.get_catalog_metadata(collection_name)\n",
    "        merged = {**existing, **updates}\n",
    "        merged[\"last_updated\"] = datetime.now(UTC).isoformat()\n",
    "        self.add_document_info(\n",
    "            info_type=\"catalog\",\n",
    "            collection_name=collection_name,\n",
    "            document_name=\"NA\",\n",
    "            info_value=merged,\n",
    "        )\n",
    "    def get_document_catalog_metadata(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "        document_name: str,\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"Get catalog metadata for a document.\n",
    "        Args:\n",
    "            collection_name: Name of the collection\n",
    "            document_name: Name of the document\n",
    "        Returns:\n",
    "            Dictionary containing document catalog metadata\n",
    "        \"\"\"\n",
    "        doc_info = self.get_document_info(\n",
    "            info_type=\"document\",\n",
    "            collection_name=collection_name,\n",
    "            document_name=document_name,\n",
    "        )\n",
    "        return {\n",
    "            \"description\": doc_info.get(\"description\", \"\"),\n",
    "            \"tags\": doc_info.get(\"tags\", []),\n",
    "        }\n",
    "    def update_document_catalog_metadata(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "        document_name: str,\n",
    "        updates: dict[str, Any],\n",
    "    ) -> None:\n",
    "        \"\"\"Update catalog metadata for a document.\n",
    "        Args:\n",
    "            collection_name: Name of the collection\n",
    "            document_name: Name of the document\n",
    "            updates: Dictionary of fields to update\n",
    "        \"\"\"\n",
    "        from datetime import datetime, UTC\n",
    "        existing = self.get_document_info(\n",
    "            info_type=\"document\",\n",
    "            collection_name=collection_name,\n",
    "            document_name=document_name,\n",
    "        )\n",
    "        for key in [\"description\", \"tags\"]:\n",
    "            if key in updates:\n",
    "                existing[key] = updates[key]\n",
    "        existing[\"last_updated\"] = datetime.now(UTC).isoformat()\n",
    "        self.add_document_info(\n",
    "            info_type=\"document\",\n",
    "            collection_name=collection_name,\n",
    "            document_name=document_name,\n",
    "            info_value=existing,\n",
    "        )\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Document Info Management Methods (similar to Milvus/Elasticsearch implementations)\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def create_document_info_collection(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the document info storage collection.\n",
    "        This method creates a dedicated OpenSearch index for storing document and collection\n",
    "        metadata information. The document info collection stores aggregated statistics,\n",
    "        document-level metadata, and collection-level metrics that are used by the RAG\n",
    "        system for tracking document properties and collection statistics.\n",
    "        Args:\n",
    "            None (uses instance attributes for configuration)\n",
    "        Returns:\n",
    "            None: This method performs index creation as a side effect.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to create indices.\n",
    "            ValueError: If the default document info collection name is invalid.\n",
    "        Side Effects:\n",
    "            - Creates the DEFAULT_DOCUMENT_INFO_COLLECTION index if it doesn't exist\n",
    "            - Applies the document info collection mapping for metadata storage\n",
    "            - Logs the creation status and configuration details\n",
    "        Note:\n",
    "            This is typically called once during system initialization. Subsequent calls\n",
    "            will detect the existing collection and log that it already exists.\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\")\n",
    "            >>> vdb.create_document_info_collection()\n",
    "            # Creates the document info storage index\n",
    "        \"\"\"\n",
    "        mapping = create_document_info_collection_mapping()\n",
    "        if not self.check_collection_exists(collection_name=DEFAULT_DOCUMENT_INFO_COLLECTION):\n",
    "            self.opensearch_vs.client.indices.create(\n",
    "                index=DEFAULT_DOCUMENT_INFO_COLLECTION, body=mapping\n",
    "            )\n",
    "            logging_message = (\n",
    "                f\"Collection {DEFAULT_DOCUMENT_INFO_COLLECTION} created \"\n",
    "                + f\"at {self.opensearch_url} with mapping {mapping}\"\n",
    "            )\n",
    "            logger.info(logging_message)\n",
    "        else:\n",
    "            logging_message = f\"Collection {DEFAULT_DOCUMENT_INFO_COLLECTION} already exists at {self.opensearch_url}\"\n",
    "            logger.info(logging_message)\n",
    "    def _get_aggregated_document_info(\n",
    "        self, collection_name: str, info_value: dict[str, Any]\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"Internal function to get the aggregated document info for a collection.\n",
    "        This private method retrieves existing collection-level document info and aggregates\n",
    "        it with new information. It is used internally by add_document_info when adding\n",
    "        collection-level metadata to ensure proper aggregation of statistics across documents.\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection to aggregate info for.\n",
    "            info_value (dict[str, Any]): New document info values to aggregate with existing info.\n",
    "        Returns:\n",
    "            dict[str, Any]: Aggregated document info dictionary containing combined statistics\n",
    "                and metadata from existing and new document info entries.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If collection_name is invalid or info_value is malformed.\n",
    "        Side Effects:\n",
    "            - Queries the document info collection for existing collection-level info\n",
    "            - Aggregates statistics using perform_document_info_aggregation utility\n",
    "        Note:\n",
    "            This is an internal method and should not be called directly by users.\n",
    "            It handles the aggregation logic for collection-level document info.\n",
    "        Example:\n",
    "            >>> existing_info = {\"total_elements\": 10, \"doc_type_counts\": {\"text\": 5}}\n",
    "            >>> new_info = {\"total_elements\": 5, \"doc_type_counts\": {\"text\": 3}}\n",
    "            >>> aggregated = vdb._get_aggregated_document_info(\"my_collection\", new_info)\n",
    "            # Returns aggregated info with total_elements: 15, doc_type_counts: {\"text\": 8}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.opensearch_vs.client.search(\n",
    "                index=DEFAULT_DOCUMENT_INFO_COLLECTION,\n",
    "                body=get_collection_document_info_query(\n",
    "                    info_type=\"collection\",\n",
    "                    collection_name=collection_name,\n",
    "                ),\n",
    "            )\n",
    "            existing_info_value = response[\"hits\"][\"hits\"][0][\"_source\"][\"info_value\"]\n",
    "        except IndexError:\n",
    "            existing_info_value = {}\n",
    "        except Exception as e:\n",
    "            logging_message = (\n",
    "                f\"Error getting aggregated document info for collection {collection_name}: {e}\"\n",
    "            )\n",
    "            logger.error(logging_message)\n",
    "            return info_value\n",
    "        \n",
    "        from nvidia_rag.utils.common import perform_document_info_aggregation\n",
    "        return perform_document_info_aggregation(existing_info_value, info_value)\n",
    "    def add_document_info(\n",
    "        self,\n",
    "        info_type: str,\n",
    "        collection_name: str,\n",
    "        document_name: str,\n",
    "        info_value: dict[str, Any],\n",
    "    ) -> None:\n",
    "        \"\"\"Add document info to a collection.\n",
    "        This method stores or updates document and collection metadata in the document info\n",
    "        collection. It handles both document-level info (for individual documents) and\n",
    "        collection-level info (aggregated statistics). For collection-level info, it\n",
    "        automatically aggregates with existing information to maintain accurate statistics.\n",
    "        Args:\n",
    "            info_type (str): The type of info being stored. Must be one of:\n",
    "                - \"document\": Document-level metadata for a specific document\n",
    "                - \"collection\": Collection-level aggregated statistics\n",
    "                - \"catalog\": Catalog-level metadata (description, tags, etc.)\n",
    "            collection_name (str): The name of the collection this info belongs to.\n",
    "            document_name (str): The name of the document this info belongs to.\n",
    "                Use \"NA\" for collection-level or catalog-level info.\n",
    "            info_value (dict[str, Any]): Dictionary containing the document info to store.\n",
    "                For document-level info, typically contains:\n",
    "                - total_elements (int): Number of elements in the document\n",
    "                - doc_type_counts (dict): Counts by element type (text, table, image, etc.)\n",
    "                - file_size (int): Size of the document file\n",
    "                - date_created (str): Document creation timestamp\n",
    "                For collection-level info, contains aggregated statistics across all documents.\n",
    "        Returns:\n",
    "            None: This method stores the document info as a side effect.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If parameters are invalid or info_value is malformed.\n",
    "            PermissionError: If insufficient permissions to modify the document info index.\n",
    "        Side Effects:\n",
    "            - Creates the document info collection if it doesn't exist\n",
    "            - Removes any existing document info entry for the same key\n",
    "            - Stores the new document info entry\n",
    "            - Refreshes the index to make the entry immediately searchable\n",
    "            - For collection-level info, aggregates with existing collection statistics\n",
    "            - Logs the document info addition for audit purposes\n",
    "        Example:\n",
    "            >>> doc_info = {\n",
    "            ...     \"total_elements\": 10,\n",
    "            ...     \"doc_type_counts\": {\"text\": 5, \"table\": 3, \"image\": 2},\n",
    "            ...     \"file_size\": 1024000,\n",
    "            ...     \"date_created\": \"2024-01-01T00:00:00Z\"\n",
    "            ... }\n",
    "            >>> vdb.add_document_info(\"document\", \"my_collection\", \"doc.pdf\", doc_info)\n",
    "            # Document info is now stored and can be retrieved with get_document_info\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.create_document_info_collection()\n",
    "            \n",
    "            # For collection-level info, aggregate with existing info\n",
    "            if info_type == \"collection\":\n",
    "                info_value = self._get_aggregated_document_info(\n",
    "                    collection_name=collection_name,\n",
    "                    info_value=info_value,\n",
    "                )\n",
    "            # Delete existing entry first to ensure idempotency\n",
    "            query = get_delete_document_info_query(\n",
    "                collection_name=collection_name,\n",
    "                document_name=document_name,\n",
    "                info_type=info_type,\n",
    "            )\n",
    "            \n",
    "            self.opensearch_vs.client.delete_by_query(\n",
    "                index=DEFAULT_DOCUMENT_INFO_COLLECTION,\n",
    "                body=query,\n",
    "                conflicts='proceed',  # OpenSearch-specific: ignore version conflicts\n",
    "            )\n",
    "            \n",
    "            # Add new entry\n",
    "            data = {\n",
    "                \"collection_name\": collection_name,\n",
    "                \"info_type\": info_type,\n",
    "                \"document_name\": document_name,\n",
    "                \"info_value\": info_value,\n",
    "            }\n",
    "            self.opensearch_vs.client.index(index=DEFAULT_DOCUMENT_INFO_COLLECTION, body=data)\n",
    "            \n",
    "            # Refresh to make it immediately searchable\n",
    "            self.opensearch_vs.client.indices.refresh(index=DEFAULT_DOCUMENT_INFO_COLLECTION)\n",
    "            \n",
    "            logging_message = (\n",
    "                f\"Document info added to the OpenSearch index {DEFAULT_DOCUMENT_INFO_COLLECTION}. \"\n",
    "                + f\"Document info: {info_type}, {document_name}\"\n",
    "            )\n",
    "            logger.info(logging_message)\n",
    "        except Exception as e:\n",
    "            logging_message = f\"Failed to add document info: {e}\"\n",
    "            logger.error(logging_message, exc_info=True)\n",
    "            raise\n",
    "    def get_document_info(\n",
    "        self,\n",
    "        info_type: str,\n",
    "        collection_name: str,\n",
    "        document_name: str,\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"Get document info from an OpenSearch index.\n",
    "        This method retrieves stored document or collection metadata from the document info\n",
    "        collection. It can retrieve document-level info (for specific documents), collection-level\n",
    "        info (aggregated statistics), or catalog-level info (collection metadata like description\n",
    "        and tags).\n",
    "        Args:\n",
    "            info_type (str): The type of info to retrieve. Must be one of:\n",
    "                - \"document\": Document-level metadata for a specific document\n",
    "                - \"collection\": Collection-level aggregated statistics\n",
    "                - \"catalog\": Catalog-level metadata (description, tags, etc.)\n",
    "            collection_name (str): The name of the collection to retrieve info for.\n",
    "            document_name (str): The name of the document to retrieve info for.\n",
    "                Use \"NA\" for collection-level or catalog-level info.\n",
    "        Returns:\n",
    "            dict[str, Any]: Dictionary containing the document info. Returns empty dictionary\n",
    "                if no matching document info is found. For document-level info, typically contains:\n",
    "                - total_elements (int): Number of elements in the document\n",
    "                - doc_type_counts (dict): Counts by element type\n",
    "                - file_size (int): Size of the document file\n",
    "                - date_created (str): Document creation timestamp\n",
    "                For collection-level info, contains aggregated statistics.\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If parameters are invalid.\n",
    "            IndexError: If the document info index is corrupted or inaccessible.\n",
    "        Side Effects:\n",
    "            - Queries the document info collection\n",
    "            - Logs informational messages about document info retrieval status\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\")\n",
    "            >>> doc_info = vdb.get_document_info(\"document\", \"my_collection\", \"doc.pdf\")\n",
    "            >>> print(f\"Document has {doc_info.get('total_elements')} elements\")\n",
    "            Document has 10 elements\n",
    "            >>> coll_info = vdb.get_document_info(\"collection\", \"my_collection\", \"NA\")\n",
    "            >>> print(f\"Collection has {coll_info.get('total_elements')} total elements\")\n",
    "            Collection has 50 total elements\n",
    "        \"\"\"\n",
    "        query = get_document_info_query(collection_name, document_name, info_type)\n",
    "        try:\n",
    "            response = self.opensearch_vs.client.search(\n",
    "                index=DEFAULT_DOCUMENT_INFO_COLLECTION, body=query\n",
    "            )\n",
    "            if len(response[\"hits\"][\"hits\"]) > 0:\n",
    "                return response[\"hits\"][\"hits\"][0][\"_source\"][\"info_value\"]\n",
    "            else:\n",
    "                logging_message = (\n",
    "                    f\"No document info found for collection: {collection_name}, \"\n",
    "                    + f\"document: {document_name}, info type: {info_type}\"\n",
    "                )\n",
    "                logger.info(logging_message)\n",
    "                return {}\n",
    "        except Exception as e:\n",
    "            logging_message = f\"Error retrieving document info: {e}\"\n",
    "            logger.warning(logging_message)\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise the Opensearch VDBRag operator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model Setup:\n",
    "# Here we configure the embedding model, the neural engine that transforms your text into high-dimensional vectors.\n",
    "# Tip: Choose a model that fits your domain and scale for best results\n",
    "from nvidia_rag.utils.configuration import NvidiaRAGConfig\n",
    "from nvidia_rag.utils.embedding import get_embedding_model\n",
    "# Load configuration from yaml file\n",
    "config = NvidiaRAGConfig.from_yaml(\"config.yaml\")\n",
    "# Create embedding model using the configuration\n",
    "embedding_model = get_embedding_model(\n",
    "    model=config.embeddings.model_name,\n",
    "    url=config.embeddings.server_url,\n",
    "    config=config,\n",
    "    # url=\"localhost:9080\" # TODO: Uncomment while using on-prem embeddings, and comment the above line\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Vector Database (VDB) Operator for OpenSearch\n",
    "# This operator acts as the bridge between your RAG pipeline and the underlying OpenSearch vector store.\n",
    "# Configure it here to enable semantic search, hybrid retrieval, and seamless document management.\n",
    "# Tip: Adjust the parameters below to match your OpenSearch deployment and embedding model\n",
    "opensearch_vdb_op = OpenSearchVDB(\n",
    "    opensearch_url=\"http://localhost:9200\",\n",
    "    index_name=\"test_library\",\n",
    "    embedding_model=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's set the required logging level. Set to INFO for displaying basic important logs. Set to DEBUG for full verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "# Set the log level via environment variable before importing nvidia_rag\n",
    "# This ensures the package respects our log level setting\n",
    "LOGLEVEL = logging.WARNING  # Set to INFO, DEBUG, WARNING or ERROR\n",
    "os.environ[\"LOGLEVEL\"] = logging.getLevelName(LOGLEVEL)\n",
    "# Configure logging\n",
    "logging.basicConfig(level=LOGLEVEL, force=True)\n",
    "# Set log levels for specific loggers after package import\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    if name == \"nvidia_rag\" or name.startswith(\"nvidia_rag.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)\n",
    "    if name == \"nv_ingest_client\" or name.startswith(\"nv_ingest_client.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Register the Opensearch VDBRag operator with `NvidiaRAG()` and `NvidiaRAGIngestor()` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag import NvidiaRAG, NvidiaRAGIngestor\n",
    "# Create ingestor with config and custom VDB operator\n",
    "ingestor = NvidiaRAGIngestor(\n",
    "    config=config,\n",
    "    vdb_op=opensearch_vdb_op\n",
    ")\n",
    "# Create RAG with config and custom VDB operator\n",
    "# You can optionally pass custom prompts via:\n",
    "#   - A path to a YAML/JSON file: prompts=\"custom_prompts.yaml\"\n",
    "#   - A dictionary: prompts={\"rag_template\": {\"system\": \"...\", \"human\": \"...\"}}\n",
    "rag = NvidiaRAG(\n",
    "    config=config,\n",
    "    vdb_op=opensearch_vdb_op\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Utilizing Nvidia RAG Library APIs with a Opensearch Vector Database Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1. Create a new collection**\n",
    "Creates a new collection in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.create_collection()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. List all collections**\n",
    "Retrieves all available collections from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.get_collections()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. Add a document**\n",
    "Uploads new documents to the specified collection in the vector database. In case you have a requirement of updating existing documents in the specified collection, you can call `update_documents()` instead of `upload_documents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.upload_documents(\n",
    "    blocking=False,\n",
    "    split_options={\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
    "    filepaths=[\n",
    "        \"../data/multimodal/woods_frost.docx\",\n",
    "        \"../data/multimodal/multimodal_test.pdf\",\n",
    "    ],\n",
    "    generate_summary=False,\n",
    ")\n",
    "task_id = response.get(\"task_id\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4. Check document upload status**\n",
    "Checks the status of a document upload/update task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.status(task_id=task_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5. [Optional] Update a document in a collection**\n",
    "In case you have a requirement of updating an existing document in the specified collection, execute below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.update_documents(\n",
    "    blocking=False,\n",
    "    filepaths=[\"../data/multimodal/woods_frost.docx\"],\n",
    "    generate_summary=False,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.6. Get documents in a collection**\n",
    "Retrieves the list of documents uploaded to a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.get_documents()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.7. Query a document using RAG**\n",
    "Sends a chat-style query to the RAG system using the specified models and endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check health of all dependent services for rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "health_status_with_deps = await rag.health()\n",
    "print(health_status_with_deps.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from IPython.display import Image, Markdown, display\n",
    "async def print_streaming_response_and_citations(rag_response):\n",
    "    \"\"\"\n",
    "    Print the streaming response and citations from the RAG response.\n",
    "    \"\"\"\n",
    "    # Check for API errors before processing\n",
    "    if rag_response.status_code != 200:\n",
    "        print(\"Error: \", rag_response.status_code)\n",
    "        return\n",
    "    # Extract the streaming generator from the response\n",
    "    response_generator = rag_response.generator\n",
    "    first_chunk_data = None\n",
    "    async for chunk in response_generator:\n",
    "        if chunk.startswith(\"data: \"):\n",
    "            chunk = chunk[len(\"data: \") :].strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            continue\n",
    "        choices = data.get(\"choices\", [])\n",
    "        if not choices:\n",
    "            continue\n",
    "        # Save the first chunk with citations\n",
    "        if first_chunk_data is None and data.get(\"citations\"):\n",
    "            first_chunk_data = data\n",
    "        # Print streaming text\n",
    "        delta = choices[0].get(\"delta\", {})\n",
    "        text = delta.get(\"content\")\n",
    "        if not text:\n",
    "            message = choices[0].get(\"message\", {})\n",
    "            text = message.get(\"content\", \"\")\n",
    "        print(text, end=\"\", flush=True)\n",
    "    print()  # Newline after streaming\n",
    "    # Display citations after streaming is done\n",
    "    if first_chunk_data and first_chunk_data.get(\"citations\"):\n",
    "        citations = first_chunk_data[\"citations\"]\n",
    "        for idx, citation in enumerate(citations.get(\"results\", [])):\n",
    "            doc_type = citation.get(\"document_type\", \"text\")\n",
    "            content = citation.get(\"content\", \"\")\n",
    "            doc_name = citation.get(\"document_name\", f\"Citation {idx + 1}\")\n",
    "            display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "            try:\n",
    "                image_bytes = base64.b64decode(content)\n",
    "                display(Image(data=image_bytes))\n",
    "            except Exception:\n",
    "                display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the generate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "await print_streaming_response_and_citations(\n",
    "    await rag.generate(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the price of a hammer?\"}],\n",
    "        use_knowledge_base=True,\n",
    "        collection_names=[\"test_library\"], # Kindly provide collection_names argument\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.8. [Optional] Search for documents**\n",
    "Performs a search in the vector database for relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_citations(citations):\n",
    "    \"\"\"\n",
    "    Display all citations from the Citations object returned by search().\n",
    "    Handles base64-encoded images and text.\n",
    "    \"\"\"\n",
    "    if not citations or not hasattr(citations, \"results\") or not citations.results:\n",
    "        print(\"No citations found.\")\n",
    "        return\n",
    "    for idx, citation in enumerate(citations.results):\n",
    "        # If using pydantic models, citation fields may be attributes, not dict keys\n",
    "        doc_type = getattr(citation, \"document_type\", \"text\")\n",
    "        content = getattr(citation, \"content\", \"\")\n",
    "        doc_name = getattr(citation, \"document_name\", f\"Citation {idx + 1}\")\n",
    "        display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "        try:\n",
    "            image_bytes = base64.b64decode(content)\n",
    "            display(Image(data=image_bytes))\n",
    "        except Exception:\n",
    "            display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_search_citations(\n",
    "    await rag.search(\n",
    "        query=\"What is the price of a hammer?\",\n",
    "        collection_names=[\"test_library\"], # Kindly provide collection_names argument\n",
    "        reranker_top_k=10,\n",
    "        vdb_top_k=100,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.9. [Optional] Retrieve documents summary**\n",
    "You can execute this cell if summary generation was enabled during document upload using `generate_summary: bool` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await rag.get_summary(\n",
    "    collection_name=\"test_library\", # Kindly provide collection_names argument\n",
    "    file_name=\"woods_frost.docx\",\n",
    "    blocking=False,\n",
    "    timeout=20,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Below APIs illustrate how to cleanup uploaded documents and collections once no more interaction is needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.10. Delete documents from a collection**\n",
    "Deletes documents from the specified collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.delete_documents(\n",
    "    document_names=[\"../data/multimodal/multimodal_test.pdf\"],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.11. Delete collections**\n",
    "Deletes the specified collection and all its documents from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.delete_collections(\n",
    "    collection_names=[\"test_library\"]\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
