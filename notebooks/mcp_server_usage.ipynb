{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MCP Server Usage (SSE, streamable-http, and stdio)\n",
        "\n",
        "This notebook demonstrates how to use the NVIDIA RAG MCP server via MCP transports (SSE, streamable-http, and stdio) instead of REST APIs. It covers:\n",
        "- Launching the server (SSE and streamable-http); stdio is spawned by the client\n",
        "- Connecting with the MCP Python client\n",
        "- Listing tools (Retriever: `generate`, `search`, `get_summary`; Ingestor: `create_collections`, `list_collections`, `upload_documents`, `get_documents`, `update_documents`, `delete_documents`, `update_collection_metadata`, `update_document_metadata`, `delete_collections`)\n",
        "- Calling Ingestor tools: `create_collections`, `upload_documents`, `delete_collections`\n",
        "- Calling Retriever tools: `generate`, `search`, `get_summary`\n",
        "\n",
        "Execute cells in sequence for each transport to validate end‑to‑end behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Install Dependencies\n",
        "\n",
        "Purpose:\n",
        "Install the libraries needed to run the MCP server and client locally in this notebook environment.\n",
        "\n",
        "- Ensure your environment has:\n",
        "  - `mcp`, `anyio`, `httpx`, `httpx-sse`, `uvicorn`\n",
        "- If using Workbench/docker, these may already be installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -qq -r ../nvidia_rag_mcp/requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prerequisites\n",
        "\n",
        "- Ensure the RAG server is running and reachable before using MCP tools. \n",
        "- Ensure the Ingestor server is running and reachable before using MCP tools.\n",
        "- Follow the [quickstart guide](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/deploy-docker-self-hosted.md) to start the RAG server and Ingestor server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch MCP Server (SSE)\n",
        "\n",
        "Purpose:\n",
        "Start the MCP server locally over SSE so that the client can connect and call tools.\n",
        "\n",
        "This launches the MCP server on `http://127.0.0.1:8000`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "PORT = 8000\n",
        "print(f\"Kill any process running on port {PORT} to start sse server in the next cell\")\n",
        "\n",
        "try:\n",
        "    subprocess.run([\"fuser\", \"-k\", f\"{PORT}/tcp\"], check=False)\n",
        "except FileNotFoundError:\n",
        "    print(\"'fuser' not found, skipping fuser-based cleanup.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while running fuser: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import atexit\n",
        "import time\n",
        "\n",
        "sse_proc = None\n",
        "try:\n",
        "\n",
        "    repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "    server_path = os.path.join(repo_root, \"nvidia_rag_mcp\", \"mcp_server.py\")\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        server_path,\n",
        "        \"--transport\",\n",
        "        \"sse\",\n",
        "        \"--host\",\n",
        "        \"127.0.0.1\",\n",
        "        \"--port\",\n",
        "        \"8000\",\n",
        "    ]\n",
        "\n",
        "    print(\"Launching:\", \" \".join(cmd))\n",
        "    sse_proc = subprocess.Popen(cmd)\n",
        "    atexit.register(lambda: sse_proc and sse_proc.poll() is None and sse_proc.terminate())\n",
        "    time.sleep(2.0)\n",
        "    print(\"SSE server PID:\", sse_proc.pid)\n",
        "except Exception as e:\n",
        "    print(\"Failed to start SSE server:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect with MCP Client (SSE), List Tools, and Call MCP Tools\n",
        "\n",
        "Purpose:\n",
        "Verify connectivity by listing available tools and invoking the retriever tools (`generate`, `search`, `get_summary`) and ingestor tools (`create_collections`, `upload_documents`, `delete_collections`) using the SSE transport.\n",
        "\n",
        "This uses the `mcp_client.py` CLI to connect over SSE, list tools, and invoke both Retriever and Ingestor tools.\n",
        "\n",
        "**Note:** Ensure the SSE server is running from Cell 6 before executing this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "SSE_URL = \"http://127.0.0.1:8000/sse\"\n",
        "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "client_path = os.path.join(repo_root, \"nvidia_rag_mcp\", \"mcp_client.py\")\n",
        "COLLECTION = \"my_collection\"\n",
        "pdf_path = os.path.join(repo_root, \"data\", \"multimodal\", \"woods_frost.pdf\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Listing available tools...\")\n",
        "print(\"=\"*80)\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"list\",\n",
        "    \"--transport=sse\",\n",
        "    f\"--url={SSE_URL}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Creating collection...\")\n",
        "print(\"=\"*80)\n",
        "create_args = json.dumps({\"collection_names\": [COLLECTION]})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    \"--transport=sse\", f\"--url={SSE_URL}\",\n",
        "    \"--tool=create_collections\",\n",
        "    f\"--json-args={create_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Uploading document...\")\n",
        "print(\"=\"*80)\n",
        "upload_args = json.dumps({\n",
        "    \"collection_name\": COLLECTION,\n",
        "    \"file_paths\": [pdf_path],\n",
        "    \"blocking\": True,\n",
        "    \"generate_summary\": True,\n",
        "    \"split_options\": {\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    \"--transport=sse\", f\"--url={SSE_URL}\",\n",
        "    \"--tool=upload_documents\",\n",
        "    f\"--json-args={upload_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'generate' tool...\")\n",
        "print(\"=\"*80)\n",
        "generate_args = json.dumps({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello from SSE demo\"}],\n",
        "    \"collection_names\": [COLLECTION],\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=sse\",\n",
        "    f\"--url={SSE_URL}\",\n",
        "    \"--tool=generate\",\n",
        "    f\"--json-args={generate_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'search' tool...\")\n",
        "print(\"=\"*80)\n",
        "search_args = json.dumps({\n",
        "    \"query\": \"Tell me about Robert Frost's poems\",\n",
        "    \"collection_names\": [COLLECTION],\n",
        "    \"reranker_top_k\": 2,\n",
        "    \"vdb_top_k\": 5,\n",
        "    \"enable_query_rewriting\": False,\n",
        "    \"enable_reranker\": True,\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=sse\",\n",
        "    f\"--url={SSE_URL}\",\n",
        "    \"--tool=search\",\n",
        "    f\"--json-args={search_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'get_summary' tool...\")\n",
        "print(\"=\"*80)\n",
        "summary_args = json.dumps({\n",
        "    \"collection_name\": COLLECTION,\n",
        "    \"file_name\": \"woods_frost.pdf\",\n",
        "    \"blocking\": False,\n",
        "    \"timeout\": 60,\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=sse\",\n",
        "    f\"--url={SSE_URL}\",\n",
        "    \"--tool=get_summary\",\n",
        "    f\"--json-args={summary_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Deleting collection...\")\n",
        "print(\"=\"*80)\n",
        "delete_args = json.dumps({\"collection_names\": [COLLECTION]})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    \"--transport=sse\", f\"--url={SSE_URL}\",\n",
        "    \"--tool=delete_collections\",\n",
        "    f\"--json-args={delete_args}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch MCP Server (streamable_http)\n",
        "\n",
        "Purpose:\n",
        "Start the MCP server locally using the streamable_http transport so that the client can connect and call tools.\n",
        "\n",
        "This launches the MCP server on `http://127.0.0.1:8000`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PORT = 8000\n",
        "print(f\"Kill any process running on port {PORT} to start streamable_http server in the next cell\")\n",
        "\n",
        "try:\n",
        "    subprocess.run([\"fuser\", \"-k\", f\"{PORT}/tcp\"], check=False)\n",
        "except FileNotFoundError:\n",
        "    print(\"'fuser' not found, skipping fuser-based cleanup.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while running fuser: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import atexit\n",
        "import time\n",
        "\n",
        "stream_proc = None\n",
        "try:\n",
        "\n",
        "    repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "    server_path = os.path.join(repo_root, \"nvidia_rag_mcp\", \"mcp_server.py\")\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        server_path,\n",
        "        \"--transport\",\n",
        "        \"streamable_http\",\n",
        "        \"--host\",\n",
        "        \"127.0.0.1\",\n",
        "        \"--port\",\n",
        "        \"8000\",\n",
        "    ]\n",
        "\n",
        "    print(\"Launching streamable_http server:\", \" \".join(cmd))\n",
        "    stream_proc = subprocess.Popen(cmd)\n",
        "    atexit.register(lambda: stream_proc and stream_proc.poll() is None and stream_proc.terminate())\n",
        "    time.sleep(2.0)\n",
        "    print(\"streamable_http server PID:\", stream_proc.pid)\n",
        "except Exception as e:\n",
        "    print(\"Failed to start streamable_http server:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect with MCP Client (streamable_http), List Tools, and Call MCP Tools\n",
        "\n",
        "Purpose:\n",
        "Verify connectivity by listing available tools and invoking the retriever tools (`generate`, `search`, `get_summary`) and ingestor tools (`create_collections`, `upload_documents`, `delete_collections`) using the streamable_http transport.\n",
        "\n",
        "This uses the `mcp_client.py` CLI to connect over streamable_http, list tools, and invoke both Retriever and Ingestor tools.\n",
        "\n",
        "**Note:** Ensure the streamable_http server is running from the cell above before executing this cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "STREAMABLE_HTTP_URL = \"http://127.0.0.1:8000/mcp\"\n",
        "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "client_path = os.path.join(repo_root, \"nvidia_rag_mcp\", \"mcp_client.py\")\n",
        "COLLECTION = \"my_collection\"\n",
        "pdf_path = os.path.join(repo_root, \"data\", \"multimodal\", \"woods_frost.pdf\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Listing available tools...\")\n",
        "print(\"=\"*80)\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"list\",\n",
        "    \"--transport=streamable_http\",\n",
        "    f\"--url={STREAMABLE_HTTP_URL}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Creating collection...\")\n",
        "print(\"=\"*80)\n",
        "create_args = json.dumps({\"collection_names\": [COLLECTION]})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    \"--transport=streamable_http\", f\"--url={STREAMABLE_HTTP_URL}\",\n",
        "    \"--tool=create_collections\",\n",
        "    f\"--json-args={create_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Uploading document...\")\n",
        "print(\"=\"*80)\n",
        "upload_args = json.dumps({\n",
        "    \"collection_name\": COLLECTION,\n",
        "    \"file_paths\": [pdf_path],\n",
        "    \"blocking\": True,\n",
        "    \"generate_summary\": True,\n",
        "    \"split_options\": {\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    \"--transport=streamable_http\", f\"--url={STREAMABLE_HTTP_URL}\",\n",
        "    \"--tool=upload_documents\",\n",
        "    f\"--json-args={upload_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'generate' tool...\")\n",
        "print(\"=\"*80)\n",
        "generate_args = json.dumps({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello from SSE demo\"}],\n",
        "    \"collection_names\": [COLLECTION],\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=streamable_http\",\n",
        "    f\"--url={STREAMABLE_HTTP_URL}\",\n",
        "    \"--tool=generate\",\n",
        "    f\"--json-args={generate_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'search' tool...\")\n",
        "print(\"=\"*80)\n",
        "search_args = json.dumps({\n",
        "    \"query\": \"Tell me about Robert Frost's poems\",\n",
        "    \"collection_names\": [COLLECTION],\n",
        "    \"reranker_top_k\": 2,\n",
        "    \"vdb_top_k\": 5,\n",
        "    \"enable_query_rewriting\": False,\n",
        "    \"enable_reranker\": True,\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=streamable_http\",\n",
        "    f\"--url={STREAMABLE_HTTP_URL}\",\n",
        "    \"--tool=search\",\n",
        "    f\"--json-args={search_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'get_summary' tool...\")\n",
        "print(\"=\"*80)\n",
        "summary_args = json.dumps({\n",
        "    \"collection_name\": COLLECTION,\n",
        "    \"file_name\": \"woods_frost.pdf\",\n",
        "    \"blocking\": False,\n",
        "    \"timeout\": 60,\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=streamable_http\",\n",
        "    f\"--url={STREAMABLE_HTTP_URL}\",\n",
        "    \"--tool=get_summary\",\n",
        "    f\"--json-args={summary_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Deleting collection...\")\n",
        "print(\"=\"*80)\n",
        "delete_args = json.dumps({\"collection_names\": [COLLECTION]})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    \"--transport=streamable_http\", f\"--url={STREAMABLE_HTTP_URL}\",\n",
        "    \"--tool=delete_collections\",\n",
        "    f\"--json-args={delete_args}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect with MCP Client (stdio), List Tools, and Call MCP Tools\n",
        "\n",
        "Purpose:\n",
        "Verify connectivity by listing available tools and invoking the retriever tools (`generate`, `search`, `get_summary`) and ingestor tools (`create_collections`, `upload_documents`, `delete_collections`) using the stdio transport. The client will spawn the server in stdio mode via `--command` and `--args`.\n",
        "\n",
        "Note: No separate server launch is needed for stdio; the client manages the server subprocess.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "client_path = os.path.join(repo_root, \"nvidia_rag_mcp\", \"mcp_client.py\")\n",
        "server_path = os.path.join(repo_root, \"nvidia_rag_mcp\", \"mcp_server.py\")\n",
        "\n",
        "STDIO_CMD = sys.executable\n",
        "STDIO_ARGS = f\"{server_path} --transport stdio\"\n",
        "COLLECTION = \"my_collection\"\n",
        "pdf_path = os.path.join(repo_root, \"data\", \"multimodal\", \"woods_frost.pdf\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Listing available tools (stdio)...\")\n",
        "print(\"=\"*80)\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"list\",\n",
        "    \"--transport=stdio\",\n",
        "    f\"--command={STDIO_CMD}\",\n",
        "    f\"--args={STDIO_ARGS}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Creating collection (stdio)...\")\n",
        "print(\"=\"*80)\n",
        "create_args = json.dumps({\"collection_names\": [COLLECTION]})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    \"--transport=stdio\", f\"--command={STDIO_CMD}\", f\"--args={STDIO_ARGS}\",\n",
        "    \"--tool=create_collections\",\n",
        "    f\"--json-args={create_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Uploading document (stdio)...\")\n",
        "print(\"=\"*80)\n",
        "upload_args = json.dumps({\n",
        "    \"collection_name\": COLLECTION,\n",
        "    \"file_paths\": [pdf_path],\n",
        "    \"blocking\": True,\n",
        "    \"generate_summary\": True,\n",
        "    \"split_options\": {\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    \"--transport=stdio\", f\"--command={STDIO_CMD}\", f\"--args={STDIO_ARGS}\",\n",
        "    \"--tool=upload_documents\",\n",
        "    f\"--json-args={upload_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'generate' tool (stdio)...\")\n",
        "print(\"=\"*80)\n",
        "generate_args = json.dumps({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Say 'ok'\"}],\n",
        "    \"collection_names\": [COLLECTION],\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=stdio\",\n",
        "    f\"--command={STDIO_CMD}\",\n",
        "    f\"--args={STDIO_ARGS}\",\n",
        "    \"--tool=generate\",\n",
        "    f\"--json-args={generate_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'search' tool (stdio)...\")\n",
        "print(\"=\"*80)\n",
        "search_args = json.dumps({\n",
        "    \"query\": \"Tell me about Robert Frost's poems\",\n",
        "    \"collection_names\": [COLLECTION],\n",
        "    \"reranker_top_k\": 2,\n",
        "    \"vdb_top_k\": 5,\n",
        "    \"enable_query_rewriting\": False,\n",
        "    \"enable_reranker\": True,\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=stdio\",\n",
        "    f\"--command={STDIO_CMD}\",\n",
        "    f\"--args={STDIO_ARGS}\",\n",
        "    \"--tool=search\",\n",
        "    f\"--json-args={search_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'get_summary' tool (stdio)...\")\n",
        "print(\"=\"*80)\n",
        "summary_args = json.dumps({\n",
        "    \"collection_name\": COLLECTION,\n",
        "    \"file_name\": \"woods_frost.pdf\",\n",
        "    \"blocking\": False,\n",
        "    \"timeout\": 60,\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    \"--transport=stdio\",\n",
        "    f\"--command={STDIO_CMD}\",\n",
        "    f\"--args={STDIO_ARGS}\",\n",
        "    \"--tool=get_summary\",\n",
        "    f\"--json-args={summary_args}\",\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Deleting collection (stdio)...\")\n",
        "print(\"=\"*80)\n",
        "delete_args = json.dumps({\"collection_names\": [COLLECTION]})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    \"--transport=stdio\", f\"--command={STDIO_CMD}\", f\"--args={STDIO_ARGS}\",\n",
        "    \"--tool=delete_collections\",\n",
        "    f\"--json-args={delete_args}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup & Troubleshooting\n",
        "\n",
        "Purpose:\n",
        "Wrap up the session, stop background processes, and provide guidance for common issues across SSE, streamable_http, and stdio.\n",
        "\n",
        "- Stopping servers:\n",
        "  - SSE / streamable_http: restart the kernel or terminate the subprocesses started in earlier cells.\n",
        "  - stdio: no separate server process is kept running; the client starts and stops it per command.\n",
        "- Readiness checks:\n",
        "  - SSE: 200-range on `http://127.0.0.1:8000/sse` indicates ready.\n",
        "  - streamable_http: GET on `/mcp` may return 406; treat 200-range OR 406 as ready.\n",
        "- Ingestor + RAG endpoints:\n",
        "  - Ensure RAG and Ingestor servers are reachable.\n",
        "- Port conflicts:\n",
        "  - Free port 8000 if needed (e.g., `fuser -k 8000/tcp` on Linux) before starting SSE/streamable_http again.\n",
        "- stdio usage tips:\n",
        "  - Use `--command` and `--args` to spawn the server, e.g. `--command=python --args=\"-m nvidia_rag_mcp.mcp_server --transport stdio\"`.\n",
        "- Dependencies:\n",
        "  - Ensure recent versions of `mcp`, `anyio`, and `uvicorn` are available in your environment."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
