{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MCP Server Usage\n",
        "\n",
        "This notebook demonstrates how to use the NVIDIA RAG MCP server via MCP transports instead of REST APIs. It covers:\n",
        "\n",
        "- Installing dependencies\n",
        "- Launching the MCP server using `streamable_http` transport\n",
        "- Connecting with the MCP Python client\n",
        "- Listing available tools\n",
        "- Calling Ingestor tools: `create_collection`, `upload_documents`, `delete_collections`\n",
        "- Calling RAG tools: `generate`, `search`, `get_summary`\n",
        "\n",
        "**Available MCP Tools:**\n",
        "- **RAG tools:** `generate`, `search`, `get_summary`\n",
        "- **Ingestor tools:** `create_collection`, `list_collections`, `upload_documents`, `get_documents`, `update_documents`, `delete_documents`, `update_collection_metadata`, `update_document_metadata`, `delete_collections`\n",
        "\n",
        "Execute cells in sequence to validate end-to-end behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisites\n",
        "\n",
        "**Before running this notebook, ensure you have:**\n",
        "\n",
        "1. **Python 3.11 or higher** installed with `pip`\n",
        "   ```bash\n",
        "   python --version  # Should be 3.11+\n",
        "   ```\n",
        "\n",
        "2. **NVIDIA RAG services running** - The end-to-end RAG workflow must be up and running\n",
        "   - Follow the [quickstart guide](../docs/deploy-docker-nvidia-hosted.md) to start the RAG and Ingestor services\n",
        "   - Verify that RAG (port 8081) and Ingestor (port 8082) services are accessible\n",
        "\n",
        "3. **Jupyter environment** - JupyterLab or Jupyter Notebook to run this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Install Dependencies\n",
        "\n",
        "Install the libraries needed to run the MCP server and client:\n",
        "- `mcp` - MCP SDK for client/server communication\n",
        "- `anyio` - Async I/O framework\n",
        "- `httpx`, `httpx-sse` - HTTP client with SSE support\n",
        "- `uvicorn` - ASGI server for running MCP server\n",
        "- `fastmcp` - FastMCP framework for building MCP servers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qq -r ../examples/nvidia_rag_mcp/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Configuration\n",
        "\n",
        "Set up configuration variables for the MCP server and client.\n",
        "\n",
        "**Environment variables:** Setup following environment variables in the next cell for the INGESTOR server and RAG server urls before starting the MCP server.\n",
        "- INGESTOR_SERVER_URL=\"http://localhost:8082\"\n",
        "- VITE_API_CHAT_URL=\"http://localhost:8081\"\n",
        "\n",
        "**Transport Options:**\n",
        "- `streamable_http` (default) - HTTP-based streaming, recommended for most use cases\n",
        "- `sse` - Server-Sent Events over HTTP\n",
        "- `stdio` - Standard input/output, ideal for local development\n",
        "\n",
        "**To switch transports:** Simply change the `TRANSPORT` variable below. The `MCP_URL` will be automatically set based on your transport choice:\n",
        "- `streamable_http` → `http://127.0.0.1:8000/mcp`\n",
        "- `sse` → `http://127.0.0.1:8000/sse`\n",
        "- `stdio` → No URL needed\n",
        "\n",
        "See the end of this notebook for detailed instructions on using other transports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Transport configuration\n",
        "TRANSPORT = \"streamable_http\"  # Options: \"streamable_http\", \"sse\", \"stdio\"\n",
        "PORT = 8000\n",
        "HOST = \"127.0.0.1\"\n",
        "\n",
        "# Export the INGESTOR server and RAG server urls before starting the MCP server\n",
        "os.environ[\"INGESTOR_SERVER_URL\"] = \"http://localhost:8082\" # Ingestor server url\n",
        "os.environ[\"VITE_API_CHAT_URL\"] = \"http://localhost:8081\" # Rag server url\n",
        "\n",
        "# Automatically set URL path based on transport\n",
        "if TRANSPORT == \"streamable_http\":\n",
        "    URL_PATH = \"/mcp\"\n",
        "elif TRANSPORT == \"sse\":\n",
        "    URL_PATH = \"/sse\"\n",
        "else:\n",
        "    URL_PATH = \"\"  # stdio doesn't use URL\n",
        "\n",
        "MCP_URL = f\"http://{HOST}:{PORT}{URL_PATH}\" if URL_PATH else None\n",
        "\n",
        "# Paths\n",
        "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "server_path = os.path.join(repo_root, \"examples\", \"nvidia_rag_mcp\", \"mcp_server.py\")\n",
        "client_path = os.path.join(repo_root, \"examples\", \"nvidia_rag_mcp\", \"mcp_client.py\")\n",
        "pdf_path = os.path.join(repo_root, \"data\", \"multimodal\", \"woods_frost.pdf\")\n",
        "\n",
        "# Collection name for this demo\n",
        "COLLECTION = \"my_collection\"\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Transport: {TRANSPORT}\")\n",
        "print(f\"  Server: {HOST}:{PORT}\")\n",
        "if MCP_URL:\n",
        "    print(f\"  MCP URL: {MCP_URL}\")\n",
        "print(f\"  Collection: {COLLECTION}\")\n",
        "print(f\"  Sample PDF: {os.path.basename(pdf_path)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Launch MCP Server\n",
        "\n",
        "Start the MCP server using the configured transport. The server will:\n",
        "- Expose NVIDIA RAG and Ingestor APIs as MCP tools\n",
        "- Listen on the configured host and port\n",
        "- Forward requests to RAG (port 8081) and Ingestor (port 8082) services\n",
        "\n",
        "The server runs in the background and will be automatically terminated when the kernel restarts.\n",
        "\n",
        "**Note:** Skip this section if using `stdio` transport (see end of notebook for instructions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "print(f\"Freeing port {PORT}...\")\n",
        "try:\n",
        "    subprocess.run([\"fuser\", \"-k\", f\"{PORT}/tcp\"], check=False)\n",
        "except FileNotFoundError:\n",
        "    print(\"'fuser' not found, skipping port cleanup.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while freeing port: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import atexit\n",
        "import time\n",
        "\n",
        "mcp_server_proc = None\n",
        "try:\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        server_path,\n",
        "        \"--transport\",\n",
        "        TRANSPORT,\n",
        "        \"--host\",\n",
        "        HOST,\n",
        "        \"--port\",\n",
        "        str(PORT),\n",
        "    ]\n",
        "\n",
        "    print(f\"Launching MCP server: {' '.join(cmd)}\")\n",
        "    mcp_server_proc = subprocess.Popen(cmd)\n",
        "    atexit.register(lambda: mcp_server_proc and mcp_server_proc.poll() is None and mcp_server_proc.terminate())\n",
        "    time.sleep(2.0)\n",
        "    print(f\"MCP server started (PID: {mcp_server_proc.pid})\")\n",
        "    if MCP_URL:\n",
        "        print(f\"Server URL: {MCP_URL}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to start MCP server: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. List Available Tools\n",
        "\n",
        "Connect to the MCP server and list all available tools. This verifies:\n",
        "- Server is running and accessible\n",
        "- Client can successfully connect\n",
        "- All RAG and Ingestor tools are properly exposed\n",
        "\n",
        "**Expected tools:** `generate`, `search`, `get_summary`, `create_collection`, `list_collections`, `upload_documents`, `get_documents`, `update_documents`, `delete_documents`, `update_collection_metadata`, `update_document_metadata`, `delete_collections`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Listing available MCP tools...\")\n",
        "print(\"=\"*80)\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"list\",\n",
        "    f\"--transport={TRANSPORT}\",\n",
        "    f\"--url={MCP_URL}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Create Collection\n",
        "\n",
        "Call the `create_collection` tool to create a new vector database collection.\n",
        "\n",
        "**Tool:** `create_collection`  \n",
        "**Purpose:** Creates a collection in the vector database to store document embeddings  \n",
        "**Arguments:**\n",
        "- `collection_name` - Name of the collection to create"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Creating collection...\")\n",
        "print(\"=\"*80)\n",
        "create_args = json.dumps({\"collection_name\": COLLECTION})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    f\"--transport={TRANSPORT}\", f\"--url={MCP_URL}\",\n",
        "    \"--tool=create_collection\",\n",
        "    f\"--json-args={create_args}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Upload Document\n",
        "\n",
        "Call the `upload_documents` tool to upload and process a PDF document.\n",
        "\n",
        "**Tool:** `upload_documents`  \n",
        "**Purpose:** Upload documents to a collection with chunking and optional summary generation  \n",
        "**Arguments:**\n",
        "- `collection_name` - Target collection\n",
        "- `file_paths` - List of absolute file paths to upload\n",
        "- `blocking` - Wait for ingestion to complete (True/False)\n",
        "- `generate_summary` - Generate document summaries (True/False)\n",
        "- `split_options` - Chunking configuration (chunk_size, chunk_overlap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Uploading document...\")\n",
        "print(\"=\"*80)\n",
        "upload_args = json.dumps({\n",
        "    \"collection_name\": COLLECTION,\n",
        "    \"file_paths\": [pdf_path],\n",
        "    \"blocking\": True,\n",
        "    \"generate_summary\": True,\n",
        "    \"split_options\": {\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    f\"--transport={TRANSPORT}\", f\"--url={MCP_URL}\",\n",
        "    \"--tool=upload_documents\",\n",
        "    f\"--json-args={upload_args}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Generate Answer with RAG\n",
        "\n",
        "Call the `generate` tool to generate an answer using the RAG pipeline.\n",
        "\n",
        "**Tool:** `generate`  \n",
        "**Purpose:** Generate answers using the RAG pipeline with context from the knowledge base  \n",
        "**Arguments:**\n",
        "- `messages` - Chat messages in OpenAI format (role, content)\n",
        "- `collection_names` - List of collections to search for context\n",
        "- `use_knowledge_base` - Enable RAG retrieval (default: True)\n",
        "\n",
        "The tool retrieves relevant document chunks and generates a contextually grounded response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Calling 'generate' tool...\")\n",
        "print(\"=\"*80)\n",
        "generate_args = json.dumps({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello from MCP demo\"}],\n",
        "    \"collection_names\": [COLLECTION],\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    f\"--transport={TRANSPORT}\",\n",
        "    f\"--url={MCP_URL}\",\n",
        "    \"--tool=generate\",\n",
        "    f\"--json-args={generate_args}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Search Documents\n",
        "\n",
        "Call the `search` tool to search the vector database for relevant documents.\n",
        "\n",
        "**Tool:** `search`  \n",
        "**Purpose:** Search the vector database and return relevant document chunks with citations  \n",
        "**Arguments:**\n",
        "- `query` - Search query text\n",
        "- `collection_names` - Collections to search\n",
        "- `vdb_top_k` - Number of documents to retrieve from vector DB\n",
        "- `reranker_top_k` - Number of documents after reranking\n",
        "- `enable_reranker` - Use reranker to improve results (True/False)\n",
        "- `enable_query_rewriting` - Rewrite query for better retrieval (True/False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Calling 'search' tool...\")\n",
        "print(\"=\"*80)\n",
        "search_args = json.dumps({\n",
        "    \"query\": \"Tell me about Robert Frost's poems\",\n",
        "    \"collection_names\": [COLLECTION],\n",
        "    \"reranker_top_k\": 2,\n",
        "    \"vdb_top_k\": 5,\n",
        "    \"enable_query_rewriting\": False,\n",
        "    \"enable_reranker\": True,\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    f\"--transport={TRANSPORT}\",\n",
        "    f\"--url={MCP_URL}\",\n",
        "    \"--tool=search\",\n",
        "    f\"--json-args={search_args}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Get Document Summary\n",
        "\n",
        "Call the `get_summary` tool to retrieve the pre-generated summary for a document.\n",
        "\n",
        "**Tool:** `get_summary`  \n",
        "**Purpose:** Retrieve document summaries generated during ingestion  \n",
        "**Arguments:**\n",
        "- `collection_name` - Collection containing the document\n",
        "- `file_name` - Name of the document file\n",
        "- `blocking` - Wait for summary if not ready (True/False)\n",
        "- `timeout` - Maximum seconds to wait if blocking\n",
        "\n",
        "Summaries are generated asynchronously during document upload if `generate_summary=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Calling 'get_summary' tool...\")\n",
        "print(\"=\"*80)\n",
        "summary_args = json.dumps({\n",
        "    \"collection_name\": COLLECTION,\n",
        "    \"file_name\": \"woods_frost.pdf\",\n",
        "    \"blocking\": False,\n",
        "    \"timeout\": 60,\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    client_path,\n",
        "    \"call\",\n",
        "    f\"--transport={TRANSPORT}\",\n",
        "    f\"--url={MCP_URL}\",\n",
        "    \"--tool=get_summary\",\n",
        "    f\"--json-args={summary_args}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Delete Collection\n",
        "\n",
        "Call the `delete_collections` tool to clean up the demo collection.\n",
        "\n",
        "**Tool:** `delete_collections`  \n",
        "**Purpose:** Delete one or more collections from the vector database  \n",
        "**Arguments:**\n",
        "- `collection_names` - List of collection names to delete\n",
        "\n",
        "This removes the collection and all its documents from the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Deleting collection...\")\n",
        "print(\"=\"*80)\n",
        "delete_args = json.dumps({\"collection_names\": [COLLECTION]})\n",
        "subprocess.run([\n",
        "    sys.executable, client_path, \"call\",\n",
        "    f\"--transport={TRANSPORT}\", f\"--url={MCP_URL}\",\n",
        "    \"--tool=delete_collections\",\n",
        "    f\"--json-args={delete_args}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Using Other Transport Protocols\n",
        "\n",
        "This notebook uses `streamable_http` by default, but the MCP server supports three transport modes:\n",
        "\n",
        "### SSE (Server-Sent Events)\n",
        "\n",
        "**To use SSE transport:**\n",
        "\n",
        "1. Update the configuration in Cell 2:\n",
        "   ```python\n",
        "   TRANSPORT = \"sse\"\n",
        "   ```\n",
        "\n",
        "2. Re-run all cells from Cell 2 onwards\n",
        "\n",
        "The `MCP_URL` will automatically be set to `http://127.0.0.1:8000/sse` based on the transport.\n",
        "\n",
        "### stdio (Standard Input/Output)\n",
        "\n",
        "**stdio transport doesn't require a separate server.** The client spawns the server automatically.\n",
        "\n",
        "**To use stdio transport:**\n",
        "\n",
        "1. Update the configuration in Cell 2:\n",
        "   ```python\n",
        "   TRANSPORT = \"stdio\"\n",
        "   ```\n",
        "\n",
        "2. Skip the server launch cell (Cell 3) - no server process needed\n",
        "\n",
        "3. In all tool call cells, replace the subprocess command with this pattern:\n",
        "   ```python\n",
        "   STDIO_CMD = sys.executable\n",
        "   STDIO_ARGS = f\"{server_path} --transport stdio\"\n",
        "   \n",
        "   subprocess.run([\n",
        "       sys.executable,\n",
        "       client_path,\n",
        "       \"list\",  # or \"call\"\n",
        "       \"--transport=stdio\",\n",
        "       f\"--command={STDIO_CMD}\",\n",
        "       f\"--args={STDIO_ARGS}\",\n",
        "       # ... add tool-specific arguments\n",
        "   ])\n",
        "   ```\n",
        "\n",
        "**Note:** stdio is ideal for local development and doesn't require managing server processes. Each client call spawns a fresh server instance.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup & Troubleshooting\n",
        "\n",
        "### Stopping the Server\n",
        "\n",
        "**Option 1:** Restart the kernel - this automatically terminates the MCP server process\n",
        "\n",
        "**Option 2:** Manually terminate using the PID shown when the server started\n",
        "\n",
        "### Common Issues\n",
        "\n",
        "**Port already in use:**\n",
        "```bash\n",
        "fuser -k 8000/tcp  # Linux\n",
        "lsof -ti:8000 | xargs kill  # macOS\n",
        "```\n",
        "\n",
        "**Server not responding:**\n",
        "- Verify RAG (port 8081) and Ingestor (port 8082) services are running\n",
        "- Check server logs for connection errors\n",
        "- For streamable_http: HTTP 406 on GET `/mcp` is normal; the endpoint only accepts MCP requests\n",
        "\n",
        "**Tool call failures:**\n",
        "- Ensure collection exists before calling RAG tools\n",
        "- Verify documents are fully ingested before searching\n",
        "- Check that file paths are absolute and accessible to the server\n",
        "\n",
        "**Dependencies issues:**\n",
        "- Ensure Python 3.11+ is installed\n",
        "- Re-run the dependencies installation cell\n",
        "- Verify `mcp`, `anyio`, `httpx`, `uvicorn`, and `fastmcp` are installed\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "- [MCP Documentation](https://modelcontextprotocol.io/docs/getting-started/intro)\n",
        "- [NVIDIA RAG Quickstart](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/deploy-docker-self-hosted.md)\n",
        "- [MCP Server Source Code](../examples/nvidia_rag_mcp/mcp_server.py)\n",
        "- [MCP Client Source Code](../examples/nvidia_rag_mcp/mcp_client.py)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
