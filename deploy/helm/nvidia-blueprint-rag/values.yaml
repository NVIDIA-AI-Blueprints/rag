# -- Global chart configuration
nameOverride: ""
fullnameOverride: "rag-server"
# subsection: rag-server
# RAG Orchestrator Service
# -- Kubernetes scheduling
nodeSelector: {}
affinity: {}
tolerations: []

# -- Common service account for rag-server
serviceAccount:
  create: true
  name: ""
  automount: true
  annotations: {}

# -- Replicas for rag-server
replicaCount: 1

# -- Namespace for documentation/reference; not actively used in templates
namespace: "nv-nvidia-blueprint-rag"

# -- Image pull secret for all images used by this chart
imagePullSecret:
  name: "ngc-secret"
  registry: "nvcr.io"
  username: "$oauthtoken"
  password: ""
  create: true

# -- Secret containing API keys for NVIDIA NGC model registry
ngcApiSecret:
  name: "ngc-api"
  password: ""
  create: true

# -- RAG server container image
image:
  repository: nvcr.io/nvstaging/blueprint/rag-server
  tag: "2.3.0.rc1"
  pullPolicy: Always

# -- RAG server service configuration
service:
  type: ClusterIP
  port: 8081

# -- RAG server container resources
resources:
  limits:
    memory: "64Gi"
  requests:
    memory: "8Gi"

# -- Probes for rag-server (optional)
livenessProbe:
  httpGet:
    path: /health
    port: 8081
  initialDelaySeconds: 10
  periodSeconds: 15
  timeoutSeconds: 5
  failureThreshold: 3
readinessProbe:
  httpGet:
    path: /health
    port: 8081
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# -- RAG server runtime configuration
server:
  workers: 8

# -- Enable/disable creation of prompt ConfigMap
promptConfig:
  enabled: true

# -- Environment variables for rag-server
envVars:
  EXAMPLE_PATH: "./nvidia_rag/rag_server"
  PROMPT_CONFIG_FILE: "/prompt.yaml"

  ##===MINIO specific configurations used to store multimodal base64 content===
  MINIO_ENDPOINT: "rag-minio:9000"
  MINIO_ACCESSKEY: "minioadmin"
  MINIO_SECRETKEY: "minioadmin"

  ##===Vector DB specific configurations===
  # URL on which vectorstore is hosted
  APP_VECTORSTORE_URL: "http://milvus:19530" # Use "http://elasticsearch:9200" for elasticsearch
  # Type of vectordb used to store embedding supported type "milvus" or "elasticsearch"
  APP_VECTORSTORE_NAME: "milvus"
  # Index type (e.g., GPU_CAGRA)
  APP_VECTORSTORE_INDEXTYPE: "GPU_CAGRA"
  # Type of vectordb search to be used
  APP_VECTORSTORE_SEARCHTYPE: "dense"
  # vectorstore collection name to store embeddings
  COLLECTION_NAME: "multimodal_data"
  APP_RETRIEVER_SCORETHRESHOLD: "0.25"
  # Top K from vector DB, which goes as input to reranker model - not applicable if ENABLE_RERANKER is set to False
  VECTOR_DB_TOPK: "100"
  # Number of document chunks to insert in LLM prompt
  APP_RETRIEVER_TOPK: "10"

  ##===LLM Model specific configurations===
  APP_LLM_MODELNAME: "nvidia/llama-3_3-nemotron-super-49b-v1_5"
  # URL on which LLM model is hosted. If "", Nvidia hosted API is used
  APP_LLM_SERVERURL: "nim-llm:8000"

  ##===Query Rewriter Model specific configurations===
  APP_QUERYREWRITER_MODELNAME: "meta/llama-3.1-8b-instruct"
  # URL on which query rewriter model is hosted. If "", Nvidia hosted API is used
  APP_QUERYREWRITER_SERVERURL: "nim-llm-llama-8b:8000"

  ##===Filter Expression Generator Model specific configurations===
  APP_FILTEREXPRESSIONGENERATOR_MODELNAME: "nvidia/llama-3_3-nemotron-super-49b-v1_5"
  # URL on which filter expression generator model is hosted. If "", Nvidia hosted API is used
  APP_FILTEREXPRESSIONGENERATOR_SERVERURL: "nim-llm:8000"
  # enable filter expression generator for natural language to filter expression conversion
  ENABLE_FILTER_GENERATOR: "False"

  ##===Embedding Model specific configurations===
  # URL on which embedding model is hosted. If "", Nvidia hosted API is used
  APP_EMBEDDINGS_SERVERURL: "nemoretriever-embedding-ms:8000"
  APP_EMBEDDINGS_MODELNAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"

  ##===Reranking Model specific configurations===
  # URL on which ranking model is hosted. If "", Nvidia hosted API is used
  APP_RANKING_SERVERURL: "nemoretriever-ranking-ms:8000"
  APP_RANKING_MODELNAME: "nvidia/llama-3.2-nv-rerankqa-1b-v2"
  ENABLE_RERANKER: "True"
  # Default confidence threshold for filtering documents by reranker relevance scores (0.0 to 1.0)
  RERANKER_CONFIDENCE_THRESHOLD: "0.0"

  ##===VLM Model specific configurations===
  ENABLE_VLM_INFERENCE: "false"
  # Reasoning gate on VLM response
  ENABLE_VLM_RESPONSE_REASONING: "false"
  # Max images sent to VLM per request (query + context)
  APP_VLM_MAX_TOTAL_IMAGES: "4"
  # Max number of query images to include in VLM input
  APP_VLM_MAX_QUERY_IMAGES: "1"
  # Max number of context images to include in VLM input
  APP_VLM_MAX_CONTEXT_IMAGES: "1"
  APP_VLM_SERVERURL: "http://nim-vlm:8000/v1"
  APP_VLM_MODELNAME: "nvidia/llama-3.1-nemotron-nano-vl-8b-v1"

  # === Text Splitter ===
  APP_TEXTSPLITTER_CHUNKSIZE: "2000"
  APP_TEXTSPLITTER_CHUNKOVERLAP: "200"

  # === General ===
  # Choose whether to enable citations in the response
  ENABLE_CITATIONS: "True"
  # Choose whether to enable/disable guardrails
  ENABLE_GUARDRAILS: "False"
  # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
  LOGLEVEL: "INFO"
  # enable multi-turn conversation in the rag chain - this controls conversation history usage
  # while doing query rewriting and in LLM prompt
  ENABLE_MULTITURN: "True"
  # enable query rewriting for multiturn conversation in the rag chain.
  # This will improve accuracy of the retrieiver pipeline but increase latency due to an additional LLM call
  ENABLE_QUERYREWRITER: "False"
  # number of last n chat messages to consider from the provided conversation history
  CONVERSATION_HISTORY: "5"

  # === Tracing ===
  APP_TRACING_ENABLED: "False"
  # HTTP endpoint
  APP_TRACING_OTLPHTTPENDPOINT: "http://rag-opentelemetry-collector:4318/v1/traces"
  # GRPC endpoint
  APP_TRACING_OTLPGRPCENDPOINT: "grpc://rag-opentelemetry-collector:4317"

  # === Reflection ===
  # enable reflection (context relevance and response groundedness checking) in the rag chain
  ENABLE_REFLECTION: "False"
  # Maximum number of context relevance loop iterations
  MAX_REFLECTION_LOOP: "3"
  # Minimum relevance score threshold (0-2)
  CONTEXT_RELEVANCE_THRESHOLD: "1"
  # Minimum groundedness score threshold (0-2)
  RESPONSE_GROUNDEDNESS_THRESHOLD: "1"
  # reflection llm
  REFLECTION_LLM: "nvidia/llama-3_3-nemotron-super-49b-v1_5"
  # reflection llm server url. If "", Nvidia hosted API is used
  REFLECTION_LLM_SERVERURL: "nim-llm:8000"

  # Choose whether to enable source metadata in document content during generation
  ENABLE_SOURCE_METADATA: "true"

  # Whether to filter content within <think></think> tags in model responses
  FILTER_THINK_TOKENS: "true"

  # Whether to enable thinking in the rag chain for llama-3.3-nemotron-super-49b model
  ENABLE_NEMOTRON_THINKING: "false"

  NEMO_GUARDRAILS_URL: "nemo-guardrails-microservice:7331"

  # enable iterative query decomposition
  ENABLE_QUERY_DECOMPOSITION: "false"
  # maximum recursion depth for iterative query decomposition
  MAX_RECURSION_DEPTH: "3"

# -- Ingestor Server
# subsection: ingestor-server
# Ingestor API Service
ingestor-server:
  enabled: true
  appName: ingestor-server

  # -- Pod scheduling
  nodeSelector: {}
  affinity: {}
  tolerations: []

  replicaCount: 1

  imagePullSecret:
    create: false
    name: "ngc-secret"
    registry: "nvcr.io"
    username: "$oauthtoken"
    password: ""

  image:
    repository: nvcr.io/nvstaging/blueprint/ingestor-server
    tag: "2.3.0.rc1"
    pullPolicy: Always

  # -- Service config for ingestor-server
  service:
    type: ClusterIP
    port: 8082

  server:
    workers: 1

  # -- Probes for ingestor-server (optional)
  livenessProbe: {}
  readinessProbe: {}

  resources:
    limits:
      memory: "25Gi"
    requests:
      memory: "25Gi"

  envVars:
    # === Vector Store Configurations ===
    APP_VECTORSTORE_URL: "http://milvus:19530" # Use "http://elasticsearch:9200" for elasticsearch
    APP_VECTORSTORE_NAME: "milvus" # supported values: "milvus" or "elasticsearch"
    APP_VECTORSTORE_SEARCHTYPE: "dense"
    APP_VECTORSTORE_ENABLEGPUINDEX: "True"
    APP_VECTORSTORE_ENABLEGPUSEARCH: "True"
    COLLECTION_NAME: "multimodal_data"

    # === MinIO Configurations ===
    MINIO_ENDPOINT: "rag-minio:9000"
    MINIO_ACCESSKEY: "minioadmin"
    MINIO_SECRETKEY: "minioadmin"

    # === Embeddings Configurations ===
    APP_EMBEDDINGS_SERVERURL: "nemoretriever-embedding-ms:8000"
    APP_EMBEDDINGS_MODELNAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"
    APP_EMBEDDINGS_DIMENSIONS: "2048"

    # === NV-Ingest Configurations ===
    APP_NVINGEST_MESSAGECLIENTHOSTNAME: "rag-nv-ingest"
    APP_NVINGEST_MESSAGECLIENTPORT: "7670"

    # === NV-Ingest extraction configurations ===
    APP_NVINGEST_PDFEXTRACTMETHOD: "None"  # Method used for text extraction from "None", "pdfium", "nemoretriever_parse"
    APP_NVINGEST_EXTRACTTEXT: "True"  # Enable text extraction
    APP_NVINGEST_EXTRACTINFOGRAPHICS: "False"  # Enable infographic extraction
    APP_NVINGEST_EXTRACTTABLES: "True"  # Enable table extraction
    APP_NVINGEST_EXTRACTCHARTS: "True"  # Enable chart extraction
    APP_NVINGEST_EXTRACTIMAGES: "False"  # Enable image extraction
    APP_NVINGEST_EXTRACTPAGEASIMAGE: "False"  # Extracts each page as image if enabled
    APP_NVINGEST_STRUCTURED_ELEMENTS_MODALITY: ""  # "image", "text_image"
    APP_NVINGEST_IMAGE_ELEMENTS_MODALITY: ""  # "image"
    APP_NVINGEST_TEXTDEPTH: "page"  # Extract text by "page" or "document"

    # === NV-Ingest caption configurations ===
    APP_NVINGEST_CAPTIONMODELNAME: "nvidia/llama-3.1-nemotron-nano-vl-8b-v1"  # Model name for captioning
    APP_NVINGEST_CAPTIONENDPOINTURL: ""  # Endpoint URL for captioning model

    # === General ===
    # Summary Model Configurations
    SUMMARY_LLM: "nvidia/llama-3_3-nemotron-super-49b-v1_5"
    SUMMARY_LLM_SERVERURL: "nim-llm:8000"
    SUMMARY_LLM_MAX_CHUNK_LENGTH: "50000"
    SUMMARY_CHUNK_OVERLAP: "200"

    # === General ===
    ENABLE_CITATIONS: "True"
    LOGLEVEL: "INFO"

    # === NV-Ingest splitting configurations ===
    APP_NVINGEST_CHUNKSIZE: "512"  # Size of chunks for splitting
    APP_NVINGEST_CHUNKOVERLAP: "150"  # Overlap size for chunks
    APP_NVINGEST_ENABLEPDFSPLITTER: "True"  # Enable PDF splitter
    APP_NVINGEST_SEGMENTAUDIO: "False"  # Enable audio segmentation for NV Ingest

    # === Redis configurations ===
    REDIS_HOST: "rag-redis-master"
    REDIS_PORT: "6379"
    REDIS_DB: "0"

    # === Bulk upload to MinIO ===
    ENABLE_MINIO_BULK_UPLOAD: "True"
    TEMP_DIR: "/tmp-data"

    # === NV-Ingest Batch Mode Configurations ===
    NV_INGEST_FILES_PER_BATCH: "16"
    NV_INGEST_CONCURRENT_BATCHES: "4"

# -- Frontend
# subsection: frontend
# RAG Playground Frontend
frontend:
  enabled: true
  appName: "rag-frontend"
  
  replicaCount: 1
  
  image:
    repository: nvcr.io/nvstaging/blueprint/rag-playground
    pullPolicy: IfNotPresent
    tag: "2.3.0.rc1"
  
  imagePullSecret:
    name: "ngc-secret"
    registry: "nvcr.io"
    username: "$oauthtoken"
    password: ""
  
  service:
    type: NodePort
    port: 3000
  
  # -- Probes for frontend (optional)
  livenessProbe: {}
  readinessProbe: {}
  
  envVars:
    # Runtime environment variables for Vite frontend
    # Note: Model names are now managed by frontend settings store
    - name: VITE_API_CHAT_URL
      value: "http://rag-server:8081/v1"
    - name: VITE_API_VDB_URL
      value: "http://ingestor-server:8082/v1"
    - name: VITE_MILVUS_URL
      value: "http://milvus:19530"

# -- Elasticsearch dependency toggle
# subsection: elasticsearch
elasticsearch:
  enabled: false
  fullnameOverride: elasticsearch
  security:
    enabled: false
    tls:
      restEncryption: false
  extraConfig:
    discovery.type: single-node
  master:
    masterOnly: false
    replicaCount: 1
  data:
    replicaCount: 0
  coordinating:
    replicaCount: 0
  ingest:
    enabled: false

# -- Observability
# subsection: serviceMonitor
serviceMonitor:
  enabled: false

# subsection: opentelemetry-collector
opentelemetry-collector:
  enabled: false
  mode: deployment
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: '${env:MY_POD_IP}:4317'
          http:
            cors:
              allowed_origins:
                - "*"
    exporters:
      # NOTE: Prior to v0.86.0 use `logging` instead of `debug`.
      zipkin:
        endpoint: "http://rag-zipkin:9411/api/v2/spans"
      debug:
        verbosity: detailed
      prometheus:
        endpoint: ${env:MY_POD_IP}:8889
    extensions:
      health_check: {}
      zpages:
        endpoint: 0.0.0.0:55679
    processors:
      batch: {}
      tail_sampling:
        # filter out health checks
        # https://github.com/open-telemetry/opentelemetry-collector/issues/2310#issuecomment-1268157484
        policies:
          - name: drop_noisy_traces_url
            type: string_attribute
            string_attribute:
              key: http.target
              values:
                - \/health
              enabled_regex_matching: true
              invert_match: true
      transform:
        trace_statements:
          - context: span
            statements:
              - set(status.code, 1) where attributes["http.path"] == "/health"

              # after the http target has been anonymized, replace other aspects of the span
              - replace_match(attributes["http.route"], "/v1", attributes["http.target"]) where attributes["http.target"] != nil

              # replace the title of the span with the route to be more descriptive
              - replace_pattern(name, "/v1", attributes["http.route"]) where attributes["http.route"] != nil

              # set the route to equal the URL if it's nondescriptive (for the embedding case)
              - set(name, Concat([name, attributes["http.url"]], " ")) where name == "POST"
  service:
    extensions: [zpages, health_check]
    pipelines:
      traces:
        receivers: [otlp]
        exporters: [debug, zipkin]
        processors: [tail_sampling, transform]
      metrics:
        exporters:
          - debug
          - prometheus
        processors:
          - memory_limiter
          - batch
        receivers:
          - otlp
          - prometheus
      logs:
        receivers: [otlp]
        exporters: [debug]
        processors: [batch]
  ports:
    metrics:
      enabled: true
      containerPort: 8889
      servicePort: 8889
      protocol: TCP

# subsection: zipkin
zipkin:
  enabled: false

# subsection: kube-prometheus-stack
kube-prometheus-stack:
  enabled: false
  prometheus:
    serviceMonitor:
      interval: "1s"
    prometheusSpec:
      scrapeInterval: "1s"
      evaluationInterval: "1s"
  grafana:
    adminUser: admin
    adminPassword: "admin"

# -- NIMs (dependencies) configuration
# subsection: nim-llm
# NIM LLM
nim-llm:
  enabled: true
  service:
    name: "nim-llm"
  image:
    repository: nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5
    pullPolicy: IfNotPresent
    tag: "1.12.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  model:
    ngcAPIKey: ""
    name: "nvidia/llama-3_3-nemotron-super-49b-v1_5"
    hfTokenSecret: ""

# subsection: nvidia-nim-llama-32-nv-embedqa-1b-v2
# NIM Text Embedding
nvidia-nim-llama-32-nv-embedqa-1b-v2:
  enabled: true
  service:
    name: "nemoretriever-embedding-ms"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2
    tag: "1.9.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

# subsection: nvidia-nim-llama-32-nemoretriever-1b-vlm-embed-v1
# NIM VLM Embedding
nvidia-nim-llama-32-nemoretriever-1b-vlm-embed-v1:
  enabled: false
  service:
    name: "nemoretriever-vlm-embedding-ms"
  image:
    repository: nvcr.io/nvidia/nemo-microservices/llama-3.2-nemoretriever-1b-vlm-embed-v1
    tag: "1.7.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

# subsection: text-reranking-nim
# NIM Text Reranking
nvidia-nim-llama-32-nv-rerankqa-1b-v2:
  enabled: true
  service:
    name: "nemoretriever-ranking-ms"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2
    tag: "1.7.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

# subsection: nim-vlm
# NIM Vision-Language (VLM)
nim-vlm:
  enabled: false
  service:
    name: "nim-vlm"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.1-nemotron-nano-vl-8b-v1
    tag: "1.3.1"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

# -- NV-Ingest dependency configuration
# subsection: nv-ingest
# NV-Ingest Service
nv-ingest:
  enabled: true
  imagePullSecrets:
    - name: "ngc-secret"
  ngcApiSecret:
    create: false
  ngcImagePullSecret:
    create: false
  image:
    repository: "nvcr.io/nvstaging/nim/nv-ingest"
    tag: "25.8.0-RC6"
  resources:
    limits:
      nvidia.com/gpu: 0
  envVars:
    INGEST_LOG_LEVEL: DEFAULT
    NV_INGEST_MAX_UTIL: 48
    INGEST_EDGE_BUFFER_SIZE: 64
    MRC_IGNORE_NUMA_CHECK: 1
    READY_CHECK_ALL_COMPONENTS: "true"
    REDIS_MORPHEUS_TASK_QUEUE: morpheus_task_queue
    NV_INGEST_DEFAULT_TIMEOUT_MS: "1234"
    MAX_INGEST_PROCESS_WORKERS: 16
    EMBEDDING_NIM_ENDPOINT: "http://nemoretriever-embedding-ms:8000/v1"
    MESSAGE_CLIENT_HOST: "rag-redis-master"
    MESSAGE_CLIENT_PORT: 6379
    MESSAGE_CLIENT_TYPE: "redis"
    MINIO_INTERNAL_ADDRESS: "rag-minio:9000"
    MINIO_PUBLIC_ADDRESS: "http://localhost:9000"
    MINIO_BUCKET: "nv-ingest"
    MILVUS_ENDPOINT: "http://milvus:19530"
    OTEL_EXPORTER_OTLP_ENDPOINT: "otel-collector:4317"
    MODEL_PREDOWNLOAD_PATH: "/workspace/models/"
    INSTALL_AUDIO_EXTRACTION_DEPS: "true"

  # Expose internal Milvus/MinIO config managed by nv-ingest subchart
  milvusDeployed: true
  milvus:
    image:
      all:
        repository: milvusdb/milvus
        tag: v2.5.3-gpu
    standalone:
      resources:
        limits:
          nvidia.com/gpu: 1
    minio:
      accessKey: minioadmin
      secretKey: minioadmin
      bucketName: nv-ingest
    fullnameOverride: milvus

  # Ensure nv-ingest does not deploy its own embedding NIM
  nvidia-nim-llama-32-nv-embedqa-1b-v2:
    deployed: false

  # Ensure nv-ingest does not deploy its own observability components
  otelDeployed: false
  zipkinDeployed: false

  # WAR to fix -loadbalancer from the ingestion NIMs URLs
  OCR_GRPC_ENDPOINT: nv-ingest-paddle:8001
  OCR_HTTP_ENDPOINT: http://nv-ingest-paddle:8000/v1/infer
  OCR_INFER_PROTOCOL: grpc
  OCR_MODEL_NAME: paddle
  YOLOX_GRPC_ENDPOINT: nemoretriever-page-elements-v2:8001
  YOLOX_HTTP_ENDPOINT: http://nemoretriever-page-elements-v2:8000/v1/infer
  YOLOX_INFER_PROTOCOL: grpc
  YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT: nemoretriever-graphic-elements-v1:8001
  YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT: http://nemoretriever-graphic-elements-v1:8000/v1/infer
  YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL: grpc
  YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT: nemoretriever-table-structure-v1:8001
  YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT: http://nemoretriever-table-structure-v1:8000/v1/infer
  YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL: grpc

  # Sub-NIMs deployed by NV-Ingest
  # NIM OCR (PaddleOCR)
  paddleocr-nim:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/baidu/paddleocr
      tag: "1.4.0"
    imagePullSecrets:
      - name: ngc-secret
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1

  # NIM Graphic Elements
  nemoretriever-graphic-elements-v1:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/nvidia/nemoretriever-graphic-elements-v1
      tag: "1.4.0"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1

  # NIM Page Elements
  nemoretriever-page-elements-v2:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/nvidia/nemoretriever-page-elements-v2
      tag: "1.4.0"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1

  # NIM Table Structure
  nemoretriever-table-structure-v1:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/nvidia/nemoretriever-table-structure-v1
      tag: "1.4.0"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
